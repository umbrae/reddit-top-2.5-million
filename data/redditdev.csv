created_utc,score,domain,id,title,ups,downs,num_comments,permalink,selftext,link_flair_text,over_18,thumbnail,subreddit_id,edited,link_flair_css_class,author_flair_css_class,is_self,name,url,distinguished
1287513673.0,182,self.redditdev,dtg4j,Want to help reddit build a recommender? -- A public dump of voting data that our users have donated for research,232,50,70,http://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/,"As promised, here is the big dump of voting information that you guys
[donated to research](http://reddit.com/ddz0s). Warning: this contains
much geekery that may [result in discomfort for the
nerd-challenged](http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/).

I'm trying to use it to build a recommender, and I've got some
preliminary source code. I'm looking for feedback on all of these
steps, since I'm not experienced at machine learning.

## Here's what I've done

* I dumped all of the raw data that we'll need to generate the public
  dumps. The queries are the comments in the two `.pig` files and it
  took about 52 minutes to do the dump against production. The result
  of this raw dump looks like:

      $ wc -l *.dump
       13,830,070 reddit_data_link.dump
      136,650,300 reddit_linkvote.dump
           69,489 reddit_research_ids.dump
       13,831,374 reddit_thing_link.dump

* I filtered the list of votes for the list of users that gave us
  permission to use their data. For the curious, that's 67,059 users:
  62,763 with ""public votes"" and 6,726 with ""allow my data to be used
  for research"". I'd really like to see that second category
  significantly increased, and hopefully this project will be what
  does it. This filtering is done by `srrecs_researchers.pig` and took
  83m55.335s on my laptop.
* I converted data-dumps that were in our DB schema format to a more
  useable format using `srrecs.pig` (about 13min)
* From that dump I mapped all of the `account_id`s, `link_id`s, and
  `sr_id`s to salted hashes (using `obscure()` in `srrecs.py` with a
  random seed, so even I don't know it). This took about 13min on my
  laptop. The result of this, `votes.dump` is the file that is
  actually public. It is a tab-separated file consisting in:

      account_id,link_id,sr_id,dir

  There are 23,091,688 votes from 43,976 users over 3,436,063 links in
  11,675 reddits. (Interestingly these ~44k users represent almost 17%
  of our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.

## What to do with it

The recommendations system that I'm trying right now turns those votes
into a set of affinities. That is, ""67% of user #223's votes on
`/r/reddit.com` are upvotes and 52% on `programming`). To make these
affinities (55m45.107s on my laptop):

     cat votes.dump | ./srrecs.py ""affinities_m()"" | sort -S200m | ./srrecs.py ""affinities_r()"" &gt; affinities.dump

Then I turn the affinities into a sparse matrix representing
N-dimensional co-ordinates in the vector space of affinities (scaled
to -1..1 instead of 0..1), in the format used by R's
[skmeans](http://cran.r-project.org/web/packages/skmeans/index.html)
package (less than a minute on my laptop). Imagine that this matrix
looks like

              reddit.com pics       programming horseporn  bacon
              ---------- ---------- ----------- ---------  -----
    ketralnis -0.5       (no votes) +0.45       (no votes) +1.0
    jedberg   (no votes) -0.25      +0.95       +1.0       -1.0
    raldi     +0.75      +0.75      +0.7        (no votes) +1.0
    ...

We build it like:

    # they were already grouped by account_id, so we don't have to
    # sort. changes to the previous step will probably require this
    # step to have to sort the affinities first
    cat affinities.dump | ./srrecs.py ""write_matrix('affinities.cm', 'affinities.clabel', 'affinities.rlabel')""

I pass that through an R program `srrecs.r` (if you don't have R
installed, you'll need to install that, and the package `skmeans` like
`install.packages('skmeans')`). This program plots the users in this
vector space finding clusters using a sperical kmeans clustering
algorithm (on my laptop, takes about 10 minutes with 15 clusters and
16 minutes with 50 clusters, during which R sits at about 220mb of
RAM)

    # looks for the files created by write_matrix in the current directory
    R -f ./srrecs.r

The output of the program is a generated list of cluster-IDs,
corresponding in order to the order of user-IDs in
`affinities.clabel`. The numbers themselves are meaningless, but
people in the same cluster ID have been clustered together.

## Here are the files

These are torrents of bzip2-compressed files. If you can't use the
torrents for some reason it's pretty trivial to figure out from the
URL how to get to the files directly on S3, but *please* try the
torrents first since it saves us a few bucks. It's S3 seeding the
torrents anyway, so it's unlikely that direct-downloading is going to
go any faster or be any easier.

* [votes.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent) -- A tab-separated list of:

      account_id, link_id, sr_id, direction

* For your convenience, a tab-separated list of votes already reduced to percent-affinities [affinities.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent), formatted:

      account_id, sr_id, affinity (scaled 0..1)

* For your convenience, [affinities-matrix.tar.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent) contains the R CLUTO format matrix files `affinities.cm`, `affinities.clabel`, `affinities.rlabel`

## And the code

* [srrecs.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig), [srrecs_researchers.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig) -- what I used to
  generate and format the dumps (you probably won't need this)
* [mr_tools.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py), [srrecs.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py) -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won't need this unless you want different information in the matrix)
* [srrecs.r](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r) -- the R-code to generate the clusters

## Here's what you can experiment with

* The code isn't nearly useable yet. We need to turn the generated
  clusters into an actual set of recommendations per cluster,
  preferably ordered by predicted match. We probably need to do some
  additional post-processing per user, too. (If they gave us an
  affinity of 0% to /r/askreddit, we shouldn't recommend it, even if
  we predicted that the rest of their cluster would like it.)
* We need a test suite to gauge the accuracy of the results of
  different approaches. This could be done by dividing the data-set in
  and using 80% for training and 20% to see if the predictions made by
  that 80% match.
* We need to get the whole process to less than two hours, because
  that's how often I want to run the recommender. It's okay to use two
  or three machines to accomplish that and a lot of the steps can be
  done in parallel. That said we might just have to accept running it
  less often. It needs to run end-to-end with no user-intervention,
  failing gracefully on error
* It would be handy to be able to idenfity the cluster of just a
  single user on-the-fly after generating the clusters in bulk
* The results need to be hooked into the reddit UI. If you're willing
  to dive into the codebase, this one will be important as soon as the
  rest of the process is working and has a lot of room for creativity
* We need to find the sweet spot for the number of clusters to
  use. Put another way, how many different types of redditors do you
  think there are? This could best be done using the aforementioned
  test-suite and a good-old-fashioned binary search.

## Some notes:

* I'm not attached to doing this in R (I don't even know much R, it
  just has a handy prebaked skmeans implementation). In fact I'm not
  attached to my methods here at all, I just want a good end-result.
* This is my weekend fun project, so it's likely to move very slowly
  if we don't pick up enough participation here
* The final version will run against the whole dataset, not just the
  public one. So even though I can't release the whole dataset for
  privacy reasons, I can run your code and a test-suite against it
",,False,,t5_2qizd,False,,,True,t3_dtg4j,http://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/,
1345496292.0,128,self.redditdev,yjk55,Proposed change to the 'users online' count for low values (&lt;100),147,19,146,http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/,"Hola all,

As of right now, if the number of online users that are on a subreddit totals fewer than 100, the metric simply displays the value as ""&lt;100"". I purposefully took a very conservative approach to this, as giving a more detailed metric for small count of active users has some potential privacy implications. For example, in a very small subreddit with a limited set of active users, you could do some analysis and an educated guess at when a group of those individuals are on reddit. The less active the subreddit, the more educated the guess. It's a bit of a reach, but I decided to err on the side of caution.

Since the feature was rolled out, the general response seems to be that people want minimum display value lowered. Here's my proposal on how to execute that, while still minimizing the potential privacy problems.

Just as it is now, the metric will be accurate for values of 100 or greater. However, if the true count is fewer than 100, a random jitter will be added to fuzz the true value. The jitter will be the largest for very small counts, and exponentially decreases as the true count increases, reaching a jitter of 0 when the true value is 100. For example, a true value of 0 may display anywhere from 0-6, a true value of 40 may display anywere from 40-43.

Additionally, low values will be cached on the back-end for 5 minutes. This prevents someone from rapidly sampling the fuzzed values to determine the true value.

I also recognize that some subreddits simply want to hide low values. To easily allow for this, I will also be adding a ""fuzzed"" CSS class to any value less than 100. This will allow subreddits to hide the low value fuzzed numbers, while still displaying higher values. Of course, the count can still be hidden entirely via CSS, just as it is now.

Please let me know any thoughts or concerns you might have regarding this proposed change.

cheers,

alienth

**tl;dr** Users-online will be display all the way down to zero, but low values will be fuzzed and cached for a period of 5 minutes to protect privacy.",,False,,t5_2qizd,1345496896.0,,,True,t3_yjk55,http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/,admin
1319579263.0,120,self.redditdev,lowwf,Attempt #2: Want to help reddit build a recommender? -- A public dump of voting data that our users have donated for research,154,34,52,http://www.reddit.com/r/redditdev/comments/lowwf/attempt_2_want_to_help_reddit_build_a_recommender/,"###UPDATE 2: [Join the Google Group to stay on top of progress!](http://groups.google.com/group/rrecommender) http://groups.google.com/group/rrecommender  

I made a post about the Google Group here: http://www.reddit.com/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/

**UPDATE: User [killerstorm](http://www.reddit.com/user/killerstorm) &amp; [qubey](/user/qubey) are doing the coordination on the project. For admin assistance, please contact [chromakode](http://www.reddit.com/user/chromakode).**

Update 3: My original suggestion and overview of the problem a year ago: http://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/


---

Hello Reddit developers! [ketralnis](http://www.reddit.com/user/ketralnis) [wrote the following message below a year ago](http://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/). After leaving reddit, the project got put on the back burner. We still have the same problem introducing people to new reddits and the [reddit search](http://www.reddit.com/reddits) is still terrible. After reading the recent reddit blog post about introducing more reddits into the default set, this project needs to be revived more than ever. 

I am hoping you can help revive this project and make it a reality! Together, we can make reddit a better place. 

----

As promised, here is the big dump of voting information that you guys
[donated to research](http://reddit.com/ddz0s). Warning: this contains
much geekery that may [result in discomfort for the
nerd-challenged](http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/).

I'm trying to use it to build a recommender, and I've got some
preliminary source code. I'm looking for feedback on all of these
steps, since I'm not experienced at machine learning.

## Here's what I've done

* I dumped all of the raw data that we'll need to generate the public
  dumps. The queries are the comments in the two `.pig` files and it
  took about 52 minutes to do the dump against production. The result
  of this raw dump looks like:

      $ wc -l *.dump
       13,830,070 reddit_data_link.dump
      136,650,300 reddit_linkvote.dump
           69,489 reddit_research_ids.dump
       13,831,374 reddit_thing_link.dump

* I filtered the list of votes for the list of users that gave us
  permission to use their data. For the curious, that's 67,059 users:
  62,763 with ""public votes"" and 6,726 with ""allow my data to be used
  for research"". I'd really like to see that second category
  significantly increased, and hopefully this project will be what
  does it. This filtering is done by `srrecs_researchers.pig` and took
  83m55.335s on my laptop.
* I converted data-dumps that were in our DB schema format to a more
  useable format using `srrecs.pig` (about 13min)
* From that dump I mapped all of the `account_id`s, `link_id`s, and
  `sr_id`s to salted hashes (using `obscure()` in `srrecs.py` with a
  random seed, so even I don't know it). This took about 13min on my
  laptop. The result of this, `votes.dump` is the file that is
  actually public. It is a tab-separated file consisting in:

      account_id,link_id,sr_id,dir

  There are 23,091,688 votes from 43,976 users over 3,436,063 links in
  11,675 reddits. (Interestingly these ~44k users represent almost 17%
  of our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.

## What to do with it

The recommendations system that I'm trying right now turns those votes
into a set of affinities. That is, ""67% of user #223's votes on
`/r/reddit.com` are upvotes and 52% on `programming`). To make these
affinities (55m45.107s on my laptop):

     cat votes.dump | ./srrecs.py ""affinities_m()"" | sort -S200m | ./srrecs.py ""affinities_r()"" &gt; affinities.dump

Then I turn the affinities into a sparse matrix representing
N-dimensional co-ordinates in the vector space of affinities (scaled
to -1..1 instead of 0..1), in the format used by R's
[skmeans](http://cran.r-project.org/web/packages/skmeans/index.html)
package (less than a minute on my laptop). Imagine that this matrix
looks like

              reddit.com pics       programming horseporn  bacon
              ---------- ---------- ----------- ---------  -----
    ketralnis -0.5       (no votes) +0.45       (no votes) +1.0
    jedberg   (no votes) -0.25      +0.95       +1.0       -1.0
    raldi     +0.75      +0.75      +0.7        (no votes) +1.0
    ...

We build it like:

    # they were already grouped by account_id, so we don't have to
    # sort. changes to the previous step will probably require this
    # step to have to sort the affinities first
    cat affinities.dump | ./srrecs.py ""write_matrix('affinities.cm', 'affinities.clabel', 'affinities.rlabel')""

I pass that through an R program `srrecs.r` (if you don't have R
installed, you'll need to install that, and the package `skmeans` like
`install.packages('skmeans')`). This program plots the users in this
vector space finding clusters using a sperical kmeans clustering
algorithm (on my laptop, takes about 10 minutes with 15 clusters and
16 minutes with 50 clusters, during which R sits at about 220mb of
RAM)

    # looks for the files created by write_matrix in the current directory
    R -f ./srrecs.r

The output of the program is a generated list of cluster-IDs,
corresponding in order to the order of user-IDs in
`affinities.clabel`. The numbers themselves are meaningless, but
people in the same cluster ID have been clustered together.

## Here are the files

These are torrents of bzip2-compressed files. If you can't use the
torrents for some reason it's pretty trivial to figure out from the
URL how to get to the files directly on S3, but *please* try the
torrents first since it saves us a few bucks. It's S3 seeding the
torrents anyway, so it's unlikely that direct-downloading is going to
go any faster or be any easier.

* [votes.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent) -- A tab-separated list of:

      account_id, link_id, sr_id, direction

* For your convenience, a tab-separated list of votes already reduced to percent-affinities [affinities.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent), formatted:

      account_id, sr_id, affinity (scaled 0..1)

* For your convenience, [affinities-matrix.tar.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent) contains the R CLUTO format matrix files `affinities.cm`, `affinities.clabel`, `affinities.rlabel`

## And the code

* [srrecs.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig), [srrecs_researchers.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig) -- what I used to
  generate and format the dumps (you probably won't need this)
* [mr_tools.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py), [srrecs.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py) -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won't need this unless you want different information in the matrix)
* [srrecs.r](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r) -- the R-code to generate the clusters

## Here's what you can experiment with

* The code isn't nearly useable yet. We need to turn the generated
  clusters into an actual set of recommendations per cluster,
  preferably ordered by predicted match. We probably need to do some
  additional post-processing per user, too. (If they gave us an
  affinity of 0% to /r/askreddit, we shouldn't recommend it, even if
  we predicted that the rest of their cluster would like it.)
* We need a test suite to gauge the accuracy of the results of
  different approaches. This could be done by dividing the data-set in
  and using 80% for training and 20% to see if the predictions made by
  that 80% match.
* We need to get the whole process to less than two hours, because
  that's how often I want to run the recommender. It's okay to use two
  or three machines to accomplish that and a lot of the steps can be
  done in parallel. That said we might just have to accept running it
  less often. It needs to run end-to-end with no user-intervention,
  failing gracefully on error
* It would be handy to be able to idenfity the cluster of just a
  single user on-the-fly after generating the clusters in bulk
* The results need to be hooked into the reddit UI. If you're willing
  to dive into the codebase, this one will be important as soon as the
  rest of the process is working and has a lot of room for creativity
* We need to find the sweet spot for the number of clusters to
  use. Put another way, how many different types of redditors do you
  think there are? This could best be done using the aforementioned
  test-suite and a good-old-fashioned binary search.

## Some notes:

* I'm not attached to doing this in R (I don't even know much R, it
  just has a handy prebaked skmeans implementation). In fact I'm not
  attached to my methods here at all, I just want a good end-result.
* This is my weekend fun project, so it's likely to move very slowly
  if we don't pick up enough participation here
* The final version will run against the whole dataset, not just the
  public one. So even though I can't release the whole dataset for
  privacy reasons, I can run your code and a test-suite against it
",,False,,t5_2qizd,True,,,True,t3_lowwf,http://www.reddit.com/r/redditdev/comments/lowwf/attempt_2_want_to_help_reddit_build_a_recommender/,
1271887790.0,116,self.redditdev,bubhl,CSV dump of reddit voting data,145,29,75,http://www.reddit.com/r/redditdev/comments/bubhl/csv_dump_of_reddit_voting_data/,"Some people have asked for a dump of some voting data, so I made one. You can [download it via bittorrent](http://redditketralnis.s3.amazonaws.com/publicvotes.csv.gz?torrent ""download me!"") (it's hosted and seeded by S3, so don't worry about it going away) and have at. The format is

    username,link_id,vote

where `vote` is -1 or 1 (downvote or upvote).

The dump is 29MB gzip compressed and contains 7,405,561 votes from 31,927 users over 2,046,401 links. It contains votes only from users with the preference ""make my votes public"" turned on (which is not the default).

This doesn't have the subreddit ID or anything in there, but I'd be willing to make another dump with more data if anything comes of this one",,False,,t5_2qizd,True,,,True,t3_bubhl,http://www.reddit.com/r/redditdev/comments/bubhl/csv_dump_of_reddit_voting_data/,
1286953765.0,99,groups.google.com,dqkfz,"""Why is Reddit so slow?""",107,8,49,http://www.reddit.com/r/redditdev/comments/dqkfz/why_is_reddit_so_slow/,,,False,,t5_2qizd,False,,,False,t3_dqkfz,http://groups.google.com/group/reddit-dev/msg/c6988091fda9672d,
1276801876.0,75,blog.reddit.com,cg4i3,We've open-sourced reddit's official iPhone app,87,12,47,http://www.reddit.com/r/redditdev/comments/cg4i3/weve_opensourced_reddits_official_iphone_app/,,,False,,t5_2qizd,False,,,False,t3_cg4i3,http://blog.reddit.com/2010/06/weve-open-sourced-ireddit.html,
1296610651.0,60,self.redditdev,fdhlw,"A Beginners Guide to the reddit Source Code: Part 1, Understanding Pylons",72,12,18,http://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/,"**Introduction**

Please criticize me! This will be an ongoing thing, I will be coming back to posts and fixing things. I want to know what you think.

So after poking around in the reddit source code like a blind man for a while, I thought that the only way to motivate myself to understand it is to explain it to others, so here it goes.

I will try to explain things at a source code level. I will not be explaining how to deploy the server, or anything like that, there are already tutorials about that. Mainly, I'll be referencing the source tree and specific files a lot. I'll try to submit diagrams every now and then to illustrate certain points better.

**Pylons**

Reddit is a web-app. It uses a Python API called Pylons to deploy the webserver and handle all calls. Pylons basically does everything. When you submit a link, view comments, or just go to reddit.com, what happens is that the relevant information is sent to Pylons which then processes it according to the reddit API.

As [eurleif](http://www.reddit.com/user/eurleif) once said about [his popular Pylons based website](http://www.omegle.com), Pylons has very little documentation and tutorials available. So I'm going to break it down as simple as possible and then break it down some more.

Pylons facilitates using a MVC (Model-View-Controller) architecture to deploy a web application. This means that there are three components of the web-app and they work separate from one another. I'm not going to go into the particulars of MVC, you can google that yourself. I'm going to work with them in order of simplest to most complicated.

**Controller**

The controller is the portion of the code that facilitates calls from the client (you) and the server (reddit). For example, if I would go to ""http://www.reddit.com/r/gaming?sort=new"" the controller takes the ""r/"" portion of the URL and processes it and whatever follows it, doing whatever the code tells it to do. In this case, it strips out every part of the URL after '/r' and passes that on to the middleware, which we will discuss later.

In Pylons, this basically means that when a call is made, the code looks at a map of calls to controllers located at [/config/routing.py](https://github.com/reddit/reddit/blob/master/r2/r2/config/routing.py) and instantiates the appropriate controller class located in the files in [/controllers](https://github.com/reddit/reddit/tree/master/r2/r2/controllers) and calls the mapped function. The function then returns a complete HTML page, which is sent to the client.

Pylons generally gets HTML parameters to the controller by having them passed as parameters to the controller function (or ""action"" in Pylons speak), the reddit code base instead makes use of decorators to set method parameters. HTML parameters are passed along using the @validate decorator that you see above the GET functions in the controller. This sets the names of the HTML parameters to the function parameters usually using several decorator functions such as ""nop()"" and ""Validate()"" which I haven't figured out yet.

For instance, if I want to search, the following is called, http://www.reddit.com/search?q=hello&amp;count=50&amp;after=t3_fcf41 (search for ""hello"", list 50 results, and list them after the result titled ""t3_fcf41""). This tells Pylons to look at the routing map for something called search. Pylons finds the appropriate mapping:

     mc = map.connect
     mc('/reddits/search', controller='front', action='search_reddits')

and passes that along to the controller. Here, the controller being used is ""front"" and the function is ""search_reddits"". This tells Pylons to get the class ""[FrontController](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/front.py)"", instantiate a new object from it and call the function ""GET_search_reddits"". @validate then sets the parameters ""query=q=hello"", ""count=50"", ""after=t3_fcf41"", ""reverse="", ""num="". Several functions are then called to perform the search and render the page based on the search results. Pylons then takes the fully rendered page and sends it to the client. If you notice, several mappings have a colon before the name. This is a wildcard, which means that it is not mapped to a particular string and the controller can see that string and decide what to do with it. These are passed along to the controller as function parameters. For instance, a mapping that looks like this mc('/foo/:bar', controller='foo', action='baz') will call a function defined like so:

    def GET_baz(self, bar):
          return dostuff

The parameter ""bar"" is mapped to the next thing in the URL after 'foo/'.

All in all pretty simple. To review: The mapping of calls is stored in 'config/routing.py' in the form of ""call_name, controller, function"". The call name is everything past ""reddit.com"" in the URL, the controller is a class with the name ""FooController"" where ""foo"" is the name of the controller in the map, and ""function"" is the aliased name of the method to call in the controller class that has the form of ""GET_function(self, **params)"" where params are a series of variables including the generic request variables and the GET and POST parameters. The page is then created and sent to the client.

Questions? 

[Part 2](http://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/)",,False,,t5_2qizd,True,,,True,t3_fdhlw,http://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/,
1322423142.0,55,i.imgur.com,mr3cn,I'm a bit embarrassed every time I show reddit to someone with an RTL browser,74,19,3,http://www.reddit.com/r/redditdev/comments/mr3cn/im_a_bit_embarrassed_every_time_i_show_reddit_to/,,,False,,t5_2qizd,False,,,False,t3_mr3cn,http://i.imgur.com/Q8sQe.png,
1364865040.0,52,github.com,1bhdpa,The source code for reddit's April Fools 2013 is now available on GitHub.,63,11,12,http://www.reddit.com/r/redditdev/comments/1bhdpa/the_source_code_for_reddits_april_fools_2013_is/,,,False,,t5_2qizd,False,,,False,t3_1bhdpa,https://github.com/reddit/reddit-plugin-f2p,admin
1313000429.0,54,apigee.com,jetkj,New tool for API developers: the reddit API console.,60,6,7,http://www.reddit.com/r/redditdev/comments/jetkj/new_tool_for_api_developers_the_reddit_api_console/,,,False,,t5_2qizd,False,,,False,t3_jetkj,https://apigee.com/console/reddit,admin
1284549887.0,50,self.redditdev,de4sf,Found a problem with Reddit &amp; Imgur,62,12,14,http://www.reddit.com/r/redditdev/comments/de4sf/found_a_problem_with_reddit_imgur/,"Not sure if this is the right place, but I visited [this](http://www.reddit.com/r/pics/comments/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/) link (a couch) and noticed that the other discussions tab indicated there was another page with a duplicate link. I had a [look](http://www.reddit.com/r/pics/duplicates/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/) and found something on Imgur, ummm totally different.

The couch leads to http://i.imgur.com/kF0PI.jpg (SFW)

The other link is http://i.imgur.com/Kf0pI.jpg (NSFW)

Looks like Imgur is case sensitive with their links. Is Reddit aware of this when working out other pages with the same links?",,False,,t5_2qizd,False,,,True,t3_de4sf,http://www.reddit.com/r/redditdev/comments/de4sf/found_a_problem_with_reddit_imgur/,
1345668780.0,51,self.redditdev,ynoxx,reddit's code deploy tool is now open source,52,1,8,http://www.reddit.com/r/redditdev/comments/ynoxx/reddits_code_deploy_tool_is_now_open_source/,"We deploy our code to the ~170 application servers currently in our infrastructure via SSH and Git.

This may or may not be useful to anyone else but we like to think that there has to be a compelling reason *not* to open source code, so here it is in all its glory.

https://github.com/reddit/push",,False,,t5_2qizd,False,,,True,t3_ynoxx,http://www.reddit.com/r/redditdev/comments/ynoxx/reddits_code_deploy_tool_is_now_open_source/,admin
1331777936.0,47,i.imgur.com,qx651,I am across the table from the guys working on reddit API at #pycon.,54,7,3,http://www.reddit.com/r/redditdev/comments/qx651/i_am_across_the_table_from_the_guys_working_on/,,,False,,t5_2qizd,False,,,False,t3_qx651,http://i.imgur.com/7yZF4.jpg,
1371012383.0,37,self.redditdev,1g6dds,Reddit Comment Streams (BETA),43,6,42,http://www.reddit.com/r/redditdev/comments/1g6dds/reddit_comment_streams_beta/,"First of all, I'm not associated with Reddit.  I'm doing this for the betterment of mankind.  :)

There has been some talk about implementing comment streams for Reddit.  Well, I've created a comment stream for developers.  I've put a lot of code together to make sure that all of the comments (that Reddit is willing to deliver) are collected, properly cached, sorted and then made available for the stream.

I cache the previous 2.5 million Reddit comments in memory and the previous 5 billion on disk.  

If you wish to test the stream out, it is available [here](http://dev.redditanalytics.com/search/stream/)  Please be advised that this stream is for applications to ingest json data (one JSON block per comment).  It will do you little good to load this with your browser unless you're simply curious or testing that the stream is active. 

Until I get more funding, I'm limited to 1 terabyte outgoing per month.  I'm working on securing 1 petabyte outgoing per month, but it's going to cost a bit of money.

Please let me know how the stream works for you!  

Thanks so much.

**Why should I use this stream?**

It will save on your API calls to Reddit and alleviate stress on Reddit's servers.

**How accurate is your data?**

My stream does not filter any comments.  The comments are formatted exactly as would be delivered from Reddit minus the wrapper.  Each JSON is delimited by a new line.  My ingest script has failover built in and is smart enough to know when there is an issue on Reddit's side.  If you see the comment on Reddit, there's a 99.9999% that it is in my stream.

**How current is your stream?**

Within 5 seconds at most.  If it falls further behind, it's Reddit's fault.  :)

**Does your stream get comment edits?**

No.  Comment edits are not available on Reddit's stream.  There is currently no known way to get comment edits while bound to the rules for Reddit's API usage.  The only way to know if a message was edited is after the fact -- when scraping the comment again at some future date and reading the ""edited"" key in the JSON response.  This key should always be false in my stream -- just as it would be when viewing http://www.reddit.com/comments.json 

**Where can I send suggestions?**

Send them to my PM on Reddit if you like.

**What is the location of this stream?**

http://dev.redditanalytics.com/search/stream/

**How many connections can I make?**

Please make only one.  There is no need to make more since you will get the same data from both.

Edit:  There is now additional logic to check if there is already an open stream for a given IP address.  If so, it will give an error.  

**How many global connections do you support?**

Hopefully enough until I get a more powerful server.  This server is very old.  :)

**You mean I could see just the comments come in real-time??**

Yep!  If you're using linux and have perl installed (most come pre-installed) and assuming you have installed JSON for Perl from CPAN (sudo cpan install JSON) ... you could use this command in bash  ... 

    wget -qO- ""http://dev.redditanalytics.com/search/stream/"" | perl -MJSON -ne 'print from_json($_)-&gt;{body}.""\n""'


**That's pretty cool, but what else could I do from the command line?**

Well, you could figure out which submissions are currently most active by piping in 1,000 comments to this command (it will take 1-2 minutes to collect all of the data)

    wget -qO- ""http://dev.redditanalytics.com/search/stream/"" | head -n 1000 | perl -MJSON -ne 'print from_json($_)-&gt;{link_title}.""\n""' | sort | uniq -c | sort -n

**I found this really useful and would like to repay you in some way**

A Reddit Gold reward would always be helpful and contribute to helping me scrape comments for their score values!

** **UPDATE** **

AndrewNeo offered an awesome suggestion to help conserve bandwidth.  If you do not need the entire stream, you can filter by subreddit.  This is how you would stream comments only for askreddit:

http://dev.redditanalytics.com/search/stream/?subreddit=askreddit 

The subreddit name is not case sensitive.  

Also, if you plan on using this API for the future, plan on having to pass an API key at some point.  Traffic is going up a bit faster than expected.  I may have to implement API keys at some point.

** **COMING SOON** **

I am also going to put a *q* parameter so you can filter the stream by keywords and phrases.  You could use this to trigger a bot or send an e-mail to Wil Wheaton when his name comes through the stream *grin*

**Questions for you the user**

Would filtering by a specific submission help?
What about filtering by author name?
How can I make this better?
",,False,,t5_2qizd,1371035009.0,,,True,t3_1g6dds,http://www.reddit.com/r/redditdev/comments/1g6dds/reddit_comment_streams_beta/,
1362591561.0,36,self.redditdev,19saz0,"Learning reddit's code, Journal #1",41,5,8,http://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/,"This is what I've learned (so far)while trying to learn how to use reddit's source code. Some or all of it might be wrong.  

I'm posting a journal here to help myself and others--hopefully, some of this will help improve the documentation, somebody will hit me with a cluebat if I ""learned"" something that's horribly wrong, etc.

----

* Easy Mode: Use Ubuntu 12.04 Long Term Support

The [install script](https://github.com/reddit/reddit/blob/master/install-reddit.sh) is Ubuntu 12.04 specific and *awesome*.  It apt-get-s dependencies, sets up the databases, and even gets the stack running.  **Highly recommended**.  

If you don't like Ubuntu Unity, you can ask Ubuntu to install LXDE, XFCE, KDE, etc.

----

* Fix the /etc/hosts file

One thing install-reddit.sh does NOT do is fix your hosts table.  Adding the following line to the /ect/hosts file can help your browser find the local reddit server:

    0.0.0.0                 reddit.local
Now you should be able to go to reddit.local in your browser, log in, and look around at...a very, very empty reddit clone.

----

* Where's the create reddit button?

By default, reddit hides the ""create reddit button"" to reduce the number of reddits created by clueless newbies. There are other ways around this, but one shortcut is to go directly to http://reddit.local/reddits/create .

----

* Tweaking the configuration:

There are some important files for tweaking your local reddit's settings.

/home/reddit/reddit/r2/example.ini  
**DON'T EDIT, JUST LOOK**  *This file lets you see all the default settings.*

/home/reddit/reddit/r2/run.ini  
*This is just a pointer--it defaults to pointing at development.ini*

/home/reddit/reddit/r2/development.update  
**NOTE: Change the *.update file, not the *.ini file**

Add the following line to /home/reddit/reddit/r2/development.update :

    short_description = I changed the description!

Then run:
    /home/reddit/reddit/r2$ sudo make

Reload http://reddit.local and boom, the title of the webpage has changed.

----

* Where's the off switch?

Turning the local server on and off can be done on the comand line:

    sudo initctl emit reddit-stop
    sudo initctl emit reddit-start
    sudo initctl emit reddit-restart
",,False,,t5_2qizd,False,,,True,t3_19saz0,http://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/,
1322683665.0,37,self.redditdev,mv40x,"Hey reddit API developers, let's talk about CAPTCHAs.",47,10,18,http://www.reddit.com/r/redditdev/comments/mv40x/hey_reddit_api_developers_lets_talk_about_captchas/,"Hey all,

I'm currently investigating our CAPTCHA implementation and ways to improve it (one thing we are considering is moving to reCAPTCHA, though it has some interesting limitations). CAPTCHAs are one place that have been difficult to work with in the current reddit API, so I'd like to take this opportunity to improve things. One of the first steps for that is opening a discussion with you. I'd like to ask 3 questions to get things started:

1. How do you currently handle reddit CAPTCHAs in your client?

2. If we make a change in the way we implement CAPTCHAs on the site, how much lead time do you need to support it in your client?

3. Is a CAPTCHA that requires opening a web page sufficient for your client, or do you need more granular access to CAPTCHA images?

Many of you run reddit bots that make posts to only a single account around the site. We'd like to eventually have an API key system where we can provide some exempt status to single-account registered apps.

Another important set of apps to consider are mobile. We understand that CAPTCHAs are a big nuisance to mobile users, though they are an important tool for registrations and unproven users. I'd love to hear your thoughts on question #3 and what would be ideal for you and your users.",,False,,t5_2qizd,False,,,True,t3_mv40x,http://www.reddit.com/r/redditdev/comments/mv40x/hey_reddit_api_developers_lets_talk_about_captchas/,admin
1287442683.0,34,github.com,dt1jt,Just did a public push,38,4,6,http://www.reddit.com/r/redditdev/comments/dt1jt/just_did_a_public_push/,,,False,,t5_2qizd,False,,,False,t3_dt1jt,http://github.com/reddit/reddit/commit/37e2ba9892d1742f78ab496d8fc1d17797731078,
1359693936.0,35,self.redditdev,17oer0,API Change: login requests containing a session cookie may fail with a 409 status,36,1,18,http://www.reddit.com/r/redditdev/comments/17oer0/api_change_login_requests_containing_a_session/,"Due to CSRF technique irresponsibly announced to a group of people tonight, we've had to make a slight tweak to our login API.

POST requests to [/api/login](http://www.reddit.com/dev/api#POST_api_login) must now **not** include a `reddit_session` cookie along in the request. If a `reddit_session` cookie exists, the request may fail with a 409 status.

This change may cause some apps and API clients to break. Notably, this will affect user switcher features like RES that don't clear out their session cookie before issuing the login request. We're sorry that we couldn't give a warning before breaking these apps. Please disclose any security issues you find in reddit [discreetly and responsibly](http://www.reddit.com/wiki/whitehat). ",,False,,t5_2qizd,False,,,True,t3_17oer0,http://www.reddit.com/r/redditdev/comments/17oer0/api_change_login_requests_containing_a_session/,admin
1339629271.0,32,self.redditdev,v0oa1,On reddit's new server code plugin system,33,1,6,http://www.reddit.com/r/redditdev/comments/v0oa1/on_reddits_new_server_code_plugin_system/,"As a few of you have already noticed, yesterday I pushed the code behind reddit's new new ""plugin system"" to GitHub. The purpose of these plugins is pretty nitty-gritty, so unless you're a reddit developer, you will probably never notice they're there.

The purpose of the new plugin system is to make it possible to extend reddit without adding to the core codebase. This originally came about when developing the new [about pages](/about), which add a lot of very reddit.com specific templates and static files, but aren't something you'd by default need on a different deployment.

In general, plugins can be used to:

 * add specific functionality to your reddit instance without changing the core
 * run experimental code before folding it into the main codebase
 * implement interchangeable functionality for a specific purpose (for instance, you could have alternate search plugins implementing Solr, IndexTank, or CloudSearch backends)
 * override specific static files or templates

On the practical side, plugins are separate python packages that you install alongside reddit. You can then enable them by adding them to the `plugins` line of your server `.ini` file. They also integrate with the main reddit `Makefile` to compile static files for production.

For a sample plugin, check out the [about pages codebase](https://github.com/reddit/reddit-plugin-about). The basic plugin definition is in [`__init__.py`](https://github.com/reddit/reddit-plugin-about/blob/master/reddit_about/__init__.py). I'll be working on releasing a skeleton plugin package in the future with more detailed information.

My hope is that having the option of writing plugins will help keep the reddit core simple while making it easier to try out and integrate small extensions to the site. If you have any questions or ideas please feel free to comment here or hit us up on IRC in #reddit-dev on FreeNode. :)

Here's the commits on GitHub behind the new plugin system:

https://github.com/reddit/reddit/compare/8b4f584f~34...8b4f584f",,False,,t5_2qizd,False,,,True,t3_v0oa1,http://www.reddit.com/r/redditdev/comments/v0oa1/on_reddits_new_server_code_plugin_system/,admin
1337166201.0,30,self.redditdev,tps8u,Having trouble getting a dev instance of reddit up and running? Here's an updated reddit VM (as of 2012-05-15) and instructions I followed to make it.,37,7,16,http://www.reddit.com/r/redditdev/comments/tps8u/having_trouble_getting_a_dev_instance_of_reddit/,"This is a [Vagrant](http://vagrantup.com/) box, like the one /u/kemitche [posted](http://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/) before.

Download it here: http://aquatica.mit.edu/~ichthyos/reddit-clean.box

MD5 = 9f986e9e4dfd5a8dde4d528515a99757

The code it's running is current as of May 15, 2012. It is running reddit on port 8000, forwards that port on your host machine to the same on your VM, and has test data from populatedb.

Download and install [VirtualBox 4.1.14](https://www.virtualbox.org/wiki/Downloads) and [Vagrant](http://vagrantup.com/) to get this VM up and running:

    $ vagrant box add base reddit-clean.box
    $ vagrant init
    $ vagrant up

Add a line to your /etc/hosts file on your host machine to make reddit.local resolve to 127.0.0.1. This will make the links on your dev reddit work. Point your web browser to http://reddit.local:8000. Voila, you should see an empty reddit!

----

Just for future reference, here are the steps I took to make it.

Lines that look like this are run on my host machine:

    host$ command

Lines that look like this are run on the VM:

    vagrant$ command

First, download and install [VirtualBox 4.1.14](https://www.virtualbox.org/wiki/Downloads) and [Vagrant](http://vagrantup.com/).

Get a clean ubuntu image, tell Vagrant to forward port 8000, start your VM, and ssh to it.

    host$ vagrant box add base http://dl.dropbox.com/u/7490647/talifun-ubuntu-11.04-server-amd64.box
    host$ vagrant init
    host$ echo ""config.vm.forward_port 8000, 8000"" &gt;&gt; Vagrantfile
    host$ vagrant reload
    host$ vagrant up
    host$ vagrant ssh

Update Ubuntu packages.

    vagrant$ sudo apt-get update
    vagrant$ sudo apt-get upgrade

Update the VirtualBox Guest Additions on the VM so that they are compatible with your newer VirtualBox.

    vagrant$ sudo aptitude install dkms
    vagrant$ wget -c http://download.virtualbox.org/virtualbox/4.1.14/VBoxGuestAdditions_4.1.14.iso
    vagrant$ sudo mount VBoxGuestAdditions_4.1.14.iso -o loop /mnt
    vagrant$ sudo sh /mnt/VBoxLinuxAdditions.run

Per [these instructions](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu), get [the latest version of the reddit install script for Ubuntu](https://gist.github.com/922144) and run it. You'll probably need to change the URL in the first line.

    vagrant$ wget https://raw.github.com/gist/922144/20655c40920185a376a21cfd0975596547927c30/install-reddit.sh
    vagrant$ chmod +x install-reddit.sh
    vagrant$ sudo ./install-reddit.sh

Still following those instructions, populate your reddit with test data.

    vagrant$ cd ~reddit/reddit/r2/
    vagrant$ sudo -u reddit paster shell run.ini
    &gt;&gt;&gt; from r2.models import populatedb
    &gt;&gt;&gt; populatedb.populate()

**EDIT:** Oops, I included the commands I ran to get reddit running on the VM but not the one I ran to package it up at the end (including the port 8000 forwarding directive in the Vagrantfile config). Here it is:

    host$ vagrant package --output=reddit-clean.box --vagrantfile Vagrantfile

**EDIT 2:** Added some more instructions and the VM's MD5 hash.",,False,,t5_2qizd,1337342776.0,,,True,t3_tps8u,http://www.reddit.com/r/redditdev/comments/tps8u/having_trouble_getting_a_dev_instance_of_reddit/,
1334120069.0,34,self.redditdev,s3vcj,"Search syntax has changed - if your client uses it, please read",37,3,27,http://www.reddit.com/r/redditdev/comments/s3vcj/search_syntax_has_changed_if_your_client_uses_it/,"The search syntax has been updated as we've moved off of indextank (they were [bought by LinkedIn](http://blog.indextank.com/1221/indextank-linkedin-acquires-indextank/) several months ago). If you use a reddit client that interacts with search in any way, it probably needs an update.

For details on the new syntax, see the [search help page](/help/search). Please feel free to ask questions here (and point out any major oddities/problems with search results). The key points are:

* Text fields must be enclosed in quotes: author:'kemitche'
* AND and OR based queries become more lisp-like: (and author:'kemitche' subreddit:'redditdev')
* is_self and over18 are now integer fields: (and reddit:'blog' is_self:1)

There is one known issue where certain older links with low votes and numbers of comments are incorrectly being sorted to the top when searching and sorted by ""relevance"". This is a data issue that I'll be trying to resolve in the next day or so.

**EDIT**: For those asking, we're on [Amazon Cloudsearch](http://aws.amazon.com/cloudsearch/) now.",,False,,t5_2qizd,True,,,True,t3_s3vcj,http://www.reddit.com/r/redditdev/comments/s3vcj/search_syntax_has_changed_if_your_client_uses_it/,admin
1340163805.0,32,self.redditdev,vb83r,I'm opening up my Reddit Search engine to front-end developers.  Now's your chance to be a legend.,34,2,22,http://www.reddit.com/r/redditdev/comments/vb83r/im_opening_up_my_reddit_search_engine_to_frontend/,"This is the [Reddit Search Engine](http://venus.xelio.info).

I'm opening up the backend to developers.  Currently it accepts requests and returns a JSON object.  10,000 calls per day with up to 200 results per call.  I can handle billions of calls (yes, billions) -- so if you need more, let me know.  

Super fast, super fun and super delicious.  Oh yeah, did I also mention you can submit links and find out which submissions had those links submitted (even denormalized urls?).

What say you, front-end developers?  Let's get this party started!

Edit:  Oops.  Forgot to mention that if you are interested, just shoot me a message on here.",,False,,t5_2qizd,False,,,True,t3_vb83r,http://www.reddit.com/r/redditdev/comments/vb83r/im_opening_up_my_reddit_search_engine_to_frontend/,
1326029291.0,30,self.redditdev,o7xgw,Originally Reddit was written in Lisp. Can someone tell me why it had to be rewritten in Python?,40,10,13,http://www.reddit.com/r/redditdev/comments/o7xgw/originally_reddit_was_written_in_lisp_can_someone/,I am interested in developing a reddit-like website and I was thinking in doing it in Lisp. What are the disadvantages of this approach? Thanks!,,False,,t5_2qizd,False,,,True,t3_o7xgw,http://www.reddit.com/r/redditdev/comments/o7xgw/originally_reddit_was_written_in_lisp_can_someone/,
1303752033.0,32,self.redditdev,gx3lg,A day in the life of hacking the code.,36,4,3,http://www.reddit.com/r/redditdev/comments/gx3lg/a_day_in_the_life_of_hacking_the_code/,"Here's a little write-up of how I work through the reddit code to make a contribution to the source. My background is teaching myself C when I was a teen to develop a MUD, then 2 years of Computer Science (C++, Java, database stuff) and PHP/Python on the side as I worked on some side projects and now working at a local web business. Python is my least familiar language and I have no experience with Pylons/Cassandra, nor am I familiar with the reddit code. [Here's a blog post of my first contribution](http://blog.reddit.com/2010/11/thanks-hackers-in-both-senses-of-word.html) but I'm going over my [""mark nsfw"" contribution here](https://github.com/reddit/reddit/pull/18).

The first thing I had to do was get the button in, otherwise I can't even test the code I'll be doing. So I did a grep for one of the buttons, in this case ""approve"". The first thing I got was **POST_approve** in api.py, which I figured was the actual code to handle approving the submission. I'd come to that later. Then a number of results appeared unrelated, so I narrowed it down by doing a grep for **\""approve\""** and sure enough I found data/templates/printablebuttons.html.py. A quick glance at it and it seems to be a ""compiled"" version, but grep also listed r2/templates/printablebuttons.html which looks to be the right place for buttons. A quick glance at the template code and it looks simple enough. I decide to use the yes/no confirmation button which report uses:

    %elif thing.show_report:
      &lt;li&gt;
        ${ynbutton(_(""report""), _(""reported""), ""report"", ""hide_thing"")}
      &lt;/li&gt;
    %endif

And I create this code based on the report code in the same file:


    %if thing.show_marknsfw:
      &lt;li&gt;${ynbutton(_(""mark NSFW""), _(""marked NSFW""), ""marknsfw"")}&lt;/li&gt;
    %endif

Along with the bool show_marknsfw, the last argument is clearly an identifier of some sort (since one of the buttons was ""hide_thing""), so I have to find where that is defined now and create marknsfw and unmarknsfw. Again I'll use grep to search for an existing identifier; I picked ""indict"". Oh and I used grep like this **find . -exec grep -l ""indict"" {} \;** I came across a number of strings and the sort (which is what I'm not looking for) and then found POST_indict in api.py and figured that must be it. An example of what I was looking over:

      @noresponse(VUser(),
                  VModhash(),
                  thing = VByNameIfAuthor('id'))
      def POST_del(self, thing):
          if not thing: return
          '''for deleting all sorts of things'''
          thing._deleted = True
          if (getattr(thing, ""promoted"", None) is not None and
              not promote.is_promoted(thing)):
              promote.reject_promotion(thing)
          thing._commit()

          # flag search indexer that something has changed
          changed(thing)

          #expire the item from the sr cache
          if isinstance(thing, Link):
              sr = thing.subreddit_slow
              expire_hot(sr)
              queries.delete_links(thing)

          #comments have special delete tasks
          elif isinstance(thing, Comment):
              thing._delete()
              delete_comment(thing)
              queries.new_comment(thing, None)

 I understood the Python in the code, but things like the noresponse() parts wasn't I knew offhand. Regardless I looked over a number of the functions and found that I needed to **thing._commit()** and **changed(thing)** (found in POST_del) when making a change.

Next up I needed to find how to mark something NSFW. (I spent a bit working on appending the title of the submission with ""(NSFW)"" into the database, but later removed that to append it in the template). So once again I did a grep for ""nsfw"" (case insensitive) and found a line like this **item.nsfw = item.over_18 and user.pref_label_nsfw**, so it looks like bool values in a struct or the sort (originally I ended up setting just item.nsfw to true and that didn't work). So back in **api.py**, I create this code basing it off the other functions in the file:

      @noresponse(VUser(),
                  VModhash(),
                  thing = VByName('id'))
      def POST_marknsfw(self, thing):
          thing.nsfw = True
          thing.over_18 = True
          thing._commit()

          # flag search indexer that something has changed
          changed(thing)

Next up is to figure out where to define show_marknsfw that defines when it'll show up. It looks like a ""method"" of thing that returns a bool.

Again, I used grep to look for show_report and found some assignments of it in **r2/lib/pages/things.py** and found that it's not so much a method, but a bool variable in the PrintableButtons class. There's a number of things done the file, such as initializing and determining the final value etc. Here are the changes I made:

    def __init__(self, style, thing,
....
    -            show_distinguish = False,
    +            show_distinguish = False, show_marknsfw = True,

...

                     show_distinguish = show_distinguish,
    +                show_marknsfw = show_marknsfw,

Basically following how the code is for similar vars. Next up is to set the actual bool to determine True/False. First was just simple logic to decide who could mark submissions NSFW. I picked mods, the submitter and admins. Did some looking around (like on the ""remove"" code that mods use) and came up with this code:

        if (c.user_is_admin or thing.can_ban or is_author) and not thing.nsfw:
            show_marknsfw = True
        else:
            show_marknsfw = False

Note: I also wrote code to unmark something NSFW, but didn't include it here because I didn't add the code until months later (just now).

I think that's it, unless I forgot anything. It took me a number of days going through this and testing it. Maybe an admin can give the technical details of exactly what is going on here (like noresponse) but I found most of it self explanatory. ",,False,,t5_2qizd,True,,,True,t3_gx3lg,http://www.reddit.com/r/redditdev/comments/gx3lg/a_day_in_the_life_of_hacking_the_code/,
1292840231.0,30,self.redditdev,eopdx,"Hey reddit, please fix your mobile page",38,8,11,http://www.reddit.com/r/redditdev/comments/eopdx/hey_reddit_please_fix_your_mobile_page/,"If I click on a link that is a picture, [this bar](http://i.imgur.com/LsXzq.png) destroys most pictures for me. And if you zoom the site to hit that way too small x, the whole page reloads. My suggestion would be to minimize the bar by omitting the number of points and comments (who cares anyway?) and to decrease the opacity.",,False,,t5_2qizd,False,,,True,t3_eopdx,http://www.reddit.com/r/redditdev/comments/eopdx/hey_reddit_please_fix_your_mobile_page/,
1361923687.0,31,self.redditdev,19ak1b,"API change: ""distinguished"" is now available in the JSON properties for submissions and comments",34,3,10,http://www.reddit.com/r/redditdev/comments/19ak1b/api_change_distinguished_is_now_available_in_the/,"`distinguished` has been added to the JSON properties on submissions and comments, to allow determining whether they have been distinguished by moderators/admins.

Possible values are:

* `null` - not distinguished
* `moderator` - the green [M]
* `admin` - the red [A]
* `special` - various other special distinguishes (most commonly seen as the darker red [] ""admin emeritus"" - [example](http://www.reddit.com/r/bestof/comments/175prt/alilarter_connects_with_a_user_who_has_a/c82tlns))

[see the code on github](https://github.com/reddit/reddit/commit/c870dde2760f147bfc58c43a8b3e4da71d793df0)",,False,,t5_2qizd,1361924199.0,,,True,t3_19ak1b,http://www.reddit.com/r/redditdev/comments/19ak1b/api_change_distinguished_is_now_available_in_the/,admin
1359866488.0,29,redditlet.com,17sjc6,"Hey guys, check out a bookmarklet I made over the last week. It lets you see what Reddit has to say about anything on the internet without leaving the current page.",36,7,11,http://www.reddit.com/r/redditdev/comments/17sjc6/hey_guys_check_out_a_bookmarklet_i_made_over_the/,,,False,,t5_2qizd,False,,,False,t3_17sjc6,http://redditlet.com,
1303321034.0,31,reddit.com,gumrm,Could someone please update the socialite firefox extention so that it works with firefox 4?,38,7,0,http://www.reddit.com/r/redditdev/comments/gumrm/could_someone_please_update_the_socialite_firefox/,,,False,,t5_2qizd,False,,,False,t3_gumrm,http://www.reddit.com/socialite/,
1274389725.0,30,self.redditdev,c6gf9,Updated VMs are now available.  They fix the networking issues (I hope) and one uses OVF for use with VirtualBox,38,8,22,http://www.reddit.com/r/redditdev/comments/c6gf9/updated_vms_are_now_available_they_fix_the/,"Here's the new links:

 * [VMWare](http://sp.reddit.com/reddit-vm-vmware.tar.gz?torrent) -- MD5: 237f88bae195816b2c2b5645469dd35d
 * [OVF/VirtualBox](http://sp.reddit.com/reddit-vm-ovf.tar.gz?torrent) -- MD5: 1da91c51f623bdec340f06430ba90bf9

I've updated the links in the blog post to point to both.  These are about 40% smaller than the previous one.  A huge amount of thanks to [grotgrot](http://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/c0qd7kt) for the how-to on cleaning it. ",,False,,t5_2qizd,False,,,True,t3_c6gf9,http://www.reddit.com/r/redditdev/comments/c6gf9/updated_vms_are_now_available_they_fix_the/,admin
1288917431.0,30,self.redditdev,e1glq,So what is actually stopping reddit from going to https?,35,5,18,http://www.reddit.com/r/redditdev/comments/e1glq/so_what_is_actually_stopping_reddit_from_going_to/,"Firesheep has reminded everyone about the risks, and a lot of websites are moving to https only (for example, github moved yesterday.) 

What is stopping reddit from moving to https for logged in users? Is it a just a lack of time, performance issue, or a problem with how the CDN is setup?",,False,,t5_2qizd,False,,,True,t3_e1glq,http://www.reddit.com/r/redditdev/comments/e1glq/so_what_is_actually_stopping_reddit_from_going_to/,
1367346993.0,29,self.redditdev,1dfdpp,"API change: ""score_hidden"" added to comments (and some HTML changes as well)",44,15,8,http://www.reddit.com/r/redditdev/comments/1dfdpp/api_change_score_hidden_added_to_comments_and/,"To go with [the new ability for subreddits to hide comment scores initially](http://www.reddit.com/r/modnews/comments/1dd0xw/moderators_new_subreddit_feature_comment_scores/), I've made a couple of updates today that will allow API clients and browser addons to support it properly (instead of just showing 1 upvote and 0 downvotes while hidden):

* `score_hidden` is now available through the API for comments
* In the HTML, a comment with a hidden score will have the class `score-hidden`, and both `data-ups` and `data-downs` will be zero.

[See the code on github](https://github.com/reddit/reddit/commit/8eac5bf1e6141cb7cebd38876f61479e2bde0ed8)",,False,,t5_2qizd,1367348844.0,,,True,t3_1dfdpp,http://www.reddit.com/r/redditdev/comments/1dfdpp/api_change_score_hidden_added_to_comments_and/,admin
1356992541.0,29,self.redditdev,15qtwh,"Congrats on a banner year in 2012, /r/redditdev!",33,4,7,http://www.reddit.com/r/redditdev/comments/15qtwh/congrats_on_a_banner_year_in_2012_rredditdev/,"reddit the open source project is flourshing.

In 2012, we've had **almost 50** distinct contributors[*](#tooltiptime ""note: the reddit.com tech team is only nine people."") to reddit's source code. That's over 50% more than last year. There've also been over 1,600 total commits to the reddit repository during 2012. Daaaaaamn.

We wanted to take this opportunity at the end of such a great year to say thank you to everyone who has taken part in this, whether it be through patches, wiki documentation, API libraries, client apps, useful bots, answering questions or even asking questions, and everyone else who uses the site. You all make it awesome.

Some yearly stats for fun:

year|distinct authors|total commits
--|--|--
2007|3|795
2008|5|599
2009|7|435
2010|14|1,175
2011|31|885
2012|48|1,615

Thank you for being a part of this.",,False,,t5_2qizd,False,,,True,t3_15qtwh,http://www.reddit.com/r/redditdev/comments/15qtwh/congrats_on_a_banner_year_in_2012_rredditdev/,admin
1331241356.0,29,self.redditdev,qnuxp,PyCon pre-packaged VM,35,6,6,http://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/,"I've uploaded a pre-packaged reddit VM for use at the PyCon sprint, but first-timers may find it useful on its own.

[Get it here (torrent)](http://s3.amazonaws.com/kemitche/reddit.box?torrent)

It's packaged up as a [""vagrant""](http://vagrantup.com) box, but you can also un-tar it to get to the VMDK disk and OVF file.

EDIT: /u/dakta has pointed out some obviously helpful and much needed info:

&gt; If the VM goes into ""Guru Meditation"", you've probably maxed out your available system memory. Go into the VM settings are tone it down until VirtualBox says it's OK.

&gt; If the VM doesn't boot after that, you need to enable PAE/NX under Settings&gt;System&gt;Processor.

&gt; Finally, I had to set the VM to open up with a GUI screen to let me select what Ubuntu shit to boot using grub.

&gt; For reference, since kemitche didn't say so, the default user is ""reddit"" passwd ""reddit""... I don't know how root is really set up, but sudo for ""reddit"" is the Vagrant default of passwordless, so if you really must change root password just sudo passwd root, although I don't suggest it.
&gt;
&gt; As expected, reddit is installed already and rearin' to go. Just point your host machine at localhost:8000 to get to the reddit install. SSH is forwarded on port 2222, so you can ssh reddit@localhost:2222 and connect through that instead.

In addition to manually tweaking the VM, if using vagrant, you can modify the VM with [config.vm.customize](http://vagrantup.com/docs/config/vm/customize.html) commands:

    config.vm.customize [""modifyvm"", :id, ""--memory"", ""1024""] # quotes around the number needed; RAM size in MB
    config.vm.customize [""modifyvm"", :id, ""--pae"", ""on""]

",,False,,t5_2qizd,True,,,True,t3_qnuxp,http://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/,admin
1320971346.0,27,reddit.com,m83gi,"Reddit Developers: Improved JSON API access to user lists: friends, subreddit moderators, subreddit contributors, and more ",33,6,0,http://www.reddit.com/r/redditdev/comments/m83gi/reddit_developers_improved_json_api_access_to/,,,False,,t5_2qizd,False,,,False,t3_m83gi,http://www.reddit.com/r/changelog/comments/m80oy/reddit_change_improved_json_api_access_to_user/,admin
1309963073.0,28,reddit.com,ii4q2,spladug explains reddit comments,32,4,1,http://www.reddit.com/r/redditdev/comments/ii4q2/spladug_explains_reddit_comments/,,,False,,t5_2qizd,False,,,False,t3_ii4q2,http://www.reddit.com/r/help/comments/ihpra/how_do_reddit_comments_work_scripting_wise/,
1285270761.0,30,imgur.com,dhzhp,This is why Reddit Web Developers are losing their hair at an increasing rate,36,6,8,http://www.reddit.com/r/redditdev/comments/dhzhp/this_is_why_reddit_web_developers_are_losing/,,,False,,t5_2qizd,False,,,False,t3_dhzhp,http://imgur.com/XZUa7.jpg,
1253896285.0,28,self.redditdev,9o2mm,I'm unable to change my password.,31,3,1,http://www.reddit.com/r/redditdev/comments/9o2mm/im_unable_to_change_my_password/,"Hi,

I'm unable to change my password. It says ""invalid password' for the current (to-be-changed) password, even though I'm perfectly able to log in with it.

I seem not to be authorized to submit a bug report under fixxit. (""Logged in users *with sufficient Reddit history* can edit all the wiki pages and submit tickets for bug reports"", emphasis mine.) I searched for tickets containing ""password"", but all such bug tickets are struck-through.

I tried to use http://www.reddit.com/feedback/ but got no reply.

Please help! Thanks.",,False,,t5_2qizd,True,,,True,t3_9o2mm,http://www.reddit.com/r/redditdev/comments/9o2mm/im_unable_to_change_my_password/,
1290567263.0,27,self.redditdev,eavt0,Why is reddit doing so much better?,32,5,15,http://www.reddit.com/r/redditdev/comments/eavt0/why_is_reddit_doing_so_much_better/,"As I type this, there's [a topic](http://www.reddit.com/r/AskReddit/comments/eaiiv/so_reddit_what_is_one_thing_you_think_everyone/) with 6872 comments on the front page. In the past, things like this have brought reddit to its knees. What's changed that is making it work so much better?",,False,,t5_2qizd,False,,,True,t3_eavt0,http://www.reddit.com/r/redditdev/comments/eavt0/why_is_reddit_doing_so_much_better/,
1374000938.0,26,self.redditdev,1ifjl1,Today I received the 'Open Sorcerer' trophy.,32,6,10,http://www.reddit.com/r/redditdev/comments/1ifjl1/today_i_received_the_open_sorcerer_trophy/,"[My commit may be small](https://github.com/reddit/reddit/commit/071986e0c7bab07f1c39d37335aa70875e0940c4), but now I know that 2% of every page on Reddit has my code in it. Every bit counts!",,False,,t5_2qizd,False,,,True,t3_1ifjl1,http://www.reddit.com/r/redditdev/comments/1ifjl1/today_i_received_the_open_sorcerer_trophy/,
1369269511.0,29,github.com,1evezm,The code for our new beta system (in use at /r/multibeta) is now available on GitHub.,30,1,0,http://www.reddit.com/r/redditdev/comments/1evezm/the_code_for_our_new_beta_system_in_use_at/,,,False,,t5_2qizd,False,,,False,t3_1evezm,https://github.com/reddit/reddit-plugin-betamode,admin
1368801701.0,27,209.208.27.225,1eipl7,I am creating a Reddit API backend that will allow for large data dumps including submissions and comments. This is an example of some real-time info for Reddit.,30,3,13,http://www.reddit.com/r/redditdev/comments/1eipl7/i_am_creating_a_reddit_api_backend_that_will/,,,False,,t5_2qizd,False,,,False,t3_1eipl7,http://209.208.27.225/,
1296784494.0,27,self.redditdev,fewoh,"A Beginners Guide to the reddit Source Code: Part 2, The View, Templates and Mako",30,3,2,http://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/,"[&lt;---Part 1](http://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/)

**Introduction** 

Please criticize and correct!

Last time, we discussed Pylons and the MVC architecture and how the Pylons controllers directs the client to the proper web page. Now we will discuss the ""view"" portion of the MVC, how the web pages are displayed.

**Mako**

Pylons uses a library called [Mako](http://www.makotemplates.org/) to generate HTML pages. If you are familiar with other server-side HTML renderers, like PHP and JSP, this should be very familiar to you. The basic structure involves writing a plain HTML page with special tags for Python variables and special PHP-like brackets for Python scripts. In Pylons, all of the templates are stored in the [/templates](https://github.com/reddit/reddit/tree/master/r2/r2/templates) directory.

Some basic Mako structures:

*Lines that start with a hash (#) are comments.    
*Lines that start with percentage (%) are Python code.    
*Python code can go anywhere between a &lt;% and a %&gt;.     
*Include statements are defined with the `&lt;%include file=""filename""&gt;` tag, where ""filename"" is the name of another template.     
*Functions can be defined with    
     &lt;%def name=""functionName(params)""&gt;
     function stuff    
     &lt;/%def&gt;    
*A function can then be called with `${functionName(params)}`     
*Variables can be defined with `${variableName}` and can be set in the Python code before the template is rendered (more on that later).

The basic method for rendering templates is the (drumroll...) `template.render()` call. Whenever this is called it returns a template rendered as plain HTML. The render function takes in the form of the variables set in the template. You set the variables by naming them explicitly in the function, i.e. `render(var1=""Hello"", var2=""world"")`. In general, the standard way to call the render function is to pass it as a string to the Templater constructor, like so:

     Template(""&lt;html&gt;&lt;head&gt;&lt;title&gt;${title}&lt;/title&gt;&lt;/head&gt;&lt;body&gt;${someText}  
    &lt;/body&gt;&lt;/html&gt;"").render(title=""Hello Templates!"", someText=""Blah de blah"")

This outputs a  simple HTML page:

    &lt;html&gt;
        &lt;head&gt;&lt;title&gt;Hello Templates!&lt;/title&gt;&lt;/head&gt;
        &lt;body&gt;Blah de blah&lt;/body&gt;
    &lt;/html&gt;

All pretty straight forward.

**Pylons and Mako**

In Pylons, a ""render"" function is included in the controller files. This function works just like the standard Mako renderer, only the first parameter is the name of the template file (located in /templates, but that can be changed.) Another difference is the `tmpl_context` class, generally imported as ""c"". This allows a cleaner way to access the template variables. All the variables found in a template are members of ""c"". So in our example, we would do the following (assuming the template is named ""hello.html"":

      def GET_index(self):
             c.title = ""Hello Templates!""
             c.someText = ""Blah de blah""
             render('/hello.html')

**How reddit connects**

The reddit codebase has  a series of classes that inherit from the reddits own home brewed [Templated](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx). These are in the directory [/lib/pages](https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages). The main file is pages.py. This contains the class Reddit which is a child class of Templated which is a parent class to most of the pages that reddit renders. These are the content renderer classes. If you look at the templates, such as the one for the [main page](https://github.com/reddit/reddit/blob/master/r2/r2/templates/reddit.html) you don't see any content. All of the content is set by these content renderer classes. Most of what the class does is set a whole lot of variables and functions that will be used by the templates. 

A level of abstraction above are the [Wrapped](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx) classes. These are the classes that define most of the HTML.

Lets look at the reddit footer for an example, since it's static and all the information is self contained in two files. (The footer is the thing at the bottom with all those outside links, that nobody ever looks at). First there is the [template](https://github.com/reddit/reddit/blob/master/r2/r2/templates/redditfooter.html). Look at this bit of code:

     %for toolbar in thing.nav:
          &lt;div class=""col""&gt;
             ${toolbar}
         &lt;/div&gt;
     %endfor

That's about 95% of the footer, right there. What it does is cycle through every item defined in thing.nav and prints it out. Looking back at the [renderer code](https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages/pages.py) (look for RedditFooter) you see that it is made up of a whole bunch of smaller pieces, each of them HTML renderers in their own right. These smaller items are defined in [/lib/menus.py](https://github.com/reddit/reddit/blob/master/r2/r2/lib/menus.py). These are subclasses for the class [Styled](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx) which is a Templated type of class. When the footer render method is called, the template cycles through all of the Styled typed classes and calls the ""render()"" function on each one, shown here as `${toolbar}`. 

Questions?",,False,,t5_2qizd,True,,,True,t3_fewoh,http://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/,
1274302366.0,28,self.redditdev,c625v,Questions and discussion about the new reddit VM. ,32,4,63,http://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/,"Since we've never actually distributed or maintained a VM before, I thought it best to pre-emptively start a discussion thread. 

[Here's a link to the torrent file.](http://sp.reddit.com/reddit-vm.tar.gz?torrent)  It runs Ubuntu 10.04 and was built on VMWare 3.0.2 on Mac OS which seems to store the vm as a bundle.  Since I can see the vmx file in the bundle, I'm hoping that there are no issues getting it working on linux and windows.  If there are, we'll just have to tweak it. 

**EDIT** (T+1hour): 26 Seeders and 55 leechers at the moment.  Whoo-hoo!  Please let me know if you manage to get the VM running, too. ",,False,,t5_2qizd,True,,,True,t3_c625v,http://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/,
1337624526.0,26,self.redditdev,txuic,Search syntax update: returning to lucene by default soon,28,2,4,http://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/,"FYI: The default search syntax will be reverting to the old ""lucene"" style before too long. If your library, script, or program uses or interacts with search, you will want to prepare for this change.

You will have two options:

1. Revert your stuff to use the old ""lucene"" syntax
2. Continue using ""cloudsearch"" syntax, and add ""syntax=cloudsearch"" to your query params.

The nitty gritty is that I've thrown together a query parser/rewriter that will rewrite lucene queries into matching cloudsearch. This means that (a) there may be bugs if using lucene syntax, and (b) lucene queries may not have access to the full range of expressions allowed - for example because cloudsearch doesn't support it, or because the rewriter doesn't support it. ([Here's the rewriter code](https://github.com/kemitche/l2cs), if you'd like to see it).",,False,,t5_2qizd,1337624738.0,,,True,t3_txuic,http://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/,admin
1283055479.0,26,github.com,d6r8v,"Hey r/redditdev, I made a Python wrapper for the Reddit API; check it out! [x-post from r/prog]",29,3,3,http://www.reddit.com/r/redditdev/comments/d6r8v/hey_rredditdev_i_made_a_python_wrapper_for_the/,,,False,,t5_2qizd,False,,,False,t3_d6r8v,http://github.com/mellort/reddit_api,
1326192669.0,25,self.redditdev,oapba,Understanding the reddit DB,31,6,9,http://www.reddit.com/r/redditdev/comments/oapba/understanding_the_reddit_db/,"So I've been trying to understand how reddit interacts with its many databases and was hoping some clever people could help confirm/correct my understanding.  To illustrate my understanding (or lack thereof) I'm going to construct an example of two users, Alice and Bob, reading a subreddit.

Bob is already reading the subreddit and is happily clicking on up/down voting buttons as he sees fit.  When he clicks on a voting arrow, some javascript is executed that changes the visibility of the up/down arrow and the dispaly with the number of votes for a link (a purely cosmetic change visible only to Bob) and ajax sends a POST request to the appropriate voting action of the API controller.  This bit of code then adds an entry to the rabbitmq link_vote queue, which says that Bob voted up/down/null on link l.

At some point, the link_vote_q process, which handles the link_vote queue, decides to do something with these votes.  It takes all the votes and updates the postgresql database to reflect this new information.

Meanwhile, Alice has decided she might like to look at the same page as Bob.  She sends a request to the appropriate GET method of the listing controller, which gets the appropriate mako template and renders it by fetching data from the postgresql database.  If rabbitmq has gotten around to commiting Bob's votes at this point she will see them, if not, she won't.

However, rendering each mako template by fetching data from the main db each time a user requests a page is time consuming.  This is where cassandra comes in.  Cassandra stores the rendered html for each page (or at least for the commonly accessed ones) and can give them to the user instead of rendering everything from the sql db.  This works great so long as nothing changes, but of course Bob is voting on things so the html in cassandra needs to be updated.  How does this happen?  I would guess that when link_process_q commits stuff to the sql db it also submits something to cassandra saying the pages that depend on this vote need updating as of ""current time"".  Then when Alice comes to view the page, cassandra knows the rendered html in its cache is too old and goes off to the mako template and the sql db and renders a fresh version.

But wait, there's more!  Even fetching stuff from cassandra is annoying, because it requires accessing the hard disc.  To minimize this, memcache keeps the most commonly accessed bits of html served by cassandra in memory, so they can accessed super quickly.

Sorry that was a bit long, but the reddit db system is a bit complicated so it kind of had to be.  If anyone could help out and tell me how far off I am, that would be great.

tl;dr Bob and Alice have a fun time on reddit.",,False,,t5_2qizd,False,,,True,t3_oapba,http://www.reddit.com/r/redditdev/comments/oapba/understanding_the_reddit_db/,
1223313414.0,27,reddit.com,75j29,Sticky for 'self' posts allowing the initial comment to remain at the top,28,1,0,http://www.reddit.com/r/redditdev/comments/75j29/sticky_for_self_posts_allowing_the_initial/,,,False,,t5_2qizd,False,,,False,t3_75j29,http://www.reddit.com/r/AskReddit/comments/75iqo/dear_reddit_should_the_submitters_comment_should/,
1361031290.0,23,reditr.com,18n3t5,"Made a Reddit Client a few months ago, still has some work to do, but what do you guys think?",29,6,11,http://www.reddit.com/r/redditdev/comments/18n3t5/made_a_reddit_client_a_few_months_ago_still_has/,,,False,,t5_2qizd,False,,,False,t3_18n3t5,http://reditr.com,
1358381310.0,24,self.redditdev,16pusk,"Tweaks and fixes to the ""banned_by"" / ""approved_by"" API properties",25,1,2,http://www.reddit.com/r/redditdev/comments/16pusk/tweaks_and_fixes_to_the_banned_by_approved_by_api/,"If you are signed in to a moderator account for a subreddit, when fetching the JSON representation of a link or comment, the `banned_by` and `approved_by` properties reflect the name of the moderator who banned or approved the submission.

These properties had two problems:

 * If a submission had been banned and then approved, both `banned_by` and `approved_by` would be set, making it difficult to determine the current state of the submission.

 * If a submission had been banned by a non-moderator (admin / admin script / etc), the `banned_by` value is supposed to be `true`. Due to a bug in this code, `banned_by` would be set to `true` even when a submission was not banned.

These api properties have now been tweaked to be mutually exclusive. If a post is banned, `banned_by` will be set, and `approved_by` will be `null` (and vice-versa). In addition, `banned_by` will now only be non-null if the submission is actually banned.

[see the code on github](https://github.com/reddit/reddit/commit/fcedf701ee1587e4de153b983ae9cca5ea5d7451)",,False,,t5_2qizd,False,,,True,t3_16pusk,http://www.reddit.com/r/redditdev/comments/16pusk/tweaks_and_fixes_to_the_banned_by_approved_by_api/,admin
1358245022.0,24,self.redditdev,16m0uu,PRAW 2.0 is Coming (release in ~2 days),25,1,4,http://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/,"PRAW users,

I've been working on-and-off for the last few weeks (with significant help from /u/_Daimon_) on extending the initial OAuth2 support added to PRAW by /u/intortus. My original plan was for addition-only changes thus resulting in a backward-compatible 1.1 release. However, properly handling reddit's OAuth2 scopes necessitated a number of backwards incompatible namespace changes (which in turn prompted numerous other namespace changes) thus I am bumping the major version number.

To minimize the amount of code from breaking that has a PRAW dependency, I wanted to give everyone a little time to update their projects before I add PRAW 2.0 to the cheeseshop (pypi). If you have code that depends on PRAW and you don't want it to break for other users of your projects you have two primary options:

The first option is to simply update your `setup.py` or `requirements.txt` file to depend on any PRAW version less than 2.0 (1.0.16 will be the last 1.0 version). If your package installation does not automatically handle dependencies then you can inform your users to run `pip install praw==1.0.16` to get an appropriate version, or point them to [this source tarball](https://github.com/praw-dev/praw/archive/praw-1.0.16.tar.gz) or [this source zipfile](https://github.com/praw-dev/praw/archive/praw-1.0.16.zip).

The second option, of course, is to update your package to work with PRAW versions &gt;= 2.0. To help you get started you'll want to checkout the PRAW 2.0 [Change Log](https://github.com/praw-dev/praw/wiki/Changelog). Until I actually release the package on pypi, you'll need to clone the github repository (`git clone git://github.com/praw-dev/praw.git`), or manually download the source ([zipfile](https://github.com/praw-dev/praw/archive/master.zip)).

I'm happy to answer any questions you have as replies to this submission. Assuming no major issues are discovered, I will release PRAW 2.0 Wednesday evening (PST).

Edit: I should add that if you are interested in using PRAW via OAuth2, check out the [PRAW OAuth wiki page](https://github.com/praw-dev/praw/wiki/OAuth).",,False,,t5_2qizd,1358245810.0,,,True,t3_16m0uu,http://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/,
1354662127.0,23,self.redditdev,14adms,API Announcement: Deleted authors in JSON responses,26,3,11,http://www.reddit.com/r/redditdev/comments/14adms/api_announcement_deleted_authors_in_json_responses/,"Yesterday, [we fixed](https://github.com/reddit/reddit/commit/3a5eabc7cccc7b5136c5e638a9bb4a7441e1e38f) a long-standing issue in the JSON responses from our API.  The issue was that the author of a thing would always have their name returned with that thing, even if their account was now deleted. This fix made it so that the `author` field in the JSON response would be `null` if the author's account was deleted.  Unfortunately, some popular clients were unhappy with this value being `null` (understandably, as it'd never been null before). /u/chromakode made [a patch](https://github.com/reddit/reddit/commit/d566d91d84327a9233da9c0c9c534c3547dc2e5f) that changed it to make the author field `""[deleted]""` instead of `null` to prevent client issues.  We do feel that `null` was the more appropriate value to indicate the account's deletion so this is considered temporary and will return to `null`, no earlier than Dec 31, 2012.",,False,,t5_2qizd,False,,,True,t3_14adms,http://www.reddit.com/r/redditdev/comments/14adms/api_announcement_deleted_authors_in_json_responses/,admin
1335887579.0,24,github.com,t1m8x,"As suggested, I open sourced my /r/IAMA bot.  Example usage of narwal, Heroku, MongoLab, and spending no $.",30,6,11,http://www.reddit.com/r/redditdev/comments/t1m8x/as_suggested_i_open_sourced_my_riama_bot_example/,,,False,,t5_2qizd,False,,,False,t3_t1m8x,https://github.com/larryng/reddit-iama-bot,
1213807146.0,23,self.redditdev,6nxb3,"Reddit developer discussion thread for June 18, 2008",28,5,24,http://www.reddit.com/r/redditdev/comments/6nxb3/reddit_developer_discussion_thread_for_june_18/,,,False,,t5_2qizd,False,,,True,t3_6nxb3,http://www.reddit.com/r/redditdev/comments/6nxb3/reddit_developer_discussion_thread_for_june_18/,
1374533728.0,23,self.redditdev,1iuged,API change: Your moderator permissions are now available through /subreddits/mine/moderator/.json,28,5,0,http://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/,"From reddit's student contractor /u/slyf:

---

Moderator permissions are now available in the http://www.reddit.com/subreddits/mine/moderator/.json view with a key of ""mod_permissions"". The value should be a list of permissions the current user has in that subreddit by name or [""all""].

Current available permissions are: access, config, flair, mail, posts, wiki, and all. However, more could be added or changed in the future.

---

[View the code for this change on github](https://github.com/reddit/reddit/commit/8c4732418dbe09e06f3fb0f7c911fd16386d7944)",,False,,t5_2qizd,False,,,True,t3_1iuged,http://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/,admin
1365670679.0,22,self.redditdev,1c4i8q,Attention Reddit Developers -- I am creating a massive archive of Reddit submissions for developers to use.,28,6,23,http://www.reddit.com/r/redditdev/comments/1c4i8q/attention_reddit_developers_i_am_creating_a/,"In the interest in helping out both Reddit's servers and to provide developers with an easy to use source for all Reddit submissions, I am creating an online archive of all Reddit submissions in JSON format.  

Each T3 object (a submission that was not spam) is represented as one line of JSON in a file.  Each file is an entire day's worth of submissions to Reddit and compressed using gzip.  

I will also include Python files to serve as an example of how to process each file.  You can take the information and put it into a database or a flat file or run your own analysis on the data.  I am working backwards from the present and will add approximately a month's worth of submissions each day.  Within a month or two, all Reddit submissions will be available in this archive.

Please use these archive files instead of using Reddit's API if you simply want submission data.  It will spare the Reddit servers a bit of load *and* you can get data far more quickly than making requests to the API every two seconds.

The URL for the archive is:  http://redditfiles.4shared.com

If you are familiar with JSON data, you should have no problem using this data for your projects.  

I am creating a search engine for Reddit that is (I hope) superior to what is currently available.  I'd love to work with anyone who is good at front-end development -- so please send me a message if you are interested.  

Thanks and enjoy!  

**Sample Python Parser:**

*gzip -cd reddit_2013-04-10.json.gz | thisPythonScript.py*

There are a lot more variables you can pull from the JSON objects. Just open up the JSON and look at all the variables you have access to and just add them to this script using the same format as I used for the variables already in this script.

    #!/usr/bin/python
    # --------------------------------------------------------------------------------------------- 
    # This Python script will read from STDIN a piped file containing Reddit JSON data
    # Example: gzip -cd reddit.json.gz | thisScript.py
    # --------------------------------------------------------------------------------------------- 

    import sys
    import json
    import datetime

    for line in iter(sys.stdin.readline, """"):
        decode = json.loads(line)
        subreddit = decode['subreddit']
        url = decode['url']
        id = decode['id']
        title = decode['title']
        score = decode['score']
        time = datetime.datetime.fromtimestamp(int(decode['created_utc'])).strftime('%Y-%m-%d %H:%M:%S')
        link_flare = decode['link_flair_css_class']
        num_comments = decode['num_comments']
        num_reports = decode['num_reports']
        print id, subreddit, link_flare, time, score, num_comments, title.encode('utf-8')

There is probably a more elegant way to write this script but I hacked it out quickly for you guys to play with.  If you're running linux, you have all the tools you need to start playing with raw Reddit data!  (I just started writing Python code a few weeks ago after a lifetime of writing Perl)",,False,,t5_2qizd,1365759439.0,,,True,t3_1c4i8q,http://www.reddit.com/r/redditdev/comments/1c4i8q/attention_reddit_developers_i_am_creating_a/,
1322233890.0,22,self.redditdev,mox66,Does anyone actually have a working reddit on their own site?,24,2,12,http://www.reddit.com/r/redditdev/comments/mox66/does_anyone_actually_have_a_working_reddit_on/,"Just wondering if anyone has actually succeeded in installing reddit on a server.  I'm trying to understand how I'd install it on a VPS server I'm paying for.

I've tried reading the wiki but I think I'm underestimating the task, so how easy is it to do this?",,False,,t5_2qizd,False,,,True,t3_mox66,http://www.reddit.com/r/redditdev/comments/mox66/does_anyone_actually_have_a_working_reddit_on/,
1292711831.0,25,self.redditdev,eo2p0,FYI - Raldi has promised a props post for polishing up a CSS theme for the colour blind :),26,1,0,http://www.reddit.com/r/redditdev/comments/eo2p0/fyi_raldi_has_promised_a_props_post_for_polishing/,"The thread from /r/IdeasForTheAdmins is [here](http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/) as is a new shape replacement for the ""You've Got Mail"" icon c/o user [Tsadiq](http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/c19jooi).   
  
User SquireCD in that thread is the colour blind redditor if you're seeking target user feedback. ",,False,,t5_2qizd,False,,,True,t3_eo2p0,http://www.reddit.com/r/redditdev/comments/eo2p0/fyi_raldi_has_promised_a_props_post_for_polishing/,
1284747871.0,22,self.redditdev,dfbh8,How many of you would be interested in a live CD of reddit?,33,11,17,http://www.reddit.com/r/redditdev/comments/dfbh8/how_many_of_you_would_be_interested_in_a_live_cd/,"I have done Live CD's of Ubuntu and other Linux distros before.See http://pubserver.co.nr

Both of these had the option of installation. 


How many of you would be interested in a live CD distro from the VM of Reddit? 

EDIT: sorry guys reddit doesnt want live CD's of the system. Thanks for the support!",,False,,t5_2qizd,True,,,True,t3_dfbh8,http://www.reddit.com/r/redditdev/comments/dfbh8/how_many_of_you_would_be_interested_in_a_live_cd/,
1276725453.0,24,chrome.google.com,cfrxf,"Hi guys, i made a reddit chrome plugin. I think this is a nice place to post this; Hope it could be useful to you.",27,3,15,http://www.reddit.com/r/redditdev/comments/cfrxf/hi_guys_i_made_a_reddit_chrome_plugin_i_think/,,,False,,t5_2qizd,False,,,False,t3_cfrxf,https://chrome.google.com/extensions/detail/cnlggdhmllfmmonecfeabjekpblkocma?hl=en,
1372184536.0,24,redditanalytics.com,1h1wqu,Anonymous FTP access for Reddit comment data is now available,27,3,8,http://www.reddit.com/r/redditdev/comments/1h1wqu/anonymous_ftp_access_for_reddit_comment_data_is/,,,False,,t5_2qizd,False,,,False,t3_1h1wqu,ftp://redditanalytics.com,
1362200071.0,22,randalolson.com,19ib1f,Fun with the Python Reddit API Wrapper and word clouds,26,4,3,http://www.reddit.com/r/redditdev/comments/19ib1f/fun_with_the_python_reddit_api_wrapper_and_word/,,,False,,t5_2qizd,False,,,False,t3_19ib1f,http://www.randalolson.com/2013/03/01/fun-with-the-python-reddit-api-wrapper-and-word-clouds/,
1360862300.0,24,self.redditdev,18ivbo,"I built a website using the Reddit API, please visit the Beta and share thoughts",27,3,21,http://www.reddit.com/r/redditdev/comments/18ivbo/i_built_a_website_using_the_reddit_api_please/,"Hi! RedditDev community

We (Simon and Gilles) created a website using the Reddit API.
You will be able to see every video, post or image from any subreddit without opening any link, using an infinite scroll.

&gt;  www.reddit-roll.com

The website is not finished yet. We would like your feedback! 

What improvements could we do ? What features can you think of ? Did you encounter any bug? Any suggestions?

Please visit the website and tell us what's on your mind using the feedback button on the left. We appreciate every feedback and the time you spend helping us and the community.

Simon and Gilles out!",,False,,t5_2qizd,False,,,True,t3_18ivbo,http://www.reddit.com/r/redditdev/comments/18ivbo/i_built_a_website_using_the_reddit_api_please/,
1357833724.0,22,self.redditdev,16bh8j,Wrote my first python script using PRAW to check /r/gamedeals for new posts and notify me,26,4,6,http://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/,"I have a little obsession of checking /r/gamedeals frequently for new game deals. Just wrote my first useful python script to make my life a little easier. The script checks for new posts made to that sub-reddit and pushes a Growl notification. The Growl notification looks like - http://i.imgur.com/ET9vL.png

	#!/usr/bin/env python
	#
	# This script looks up /r/gamedeals/new every 120 seconds and pushes the notification
	# for new posts to Growl app on Mac OS
	#
	# Uses PRAW - https://github.com/praw-dev/praw (for easy access to Reddit API)
	# and GNTP - https://github.com/kfdm/gntp (for pushing Growl notification)
	#
	
	import gntp.notifier
	import praw
	import time
	
	# icon for use with growl notification
	ICON_URL = ""http://cdn2.iconfinder.com/data/icons/crystalproject/128x128/apps/package_games.png""
	USER_AGENT = 'new /r/gamedeals notifier by /u/mpheus'
	# reddit doesn't shows new posts made to a subreddit without logging in
	# REDDIT_ID = '' # not required
	# REDDIT_PASS = '' # not required
	
	growl = gntp.notifier.GrowlNotifier(
		applicationName = ""/r/gamedeals notifier"",
		notifications = [""New Deal""],
		defaultNotifications = [""New Deal""],
		# hostname = ""computer.example.com"", # Defaults to localhost
		# password = ""abc123"" # Defaults to a blank password
	)
	growl.register()
	
	r = praw.Reddit(user_agent=USER_AGENT)
	# r.login(REDDIT_ID, REDDIT_PASS)
	
	already_done = [] # for storing the uids of posts already notified
	
	while True:
		data = r.get_subreddit('gamedeals').get_new_by_date(limit=10)
		for x in data:
			if x.id not in already_done:
				already_done.append(x.id)
				growl.notify(
					noteType = ""New Deal"",
					title = x.domain,
					description = x.title,
					icon = ICON_URL,
					sticky = True, # so that notification remains on the screen until closed
					priority = 1,
					callback = x.permalink
				)
				print ""Notified about"", x.title, ""at"", time.strftime(""%d %b - %I:%M:%S %p"")
				
		print ""Last checked for game deals at"", time.strftime(""%d %b - %I:%M:%S %p"")
		time.sleep(120)

I just launch this script in a terminal window and leave it running. I'm learning python right now and plan on improving the script by storing the uids of posts that already have been notified about in a database and leave the script running on Raspberry Pi that runs 24/7. Growl can be set to receive notification pushed from remote computer (in this case, RPi) as well.

Thanks to reddit dev and PRAW dev for all their amazing work!

EDIT: Removed login call and replaced `get_new` function with `get_new_by_date`.",,False,,t5_2qizd,1357899575.0,,,True,t3_16bh8j,http://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/,
1357685813.0,23,cs229.stanford.edu,167m6w,Reddit Recommendation System - CS 229: Machine Learning [pdf],24,1,1,http://www.reddit.com/r/redditdev/comments/167m6w/reddit_recommendation_system_cs_229_machine/,,,False,,t5_2qizd,False,,,False,t3_167m6w,http://cs229.stanford.edu/proj2011/PoonWuZhang-RedditRecommendationSystem.pdf,
1323984991.0,21,self.redditdev,neabt,Will Reddit be updating to jQuery 1.7 any time soon?,25,4,1,http://www.reddit.com/r/redditdev/comments/neabt/will_reddit_be_updating_to_jquery_17_any_time_soon/,"I'm just curious, as I now see tons of annoying console log messages in Chrome about event.layerX and event.layerY being deprecated, and it turns out they're coming from jQuery...

Hardly the end of the world (although it does affect performance) -- but once webkit does deprecate those, some reddit functionality that relies on jQuery may (maybe?) break....",,False,,t5_2qizd,False,,,True,t3_neabt,http://www.reddit.com/r/redditdev/comments/neabt/will_reddit_be_updating_to_jquery_17_any_time_soon/,
1257198164.0,22,blog.reddit.com,a0aff,"We'll no longer be showing the ""edited"" star for comments edited in the first minute of their lifetime",27,5,11,http://www.reddit.com/r/redditdev/comments/a0aff/well_no_longer_be_showing_the_edited_star_for/,,,False,,t5_2qizd,False,,,False,t3_a0aff,http://blog.reddit.com/2009/11/no-stars-for-early.html,
1355700912.0,22,self.redditdev,14yppc,How does reddit come up with unique IDs for 'things'?,22,0,7,http://www.reddit.com/r/redditdev/comments/14yppc/how_does_reddit_come_up_with_unique_ids_for_things/,Reddit has to be creating 'things' at least a hundred times a second. How do they go about creating unique id's like **14kmg6** that don't overlap?,,False,,t5_2qizd,False,,,True,t3_14yppc,http://www.reddit.com/r/redditdev/comments/14yppc/how_does_reddit_come_up_with_unique_ids_for_things/,
1337327289.0,18,self.redditdev,tszxg,Trying to figure out the Reddit code... Is there some sort of docs/diagrams I can look at? Where do I start?,25,7,9,http://www.reddit.com/r/redditdev/comments/tszxg/trying_to_figure_out_the_reddit_code_is_there/,,,False,,t5_2qizd,False,,,True,t3_tszxg,http://www.reddit.com/r/redditdev/comments/tszxg/trying_to_figure_out_the_reddit_code_is_there/,
1332806722.0,21,reddit.com,rf15g,New API documentation (a project from the PyCon sprints) [xpost from /r/changelog],24,3,1,http://www.reddit.com/r/redditdev/comments/rf15g/new_api_documentation_a_project_from_the_pycon/,,,False,,t5_2qizd,False,,,False,t3_rf15g,http://www.reddit.com/r/changelog/comments/rf0y4/reddit_change_new_api_documentation_a_project/,admin
1326231725.0,21,self.redditdev,obd0c,karmawhores.net needs some help,23,2,14,http://www.reddit.com/r/redditdev/comments/obd0c/karmawhoresnet_needs_some_help/,"I run karmawhores.net.  It has been down for a while, I believe because my bot got blocked.  I have attempted to contact the admins numerous times without any response.

I think it is a useful site that people appreciate, so I am asking for help reaching someone who can tell me what to do.  I've paid for it out of my pocket, never once asked for donations or shown ads.  I've offered to reduce the crawl interval down to whatever the admins feel is reasonable (once a day, once a month, I don't care).

I am hoping that by posting here I can get the attention of someone in a position to help get the site working again.

Thanks!

    karmawhores /var/www/karmawhores.net: curl --head http://www.reddit.com/user/jontas/about.json
    HTTP/1.1 403 Forbidden
    Content-Type: text/html
    Vary: Accept-Encoding
    Cache-Control: no-cache
    Date: Fri, 02 Dec 2011 22:40:25 GMT
    Connection: keep-alive",,False,,t5_2qizd,False,,,True,t3_obd0c,http://www.reddit.com/r/redditdev/comments/obd0c/karmawhoresnet_needs_some_help/,
1272324764.0,19,github.com,bwggv,Fork our github repository!,23,4,4,http://www.reddit.com/r/redditdev/comments/bwggv/fork_our_github_repository/,,,False,,t5_2qizd,False,,,False,t3_bwggv,http://github.com/reddit/reddit,
1370366158.0,18,github.com,1fnuvk,New reddit plugin: meatspace. The code for the new meetup QR code system.,23,5,4,http://www.reddit.com/r/redditdev/comments/1fnuvk/new_reddit_plugin_meatspace_the_code_for_the_new/,,,False,,t5_2qizd,False,,,False,t3_1fnuvk,https://github.com/reddit/reddit-plugin-meatspace,admin
1358539698.0,20,github.com,16u2lh,"I created a basic Reddit OAuth PHP library, check it out!",21,1,1,http://www.reddit.com/r/redditdev/comments/16u2lh/i_created_a_basic_reddit_oauth_php_library_check/,,,False,,t5_2qizd,False,,,False,t3_16u2lh,https://github.com/jariz/RedditOAuth,
1355459446.0,21,scribd.com,14ts77,Reddit Architecture and Extension Proposal,29,8,12,http://www.reddit.com/r/redditdev/comments/14ts77/reddit_architecture_and_extension_proposal/,,,False,,t5_2qizd,False,,,False,t3_14ts77,http://www.scribd.com/doc/116782750/Reddit-Architecture-and-Extension-Proposal#fullscreen,
1326360385.0,21,self.redditdev,odqll,My new site based on the reddit source code...,24,3,12,http://www.reddit.com/r/redditdev/comments/odqll/my_new_site_based_on_the_reddit_source_code/,"I've started a new site [sciteit.com](http://sciteit.com) based on the reddit source code.  The idea is basically to be a place for scientists and others involved in academia to share links to papers, comment on them, with features/structure tailored towards this purpose.  You can read my full rationalization [here](http://dobby.dyndns.biz/constantamateur/2012/01/11/introducing-sciteit/) if you're interested.

It's still very much ""work in progress"", but I wanted to say thank you to everyone here and in the IRC channel who has helped me understand the reddit source well enough to get this far.  Special thanks to spladug and bboe.  I fully expect to have many future dumb questions to ask as things break and I hope you will be as patient and indulging as you have been thus far.

In fact, I already have a question.  My cassandra instance crashed this morning, with error message ""java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException"".  At this point I don't actually use cassandra to do anything as far as I'm aware (use_query_cache=write_query_queue=False), but it still needs to be running for the site to be live.  I'm running on a virtual private server with only 500mb of ram, is it possible that this is the problem?  The max heap size seems to be set correctly (to 256mb).
",,False,,t5_2qizd,True,,,True,t3_odqll,http://www.reddit.com/r/redditdev/comments/odqll/my_new_site_based_on_the_reddit_source_code/,
1268161711.0,20,self.redditdev,bb9b3,Any chance of accepting an OpenID patch?,24,4,5,http://www.reddit.com/r/redditdev/comments/bb9b3/any_chance_of_accepting_an_openid_patch/,"I'm a big proponent of OpenID, and I'd like to see reddit accept it as a means of logging in. I'd be willing to put some time into writing a good OpenID implementation for reddit, but I wanted to make sure that it won't be rejected on philosophical grounds first. If there are objections to my implementation I understand if it ends up being rejected, but before I put my time into it I want to make sure it won't be rejected on principle.",,False,,t5_2qizd,False,,,True,t3_bb9b3,http://www.reddit.com/r/redditdev/comments/bb9b3/any_chance_of_accepting_an_openid_patch/,
1261446509.0,18,self.redditdev,ah9v7,Feature request: Allow submitter to choose thumbnail images from the submitted webpage?,21,3,4,http://www.reddit.com/r/redditdev/comments/ah9v7/feature_request_allow_submitter_to_choose/,"Would this feature be practical?

Submitter can choose a thumbnail image for a link. The list of possible thumbnails would be generated from available images on the page. 

If this creates too much load on the servers, maybe the current thumbnail algorithm can be used to present a list of only three thumbnails at a time, sorted by judged relevancy. By using ""prev"" and ""next"" buttons the submitter can browse the list of images, three at a time. 

This can even be made optional by using a ""select thumbnail image"" button on the submission page. ",,False,,t5_2qizd,True,,,True,t3_ah9v7,http://www.reddit.com/r/redditdev/comments/ah9v7/feature_request_allow_submitter_to_choose/,
1370538520.0,17,redditanalytics.com,1fsuyo,New Reddit Search Engine is available (in alpha testing). She's buggy and filled with holes but manages to stay afloat!,22,5,3,http://www.reddit.com/r/redditdev/comments/1fsuyo/new_reddit_search_engine_is_available_in_alpha/,,,False,,t5_2qizd,False,,,False,t3_1fsuyo,http://www.redditanalytics.com/search/,
1367031143.0,19,praw.readthedocs.org,1d72t8,PRAW 2.1.0 released with multiprocessing support among other changes,19,0,8,http://www.reddit.com/r/redditdev/comments/1d72t8/praw_210_released_with_multiprocessing_support/,,,False,,t5_2qizd,False,,,False,t3_1d72t8,https://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-0,
1344972451.0,19,self.redditdev,y7vpb,INI setting change - please read if you're running a reddit clone,20,1,3,http://www.reddit.com/r/redditdev/comments/y7vpb/ini_setting_change_please_read_if_youre_running_a/,"With [this commit](https://github.com/reddit/reddit/commit/01146c301e9509f246cf3827c591f29068c82131), the ini setting for `lang` has changed to `site_lang`. This was necessary to remove an odd reliance on the completely unused r2.mo file in the reddit/reddit repo (translations are found in https://github.com/reddit/reddit-i18n if desired). It's possible there's a better/cleaner way, and if you know of one, I'd love to hear it. In the meantime, if you have `LanguageError`s on app startup, check your INI setting to make sure that you are NOT setting a `lang` variable.",,False,,t5_2qizd,1345061010.0,,,True,t3_y7vpb,http://www.reddit.com/r/redditdev/comments/y7vpb/ini_setting_change_please_read_if_youre_running_a/,admin
1288855516.0,20,self.redditdev,e120r,Support for the &lt;blink&gt; tag [patch included],35,15,15,http://www.reddit.com/r/redditdev/comments/e120r/support_for_the_blink_tag_patch_included/,"After being inspired by [this](http://www.reddit.com/r/web_design/comments/e0mnq/breaking_news_10_year_old_twins_make_a_website/c14d9le) comment, I decided that it would be worthwhile to actually create the horrific thing. So, I present to you a patch that should add blink support to Reddit.

To create a blinking section, you enclose the text between `|`s, similar to the way italics currently work with `*`s.

Example:

    |This is a blinking comment|

Patch is [here](http://pastebin.com/QADYkM75), consider it public domain. It is currently untested, so don't blame me if it sets fire to your servers or eats you for lunch.

EDIT:

New patch is [here](http://pastebin.com/BuYuMK5Z), updated because of the &lt;blink&gt; tag's removal from HTML5.",,False,,t5_2qizd,True,,,True,t3_e120r,http://www.reddit.com/r/redditdev/comments/e120r/support_for_the_blink_tag_patch_included/,
1287005864.0,21,self.redditdev,dqv56,Turning Reddit into Wikipedia,29,8,8,http://www.reddit.com/r/redditdev/comments/dqv56/turning_reddit_into_wikipedia/,"In a reply to [a link about reddit's slowness](http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127h9b), I suggested to 

&gt;&gt; [...] shift the friend-lookup into the client[.] Some javascript should be able to turn usernames red if they are in a list.

and ketralnis replied:

&gt;I'd recommend you take this discussion to /r/redditdev if you're really interested, but for this question, the client has to get the data from somewhere too, right?

My idea is, [similar to cdawzrd and AngerMCS](http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127t8s), that the client should be able to  cache the data so that reddit only serves the friends-list once. 

One final speculation:
If reddit also serves the list of ""comment upvote ids"" and other stuff seperately, it should be able to serve ""flat comment pages"" to everybody. Then, the comment pages can be cached for a second on the server before reddit has to include the recent  upvotes and comments ""batch-style"" in a single pass.

This should be similar to the wiki architecture where most people just get a plain html file that only gets updated when somebody alters an article.

",,False,,t5_2qizd,False,,,True,t3_dqv56,http://www.reddit.com/r/redditdev/comments/dqv56/turning_reddit_into_wikipedia/,
1374430339.0,20,self.redditdev,1iredp,"It's not much, but it's my first bot. I made it to run on my Raspberry Pi that's connected to a display.",20,0,8,http://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/,"This bot grabs 1000 submissions from /r/gifs. It then filters out the submissions that are from 'i.imgur.com' and adds them to a list. After that it cycles through the list and embeds one image into a HTML file as a background-image.  The webpage then refreshes every X amount of seconds to display the new image. The bot also grabs the submission title and overlays it at the bottom. 

I had an old monitor I wasn't using and was looking for a project to work on with my RPi. I figured I would just use the monitor as a sort of digital picture frame. Then I thought, lets make it more interesting by using gifs instead of images. 

If you have any suggestions on making it more efficient, I'd be glad to hear them. This is my first time working with Python and PRAW, so I'm sure there are better ways of doing this.

    import praw
    import time
    
    r=praw.Reddit(user_agent='Dynamic gif Slideshow by /u/xiggy')
    subreddit = r.get_subreddit('gifs')
    submissions = subreddit.get_hot(limit=1000)
    
    imglinks = []
    used = []
    c = 0
    
    while True:
    	
    	for submission in submissions:
    		if submission.id not in used:
    			if submission.domain == 'i.imgur.com':
    				tit = submission.title #tit... lol
    				imglinks.append(submission.url)
    				used.append(submission.id)
    
    				html_file = open(""style.css"", ""w"")
    				html_file.write(""body {  margin:0px; padding: 0px;width:100%;  height:100%;  background:url(\'"" + imglinks[c] + ""\') center center no-repeat;  background-size:contain;  overflow:hidden;  background-color:#121211;  position:relative;  font-family: sans-serif; } #title {  position:absolute;  width:100%;  min-height:60px; text-align:center; color:#FFF;   bottom:0px;v  left:0px;   background:rgba(0,0,0,0.6);  line-height:60px; align:middle; font-weight: bold; font-size: 125%; }"")
    				html_file.close()
    				
    				html_file = open(""bg-test.html"", ""w"")
    				html_file.write(""&lt;html&gt;&lt;head&gt;&lt;link rel=\""stylesheet\"" type=\""text/css\"" href=\""style.css\"" /&gt;&lt;meta http-equiv=\""refresh\"" content=\""65\""/&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\""title\""&gt;"" + tit + ""&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;"")
    				html_file.close()
    				
    				print imglinks[c]
    				c += 1
    				time.sleep(60)
    	time.sleep(1300)	
    
    
    
",,False,,t5_2qizd,False,,,True,t3_1iredp,http://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/,
1368476670.0,16,self.redditdev,1e9lwp,Proposal: robots.txt equivalent for reddit bots.,27,11,24,http://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/,"There ought to be a subreddit-level flag (and perhaps a user-level flag) to instruct bots not to engage on that subreddit (or with that user). Obviously this would only be effective for those bots that respected such a flag, but I believe something like this could reduce the amount the moderators need to fight with bots. There seem to be a lot of subreddits that have a ""we see em, we ban em"" attitude towards reddit bots: why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them?

Here's how I imagine this being implemented: 

1. The option to set this flag needs to be made available in subreddit settings. I think this should default to ""bots are ok here.""

2. The main reddit API wrappers (e.g. praw) should be set to respect this flag by default. Overriding this flag should be easy, but this way people just messing around with bots as hobby projects (which I assume is most of the population of praw users) will respect the flag by default.

EDIT: Some great ideas in here, but I'm thinking such a feature would be more trouble than it's worth and probably actually cause more work for everyone (mods, bot creators, and app developers). There's nothing really wrong with the current system of just banning bots when mods find them. Maybe if we work this idea through more we could come up with a better implementation, but I'm concerned that the options we've considered would make people's lives harder instead of easier. Maybe we should bring more mods into the discussion?",,False,,t5_2qizd,1368539728.0,,,True,t3_1e9lwp,http://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/,
1365618679.0,17,self.redditdev,1c2uvm,API change: Added 'likes' and 'link_title' attributes for messages,23,6,7,http://www.reddit.com/r/redditdev/comments/1c2uvm/api_change_added_likes_and_link_title_attributes/,"Just a couple small updates today, but they might be useful for anyone dealing with comment/post replies. [As requested by /u/ross456 last night](http://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/), I've added `likes` and `link_title` to ""messages"" that are actually comments in the API. These behave the same as they would for comments elsewhere - `link_title` is the title of the parent submission, and `likes` is whether the logged-in user has upvoted or downvoted that comment.",,False,,t5_2qizd,False,,,True,t3_1c2uvm,http://www.reddit.com/r/redditdev/comments/1c2uvm/api_change_added_likes_and_link_title_attributes/,admin
1363291088.0,19,reddit.com,1aayha,reddit @ PyCon 2013 [x-post /r/Python],20,1,1,http://www.reddit.com/r/redditdev/comments/1aayha/reddit_pycon_2013_xpost_rpython/,,,False,,t5_2qizd,False,,,False,t3_1aayha,http://www.reddit.com/r/Python/comments/1aayc4/reddit_pycon_2013/,admin
1340057661.0,18,self.redditdev,v8upy,Open Sorcerers: Install script overhauled and old files cleared out,24,6,3,http://www.reddit.com/r/redditdev/comments/v8upy/open_sorcerers_install_script_overhauled_and_old/,"The [reddit install script for Ubuntu](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) has been overhauled to use a new upstart-based init system for all of reddit. In addition to being cleaner and better mirroring how we run reddit in production these days, it also means that you can now install as a user other than `reddit` and things will work. 

As part of cleaning this up, we've removed all the old scripts in `scripts/` and daemontools runscripts in `run/`. If you're relying on these (particularly if you are symlinking the `srv` directories like the old install script did) you'll need to copy these files before merging up to the latest code. 

Please do give the new install system a try, it's [much better](http://youtu.be/I07xDdFMdgw) and make sure to let me know what problems you run into.",,False,,t5_2qizd,False,,,True,t3_v8upy,http://www.reddit.com/r/redditdev/comments/v8upy/open_sorcerers_install_script_overhauled_and_old/,admin
1337719188.0,18,self.redditdev,tztpg,API Users: New field added to subreddit JSON output and /api/site_admin input.,22,4,1,http://www.reddit.com/r/redditdev/comments/tztpg/api_users_new_field_added_to_subreddit_json/,"The new field is `public_description` and it is Markdown text. It appears in the Subreddit JSON output and is expected as input to [`/api/site_admin`](http://www.reddit.com/dev/api#POST_api_site_admin).

[See an explanation over in /r/changelog](http://www.reddit.com/r/changelog/comments/tztot/reddit_change_subreddits_now_have_a_public/)",,False,,t5_2qizd,False,,,True,t3_tztpg,http://www.reddit.com/r/redditdev/comments/tztpg/api_users_new_field_added_to_subreddit_json/,admin
1330657589.0,20,github.com,qdxhs,"To me, cake days are about giving back to the community in whatever way you can. With that in mind I just opensourced my JavaScript API wrapper on Github. Happy forking!",26,6,3,http://www.reddit.com/r/redditdev/comments/qdxhs/to_me_cake_days_are_about_giving_back_to_the/,,,False,,t5_2qizd,False,,,False,t3_qdxhs,https://github.com/CSobol/reddit.js,
1292009152.0,16,self.redditdev,ejp87,Request:  EC2 AMI (Amazon Machine Image) of Reddit,19,3,2,http://www.reddit.com/r/redditdev/comments/ejp87/request_ec2_ami_amazon_machine_image_of_reddit/,"Hello!

I am looking to assist with some of the Reddit development (in my spare time, and I have some ideas to kick around).

While setting up, and running a instance of Reddit is far from difficult, it would be extra time I'd have to spend (instead of just getting to the code).

Would it be possible for Reddit to officially release an AMI of a Reddit build?  Much like you did with the VM?

I've found [ami-56e81c3f]() which is an AMI of a Reddit build, but I don't believe it is official (and I've not run it) - so I am unsure of the state.  I'd feel more comfortable with an officially sanctioned release.

PS - I know you can convert a VM, and build an AMI from there, etc.  But it would be super easy to just spawn up an instance from EC2, and I doubt I am on the only one who would like to run a reddit build out there.",,False,,t5_2qizd,False,,,True,t3_ejp87,http://www.reddit.com/r/redditdev/comments/ejp87/request_ec2_ami_amazon_machine_image_of_reddit/,
1374062522.0,15,self.redditdev,1ihb8k,"Dear RedditTeam, do you guys make Reddit data dumps publicly available (like stackoverflow.com does)?",18,3,13,http://www.reddit.com/r/redditdev/comments/1ihb8k/dear_redditteam_do_you_guys_make_reddit_data/,,,False,,t5_2qizd,False,,,True,t3_1ihb8k,http://www.reddit.com/r/redditdev/comments/1ihb8k/dear_redditteam_do_you_guys_make_reddit_data/,
1368217585.0,17,self.redditdev,1e38x5,Minor API change: parent_id values on messages will now have the correct type prefix,23,6,1,http://www.reddit.com/r/redditdev/comments/1e38x5/minor_api_change_parent_id_values_on_messages/,"I noticed a small issue in the API the other day: All `parent_id` values in the various messages pages (inbox, unread, comment replies, etc.) were always starting with the `t4_` prefix, which means that the parent is a message. However, in the cases where the message was due to a comment/submission reply, this obviously isn't correct.

This has been fixed now, so `parent_id` will have the correct prefix depending on what the message was a reply to. I imagine most people were already disregarding this prefix since it wasn't reliable, but just wanted to let you know.

[See the code on github](https://github.com/reddit/reddit/commit/6211d82f7a5a30dcbb865a726d047c01f791c879)",,False,,t5_2qizd,False,,,True,t3_1e38x5,http://www.reddit.com/r/redditdev/comments/1e38x5/minor_api_change_parent_id_values_on_messages/,admin
1359844804.0,17,self.redditdev,17rxm1,srutils - Manipulate your subreddits from the command line,17,0,0,http://www.reddit.com/r/redditdev/comments/17rxm1/srutils_manipulate_your_subreddits_from_the/,"I've been working on a tool for a few days called srutils, or Subreddit Utilities. It's a CLI tool for manipulating subreddits. It's still pretty young, but I have two working utilities: reset and duplicate. The former allows you to reset your subreddit to the default settings and styles. The latter can copy any subreddit's style and settings into yours.

Say I want to reset /r/sircmpwn to default settings:

    srutil reset /r/sircmpwn all

This would reset all settings, styles, and the like on /r/sircmpwn. I can also make it fine grained, only resetting certain things:

    srutil reset /r/sircmpwn css,images,settings,sidebar,flair

If I wanted to copy /r/reddithax into /r/sircmpwn, I could get all the styles and settings and such like this:

    srutil dupe /r/reddithax /r/sircmpwn

You can get the code [here](https://github.com/SirCmpwn/srutils). The readme there describes how to use the tool, and has a link to binaries if you don't feel like compiling it yourself. Works on Windows, Linux and Mac.

What do you guys think? What other things could I add? I'm thinking of a backup/restore tool that creates a local copy of all the data needed to restore your subreddit styles/settings, but I don't have any other ideas at the moment.",,False,,t5_2qizd,False,,,True,t3_17rxm1,http://www.reddit.com/r/redditdev/comments/17rxm1/srutils_manipulate_your_subreddits_from_the/,
1357448577.0,16,self.redditdev,161odw,"Wrote a simple script in python using PRAW to download all my saved reddit links, thought I would share.",19,3,29,http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/,-,,False,,t5_2qizd,1367387705.0,,,True,t3_161odw,http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/,
1257733628.0,18,imgur.com,a2bh8,How does this even *happen*?,20,2,1,http://www.reddit.com/r/redditdev/comments/a2bh8/how_does_this_even_happen/,,,False,,t5_2qizd,False,,,False,t3_a2bh8,http://imgur.com/CZWPN.png,
1375585447.0,16,self.redditdev,1jnthi,"GoReddit, a Reddit client/library for Go",17,1,2,http://www.reddit.com/r/redditdev/comments/1jnthi/goreddit_a_reddit_clientlibrary_for_go/,"Hey all,

Pardon the self-promotion, but I'd like to announce [GoReddit](https://github.com/kz26/GoReddit), a Golang Reddit client/library that I'm working on. Obviously it's in a very early stage of development, but right now it's capable of getting a listing of posts from a subreddit, getting a listing of comments, and voting. All API calls go through a custom HTTP client that respects the guideline of waiting 2 seconds between requests.

I'll be using GoReddit for a bot that I'm writing, but I'd also like the library to be useful to others interested in writing a bot in Go or something. With that said, what features do you find most important for a Reddit API client?",,False,,t5_2qizd,1375585879.0,,,True,t3_1jnthi,http://www.reddit.com/r/redditdev/comments/1jnthi/goreddit_a_reddit_clientlibrary_for_go/,
1362613240.0,17,self.redditdev,19t48f,"Learning reddit's code, Journal #2: Admin status (one way or the other)",17,0,7,http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/,"This is continuation of http://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/. I'm posting as I learn--to help others and to get corrections when I ""learn"" the wrong things.

(Oh, and I switched from /u/foolblog to /u/fool_blog because ""fool_blog"" is easier to read and *^i ^kinda ^forgot ^the ^password, ^oopsie* .)

----

In Journal #1, I got a local reddit started and running on a Ubuntu 12.04 system. *Everything here assumes you're running Ubuntu 12.04.* 

Now, the quest for admin powers begins.

----

**Make Me Admin!**

By default, reddit has one admin ~~account~~ not-actually-an-account-yet. The first person to create an account named ""reddit"" is the admin.  Yikes! 

Let's change that.

Login to your local reddit.  Create two accounts.  I'm using ""fool"" and ""fool2"".

Open /home/reddit/reddit/r2/development.update in a text editor and add one line:

    admins = fool, fool2

Save the file. Open a command line terminal and:

    cd /home/reddit/reddit/r2
    sudo make

Boom!  Your two accounts are admin accounts.  ""reddit"" is no longer an admin.

However, there are still major problems--admin account have very limited powers until you verify email accounts, and I can't figure out how to get the local reddit to send out confirmation e-mails. 

That's okay. I'm gonna cheat on this one.

-----

**I'm gonna hack my database.**

**Disclaimer:** *There's a high chance that doing things this way is extremely naughty.  With any luck, somebody will post corrections.*

Reddit saves important stuff in a PostreSQL database.  

* Step 1: Set the postgres password. 

I used the instructions at http://library.linode.com/databases/postgresql/ubuntu-12.04-precise-pangolin#sph_set-the-postgres-user-s-password

* Step 2: Run the pgAdmin III GUI.

If you don't have it, get it:
    sudo apt-get install pgadmin3
Once you do have it, run it.

* Step 3: Add the local server.

1. Click on File -&gt; Add Server...

Fill in the blanks on the Properties tab:

&gt; Name: reddit [or something else, as long as it is not blank]

&gt; Host: localhost

&gt; (Port should be 5432 by default.)

&gt; (Maintence DB and Username should be ""postgres"" by default.)

&gt; Password: [Whatever password you used in Step 1.]

Now click OK to open the reddit database.

* Step 4: Commence database hacking:

In the object browser, open the tree: 

&gt;  reddit (localhost: 5432) 

&gt;  Databases(2)

&gt;  reddit

&gt;  Schemas

&gt;  public

&gt;  Tables

&gt;  reddit_data_account

Select reddit_data_account and press Ctrl-D to open the data in a new window.

Now find your admin accounts.  You're looking for a row like:

    3  |  name | fool | str

3 is the id number for the account with the name of ""fool"".  (This number may vary.  The important thing is that the 2nd column is ""name"" and the third column has the name of the admin account.)

I'm going to hack fool into having a verified email account.

I add the following rows to reddit_data_account.  I use ""3"" in column one because that's the account ID number for /u/fool .

    3 | email          | fake@fakefake.com | str
    3 | email_verified | t                 | bool

Fool2 needs email verfied too.  The row for fool2's name has ""4"" in the thing_id column, so I'll add two rows for him, too.

    4 | email          | fakest@fakefake.com | str
    4 | email_verified | t                   | bool

Now, since doing things this way is **probably very naughty**, I seem to have messed up reddit's cache system.  Alas, since I don't know how to fix that the easy way.  However, rebooting the computer forces the caches to update.  

One reboot sequence later and the hack is complete. The caches have been forced to refresh, and the local reddit is convinced that the email addresses were verified. I can use the admin accounts to do admin type stuff.

*And, yeah, this is the cue for people to tell me how clueless I am and how to do this sort of thing without a reboot.*
",,False,,t5_2qizd,1362629037.0,,,True,t3_19t48f,http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/,
1356111974.0,18,github.com,158jbq,"I'll just leave this here... (Thanks for your help, guys!)",21,3,4,http://www.reddit.com/r/redditdev/comments/158jbq/ill_just_leave_this_here_thanks_for_your_help_guys/,,,False,,t5_2qizd,False,,,False,t3_158jbq,https://github.com/chris3000/reddit_leavebot,
1347917493.0,18,self.redditdev,101kru,[api notice] Make sure you're sending a modhash with /api/submit.,23,5,4,http://www.reddit.com/r/redditdev/comments/101kru/api_notice_make_sure_youre_sending_a_modhash_with/,"We now require a ""modhash"" `uh=` parameter in `/api/submit`, similar to other POSTs in the reddit API. This is necessary to prevent [CSRF](http://en.wikipedia.org/wiki/Cross-site_request_forgery) attacks.

According to logs, the number of API clients making submit calls without a modhash is very low. However, if your bot/client is affected, you can easily request a modhash by GETing http://www.reddit.com/api/me.json.",,False,,t5_2qizd,False,,,True,t3_101kru,http://www.reddit.com/r/redditdev/comments/101kru/api_notice_make_sure_youre_sending_a_modhash_with/,admin
1326585833.0,16,self.redditdev,ohcrn,Who's in charge of the iReddit github repo?,16,0,3,http://www.reddit.com/r/redditdev/comments/ohcrn/whos_in_charge_of_the_ireddit_github_repo/,"Currently there are 2 pull requests. One is by me, and it fixes the issue with the git submodule command not working. The other pull request fixes the ""shaking the iphone when off changes the story"" bug, and that was submitted about a year ago.   
  
  
  
So my question is:is anyone actively checking that repo? Some of us don't have commit permissions and it looks like no one has updated the code in quite some time.  
  
EDIT: spladug was kind enough to respond on IRC and merged in my patch. They need more ObjC devs though, as no one who knows ObjC  is able to review the pull requests!  (Also, is anyone able to get the current branch to build?)",,False,,t5_2qizd,True,,,True,t3_ohcrn,http://www.reddit.com/r/redditdev/comments/ohcrn/whos_in_charge_of_the_ireddit_github_repo/,
1288115682.0,15,self.redditdev,dwqcx,iReddit-related question for any iPhone programmers,16,1,8,http://www.reddit.com/r/redditdev/comments/dwqcx/iredditrelated_question_for_any_iphone_programmers/,"I submitted the version of iReddit that's up on github to Apple, and they replied:

&gt; Dear CondeNet, Thank you for submitting iReddit to the App Store. iReddit cannot be posted to the App Store because it is using private or undocumented APIs: 

&gt;     Private Symbol References 
&gt;     OBJC_IVAR_$_UITouch._locationInWindow
&gt;     OBJC_IVAR_$_UITouch._phase
&gt;     OBJC_IVAR_$_UITouch._previousLocationInWindow
&gt;     OBJC_IVAR_$_UITouch._tapCount
&gt;     OBJC_IVAR_$_UITouch._timestamp
&gt;     OBJC_IVAR_$_UITouch._touchFlags
&gt;     OBJC_IVAR_$_UITouch._view
&gt;     OBJC_IVAR_$_UITouch._window

&gt; As you know, as outlined in the iPhone Developer Program License Agreement section 3.3.1, the use of non-public APIs is not permitted. Before your application can be reviewed by the App Review Team, please resolve this issue and upload a new binary to iTunes Connect. Sincerely, iPhone Developer Program       

Any ideas why this would be true? Does Three20 use these APIs, perhaps?",,False,,t5_2qizd,False,,,True,t3_dwqcx,http://www.reddit.com/r/redditdev/comments/dwqcx/iredditrelated_question_for_any_iphone_programmers/,
1286732852.0,14,rubygems.org,dpdfa,"Hello redditdev, I made a ruby wrapper for the reddit API",19,5,0,http://www.reddit.com/r/redditdev/comments/dpdfa/hello_redditdev_i_made_a_ruby_wrapper_for_the/,,,False,,t5_2qizd,False,,,False,t3_dpdfa,https://rubygems.org/gems/ruby_reddit_api,
1280406224.0,17,github.com,cv1or,"Hey /r/redditdev, I've made some bug fixes to iReddit and added Retina Display compatitbility. [github]",19,2,8,http://www.reddit.com/r/redditdev/comments/cv1or/hey_rredditdev_ive_made_some_bug_fixes_to_ireddit/,,,False,,t5_2qizd,False,,,False,t3_cv1or,http://github.com/alastairstuart/iReddit,
1214548481.0,15,reddit.com,6p7bb,User pages don't work on certain people,17,2,2,http://www.reddit.com/r/redditdev/comments/6p7bb/user_pages_dont_work_on_certain_people/,,,False,,t5_2qizd,False,,,False,t3_6p7bb,http://www.reddit.com/info/6p6al/comments/c04hwn1,
1371339298.0,16,self.redditdev,1gfbyf,How to write a summon bot code?,16,0,32,http://www.reddit.com/r/redditdev/comments/1gfbyf/how_to_write_a_summon_bot_code/,"I'm looking to build a reddit bot that can be summoned like BTCTip bot or Jiffybot. Basically, a user enters ""!message"" or ""+message"", and the bot automatically sends a reply to the comment. ",,False,,t5_2qizd,False,,,True,t3_1gfbyf,http://www.reddit.com/r/redditdev/comments/1gfbyf/how_to_write_a_summon_bot_code/,
1365064713.0,17,self.redditdev,1bney1,I want to add an API call to read a user's subreddit karma,20,3,18,http://www.reddit.com/r/redditdev/comments/1bney1/i_want_to_add_an_api_call_to_read_a_users/,"After [this discussion](http://www.reddit.com/r/ideasfortheadmins/comments/1blpok/an_option_to_restrict_posting_based_on_subreddit/) in /r/ideasfortheadmins, I've decided to start work on adding an API call that will return a user's karma for a particular subreddit.  This would be the same value that appears under 'show karma breakdown by subreddit' for the currently-logged-in user, but only for a specific subreddit so as not to be too expensive.

My questions at this point are:

* whether to base it off of [/user/username/about.json](http://www.reddit.com/dev/api#GET_user_{username}_about.json), [/r/subreddit/about.json](http://www.reddit.com/dev/api#GET_r_{subreddit}_about.json) or somewhere else
* whether there is some technical reason I'm not yet aware of that makes this infeasible
* whether there is some political reason that makes it unlikely a pull request with this feature would be accepted.
* if there's any other resources I should know about for help with what I'm planning,",,False,,t5_2qizd,False,,,True,t3_1bney1,http://www.reddit.com/r/redditdev/comments/1bney1/i_want_to_add_an_api_call_to_read_a_users/,
1336487524.0,17,self.redditdev,tcypp,Can I use the reddit name and logo in an iPhone app?,18,1,8,http://www.reddit.com/r/redditdev/comments/tcypp/can_i_use_the_reddit_name_and_logo_in_an_iphone/,"I have an idea for a reddit iPhone app.  What are the rules for using the reddit name and logo?  Part of the app will involve the user logging into their reddit account.

* Can I use reddit in the name of the app?
* Can I make references to reddit in the app (e.g., ""Please enter your reddit username and password""
* Can I use the reddit logo?
* Does it matter if the app is free or paid?

I've seen the licensing page (http://www.reddit.com/help/licensing), but it seems to be related specifically to selling products.",,False,,t5_2qizd,False,,,True,t3_tcypp,http://www.reddit.com/r/redditdev/comments/tcypp/can_i_use_the_reddit_name_and_logo_in_an_iphone/,
1298494644.0,15,github.com,frasm,February 2011 Merge,15,0,13,http://www.reddit.com/r/redditdev/comments/frasm/february_2011_merge/,,,False,,t5_2qizd,False,,,False,t3_frasm,https://github.com/reddit/reddit/commit/7fff900bbeba362b607821159f6419d7762c9957,
1371538448.0,15,self.redditdev,1gkkzo,Cool Tip for debugging without database,16,1,13,http://www.reddit.com/r/redditdev/comments/1gkkzo/cool_tip_for_debugging_without_database/,"My latest bot ran once a minute, and when it malfunctioned I needed to know why. So i left a debug string in each comment reply. The debug string could not be seen by any normal viewer unless they actually clicked Source.

There are methods to hide text in posts unless clicking ""Source"". The methods are:

###16 SUP
Using 16 **^** the text following disappears and won't even produce a new line if its on a new line. Viewing source now will show you what I mean.

^^^^^^^^^^^^^^^^Text-Here

As you can see, even though I have 2 new lines in the source, you only visually see one.

###16 Quotes
Same method as above but using **&gt;**
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Hello There

^- ^/u/TimePath

###New Line, Quote, Text
Putting a blank new line, then entering and putting |, then entering and putting text will make the text disappear. View source to see.

|
This Text can have spaces, unlike the other one.

After another new line the text shows up again.

###Empty Link
Using an empty link you can then stylesheet the text so  its visible on your end and no need to click view source

[](/debug ""Like this"")

    [](/debug ""This is some debug text"")
&gt;
&gt;With a stylesheet like this:
&gt;
&gt;a[href$=""/debug""]:before {

&gt;content: attr(title);

&gt;color: red;

&gt;cursor: pointer;

&gt;text-decoration: none;

&gt;}

^- ^/u/AsterJ",,False,,t5_2qizd,1371597355.0,,,True,t3_1gkkzo,http://www.reddit.com/r/redditdev/comments/1gkkzo/cool_tip_for_debugging_without_database/,
1369367990.0,13,self.redditdev,1ey9l1,[Bot Help] - Virtual Currency (ChromaMarket),19,6,23,http://www.reddit.com/r/redditdev/comments/1ey9l1/bot_help_virtual_currency_chromamarket/,"###Note:
First things first. I am a noob at this but Deimos asked me to enquire here. Hope this helps! :) I think this post can go here, right?

---

###What is ChromaMarket?
So along with a few others, I am a mod at **/r/ChromaMarket**, which is a place where we do some **virtual currency stuff**. We buy/sell virtual goods and everybody starts with a set amount of money. However, recently, we have gotten around **30 active users** that frequently use the system and this is set to go up. Thus, we decided to find someone who could **help us with a bot**.

---

###About What We Need:
We need a bot that can **read modmails**, and complete transactions that take place via modmail. These transactions only include **buying and selling**. As such, the bot would need to **calculate the money** after the purchase/sale and enter that info into our **subreddit wiki page** if possible. For buying, the bot would need to **add** items to the list in the wiki page and for selling, the bot would need to **remove** items from the list.

---

###More Info:

- We would like this to be done **asap**
- Anyone who **can help**, please comment below

---

**THANKS!**

",,False,,t5_2qizd,1369388358.0,,,True,t3_1ey9l1,http://www.reddit.com/r/redditdev/comments/1ey9l1/bot_help_virtual_currency_chromamarket/,
1361310036.0,13,self.redditdev,18ucm2,"I need a bot, where do I start?",16,3,13,http://www.reddit.com/r/redditdev/comments/18ucm2/i_need_a_bot_where_do_i_start/,"Okay, so I run /r/Nerdcubed and I want a bot that will post a link to a video from two particular users when they're uploaded to Youtube (With the write title and whatnot). Now, I know some very basic coding but for all intensive purposes I'm a newbie. 

So, where do I start? How do I go about making this? Any help would be much appreciated. :p",,False,,t5_2qizd,False,,,True,t3_18ucm2,http://www.reddit.com/r/redditdev/comments/18ucm2/i_need_a_bot_where_do_i_start/,
1358641455.0,14,self.redditdev,16wiwe,Why does the OAuth API suck? I can't even get a users frontpage.,18,4,7,http://www.reddit.com/r/redditdev/comments/16wiwe/why_does_the_oauth_api_suck_i_cant_even_get_a/,"I'm sorry if the title may seemed offensive, but I really need your attention. Please hear me out before getting mad.  
I've been urging myself to use the OAuth API and I love how it works. It's a lot more easy to just have a user press a button instead of making them enter their credentials on a third-party site.  
With that having said it has come to my attention that the OAuth API is really limited, It's in no way as good as the normal API.  
For example the front page (.json / hot.json) only gets the public frontpage.  
I can update a reddit's stylesheet but not get prevstylesheet (trough /about/stylesheet.json) which makes it impossible to update it at all.  
There's just tons and tons of other stuff like this which makes using it quite impossible.  
Coming back to the title, In order to try to get the users frontpage I now have to get all of their subreddits (2 requests, and then I'm still not sure if I got all of his subreddits) and then try something like /r/subreddit1+subreddit2+etc.json which costs a lot of - unnessary - server load for us both.  
There also have been a few project I just completely had to cancel because of these limitations.  
There's just so many things missing compared to the normal API and as much as I love the work you guys have done on it so far, I'm really urging you to continue working on it because it's unusable for me at the moment because of it's limitations.  
/rant",,False,,t5_2qizd,1358641862.0,,,True,t3_16wiwe,http://www.reddit.com/r/redditdev/comments/16wiwe/why_does_the_oauth_api_suck_i_cant_even_get_a/,
1356681257.0,15,self.redditdev,15kc2j,"Embarrased I need the ""Hello World"" of API",17,2,3,http://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/,"Honestly this new fangled API structure is a bit foreign to me but I am learning some. However, I have hit a roadblock that is certain to ruin all my programmer cred.

I program in python and have gotten OAuth working. I am getting a access_token. Now I feel stupid. I have not the foggiest how to make an API call through PRAW or anything else that uses that access_token. I am simply trying to find the username (eventually some other things but not now) of the person who got the access_token. It really must be something so easy that I am missing that there isn't even a guide.

import requests

url=""https://ssl.reddit.com/api/me.json?""

data= {

    'access_token': 'access_token',

    'scope' : 'http://www.my.url'

    }

print requests.get(url, data=data).text


This gets me an empty JSON response {} but, HTTP return of 200. . .I dunno",,False,,t5_2qizd,1356684982.0,,,True,t3_15kc2j,http://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/,
1338108451.0,15,venus.xelio.info,u73kt,Gave the Reddit search engine I created a trial run with /r/askreddit.  Tested for 1 million hits per day off a laptop.  ,20,5,3,http://www.reddit.com/r/redditdev/comments/u73kt/gave_the_reddit_search_engine_i_created_a_trial/,,,False,,t5_2qizd,False,,,False,t3_u73kt,http://venus.xelio.info/redditsearch/,
1335372834.0,16,self.redditdev,srzof,"Hey /r/redditdev, I'm looking to help some folks out with the reddit API, working in Python. If you're interested in learning the basics of it, read on.",19,3,9,http://www.reddit.com/r/redditdev/comments/srzof/hey_rredditdev_im_looking_to_help_some_folks_out/,"If you are just looking to interact  with the API easily, check out  [bboe and mellort's excellent Python reddit API wrapper](https://github.com/mellort/reddit_api). I'm just teaching how to do very basic things. That wrapper is a very complete tool.

----

A few people have asked for my help, when I posted in that thread a few days ago, so I figured there might be a demand for personalized help.

So, if you're friendly, willing to listen, and do, what I tell you (exercises) send me your email in a PM, after you post a comment in this thread, and I'll walk you through the basics of logging in, submitting a story, getting links from a subreddit, and deleting your posts, among other things.

Requirements:

* Python 2.x or 3.x
* Modules to use: `requests`, `json`, `pprint`, `time` or `datetime`
* Friendliness (I'm not going to waste time with dicks, looking to spam reddit or something)
* Basic Google-fu, ability to search Stack Overflow
* Willingness to listen to critique, should the need arise.

Bonus:

* Google Talk/Chat if there's ever an issue you can't solve on your own
* Creepy pics of forests. I just like looking at them.
* A subreddit you can 'spam' with test posts. I use my [/r/tankorsmash](/r/tankorsmash) for this. It only takes a minute to create a subreddit.


My motivation is that I'm going to write a generic tutorial for working with Python and the reddit API, and I'd like to get a feel for what people have trouble with. I only expect a few people to respond to this, I can't handle more than that! ;)

edit: For fun, here's my twitter, [@TankorSmash](https://twitter.com/#!/TankorSmash), and [blog](http://www.tankorsmash.com) if you need to contact me outside of reddit for more questions",,False,,t5_2qizd,True,,,True,t3_srzof,http://www.reddit.com/r/redditdev/comments/srzof/hey_rredditdev_im_looking_to_help_some_folks_out/,
1324677289.0,13,self.redditdev,noe4o,The moderator log does not register moderator actions performed through a script.,17,4,9,http://www.reddit.com/r/redditdev/comments/noe4o/the_moderator_log_does_not_register_moderator/,"Not sure if this is intentional, but I've noticed that the moderator log only registers moderators that I've added by hand, and not ones that I've added through a python script using the API.",,False,,t5_2qizd,True,,,True,t3_noe4o,http://www.reddit.com/r/redditdev/comments/noe4o/the_moderator_log_does_not_register_moderator/,
1307040876.0,15,self.redditdev,hq4g9,"When you say developed in Python, what does exactly mean?",28,13,31,http://www.reddit.com/r/redditdev/comments/hq4g9/when_you_say_developed_in_python_what_does/,"I don't know that much about programming, but I was under the impression when programming for the internet/web you need to use scripting languages and such like php, asp, xhtml, html, xml, ajax, javascript, and what not. How exactly does python fit in to all of this?",,False,,t5_2qizd,True,,,True,t3_hq4g9,http://www.reddit.com/r/redditdev/comments/hq4g9/when_you_say_developed_in_python_what_does/,
1282782924.0,14,self.redditdev,d5hyr,Does anyone here have experience working with PayPal and its IPN system?,17,3,20,http://www.reddit.com/r/redditdev/comments/d5hyr/does_anyone_here_have_experience_working_with/,"Right now, when someone wants to sign up for reddit gold, we send them off to [/help/gold](/help/gold), where we have a static HTML form that sends them off to a PayPal ""subscribe"" or ""buy now"" session (depending on whether they use the auto-renewing option or not).

Then, they finish paying, PayPal sends us an IPN with their email address, and we mail them a claim code that they have to enter in at [/thanks](/thanks) while logged in to reddit.

It's an annoying step that confuses people and leads to the vast majority of our customer support requests.

What I'd prefer to do is this: when a logged-in user goes to pay for gold, send them off to PayPal with their username stored in a hidden (or overt) parameter. Then, when they pay, the IPN notification has their username and we can simply credit their reddit account, rather than having to send them an email with a claim code.

Unfortunately, PayPal's documentation is complicated and confusing to me, and I'm not getting very far. Does anyone here happen to have experience with this?",,False,,t5_2qizd,False,,,True,t3_d5hyr,http://www.reddit.com/r/redditdev/comments/d5hyr/does_anyone_here_have_experience_working_with/,
1376247174.0,13,self.redditdev,1k5o3s,Look I've found in robots.txt - is this a joke?,24,11,9,http://www.reddit.com/r/redditdev/comments/1k5o3s/look_ive_found_in_robotstxt_is_this_a_joke/,"http://www.reddit.com/robots.txt

&gt;
User-Agent: bender
Disallow: /my_shiny_metal_ass

&gt;User-Agent: Gort
Disallow: /earth

Is this Easter egg or something?",,False,,t5_2qizd,False,,,True,t3_1k5o3s,http://www.reddit.com/r/redditdev/comments/1k5o3s/look_ive_found_in_robotstxt_is_this_a_joke/,
1376218090.0,12,self.redditdev,1k506f,Is there any popular websites based on the reddit source code?,17,5,11,http://www.reddit.com/r/redditdev/comments/1k506f/is_there_any_popular_websites_based_on_the_reddit/,I'm just wondering because it would be interesting to see it used somewhere else.,,False,,t5_2qizd,False,,,True,t3_1k506f,http://www.reddit.com/r/redditdev/comments/1k506f/is_there_any_popular_websites_based_on_the_reddit/,
1374641360.0,12,self.redditdev,1ixqu0,[PRAW] Where do you all host your python-based bots?,13,1,24,http://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/,Do you guys run your PRAW bots on a VPS or heroku or...?,,False,,t5_2qizd,False,,,True,t3_1ixqu0,http://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/,
1361458871.0,12,self.redditdev,18yfg7,I'm working on a prototype (w/o functionality yet) of a revamp of the modmail UI.,14,2,2,http://www.reddit.com/r/redditdev/comments/18yfg7/im_working_on_a_prototype_wo_functionality_yet_of/,"Now, this is a really REALLY rough sketch, so don't worry, I'm not an idiot who expects the modmail to exactly look like this. It's just an example of how it could be more convenient, possibly. I'm still gonna work on making the borders look better and such, but for now I just wanted to make the idea a bit clear.

http://redditstory.zxq.net/index.html

Only the link to \/r/braveryjerk shows.

______

I personally think having an UI like this, where you can more easily pick the modmail thread you want to read would be a great improvement. It might also save the modmail a lot of lag, when threads get really big. (No other threads would be loaded.)


",,False,,t5_2qizd,False,,,True,t3_18yfg7,http://www.reddit.com/r/redditdev/comments/18yfg7/im_working_on_a_prototype_wo_functionality_yet_of/,
1358404439.0,11,github.com,16qmq2,PRAW 2.0 with reddit OAuth2 support has been released!,14,3,0,http://www.reddit.com/r/redditdev/comments/16qmq2/praw_20_with_reddit_oauth2_support_has_been/,,,False,,t5_2qizd,False,,,False,t3_16qmq2,https://github.com/praw-dev/praw/wiki/Changelog,
1355980167.0,12,self.redditdev,155hs3,recent problem with ssl-enabled reddit,16,4,32,http://www.reddit.com/r/redditdev/comments/155hs3/recent_problem_with_sslenabled_reddit/,"recently, any time i try to use ssl-enabled reddit ive been getting ""Service Unavailable  The server is temporarily unable to service your request. Please try again later."" with a reference code.

i havent changed anything on my end, and its happening in multiple browsers.  going to ssl.reddit.come gets me a message about being a bad robot and a link to the api.

has reddit made some ssl-related changes that anyones aware of?",,False,,t5_2qizd,False,,,True,t3_155hs3,http://www.reddit.com/r/redditdev/comments/155hs3/recent_problem_with_sslenabled_reddit/,
1355003877.0,11,github.com,14ili4,Code for the now-defunct ClockStalker bot now available on GitHub,18,7,8,http://www.reddit.com/r/redditdev/comments/14ili4/code_for_the_nowdefunct_clockstalker_bot_now/,,,False,,t5_2qizd,False,,,False,t3_14ili4,https://github.com/ClockStalker/clockstalker,
1343274223.0,14,self.redditdev,x666h,Why doesn't Reddit denormalize all votes a user has made?,16,2,4,http://www.reddit.com/r/redditdev/comments/x666h/why_doesnt_reddit_denormalize_all_votes_a_user/,"I've looked at the source code, and it seems from [this method](https://github.com/reddit/reddit/blob/master/r2/r2/models/vote.py#L275) (not sure if this the only way that votes are queried) that a database `_fast_query` is used to get all of a user's votes for a set of ""things."" Was there a design decision made to offload these queries to the database every time a user loads a page, instead of caching all votes a user has made in a single key-value pair per user, and selecting from this list in the frontend servers? My intuition would be that if the database is the bottleneck, the latter choice might make page loads quicker. Or is this just premature optimization?",,False,,t5_2qizd,False,,,True,t3_x666h,http://www.reddit.com/r/redditdev/comments/x666h/why_doesnt_reddit_denormalize_all_votes_a_user/,
1338999558.0,11,self.redditdev,uo4qy,Any hope for a push API soon?,16,5,0,http://www.reddit.com/r/redditdev/comments/uo4qy/any_hope_for_a_push_api_soon/,"I was thinking that since reddit is becoming such a bigger and bigger deal, it would be nice to have for messages or what not.",,False,,t5_2qizd,False,,,True,t3_uo4qy,http://www.reddit.com/r/redditdev/comments/uo4qy/any_hope_for_a_push_api_soon/,
1335812978.0,12,self.redditdev,t067f,Would it be irresponsible of me to open source my IAMA bot?,23,11,7,http://www.reddit.com/r/redditdev/comments/t067f/would_it_be_irresponsible_of_me_to_open_source_my/,"The bot ([narwal_bot](http://www.reddit.com/user/narwal_bot)) aggregates an IAMA host's comments, formats them into an easy-to-read list, and posts the compilation into the IAMA as a chain of comments.  It does this for every ""hot"" IAMA that has (or will likely have) over 200 comments.  I have it set to run every hour.

So I want to open source it for the usual reasons, but I'm hesitant that it'll be a cause of spam, since having more than one person run the script would most definitely be annoying.  Please let me know your thoughts and opinions.

(Aside: I wrote the bot primarily to build up karma for my narwal_bot account, which I intend to use to test POST functions of [narwal](https://github.com/larryng/narwal), my open source python reddit API wrapper.  I debuted narwal [two weeks ago in r/programming](http://www.reddit.com/r/programming/comments/sczy5/narwal_a_simple_and_concise_python_wrapper_for/), not knowing this subreddit exists.  I hope you guys check it out and find it useful in your own projects.  Note that I still consider it in alpha stages since I haven't created a test suite to test POSTs yet.)

Edit: bad link",,False,,t5_2qizd,False,,,True,t3_t067f,http://www.reddit.com/r/redditdev/comments/t067f/would_it_be_irresponsible_of_me_to_open_source_my/,
1325642369.0,13,self.redditdev,o1w7b,How do I get more comments in JSON format?,13,0,11,http://www.reddit.com/r/redditdev/comments/o1w7b/how_do_i_get_more_comments_in_json_format/,"I'm looking at the reddit API and I can't seem to figure out how to get more comments from a post after the initial 50. The text below is from the reddit API:


*Fetching more*

*If you're fetching comments from a thread with more comments than the API will return in a single response, the last comment will look like this:*

*{'data': {'id': 'abc1010', 'name': 't1_abc1010'}, 'kind': 'more'}*

*To get these comments, you can fetch the url http://reddit.com/comments/FULLNAME/abc1010.json, where FULLNAME is the FULLNAME of the story.*


To use as an example for my problem, when I get the json from:
http://www.reddit.com/r/funny/comments/o0pk5/monopoly_is_an_old_game/.json

I am able to see the first 50 parent comments. At the end of the JSON Array for these comments, there is another item labeled ""more"" which has an id of ""c3dibkp"". 

So according to the API, I should be able to fetch more comments by accessing:

http://reddit.com/comments/FULLNAME/c3dibkp.json

I'm guessing that FULLNAME should be equal to the id of the original story, which is ""o0pk5"".

However I cannot seem to get the correct results using:
http://reddit.com/comments/o0pk5/c3dibkp.json

What am I doing wrong?
",,False,,t5_2qizd,False,,,True,t3_o1w7b,http://www.reddit.com/r/redditdev/comments/o1w7b/how_do_i_get_more_comments_in_json_format/,
1323324187.0,12,self.redditdev,n4i9b,Question about Reddit's Thing-&gt;Data database design,14,2,5,http://www.reddit.com/r/redditdev/comments/n4i9b/question_about_reddits_thingdata_database_design/,"I was watching this video: [http://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/](http://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/) and at 9:40 (about Lesson 3) Steve talks about how data is stored in a non-relational way.  This ideology really intrigued me as it seems like the perfect way to eliminate schema changes.  But I do have some questions about it hopefully someone more experienced than I can answer:

1.  He says there are no more joins using this design. Does he mean that completely? Wouldn't it make sense to join the Things table to the Data table on ThingID?

2.  I related this to objects in my mind where the Things table could be renamed the Objects table and the Data table holds all the object's fields/members/attributes.  Also, to make searches faster, you would need to index the Type column in Things, but indexing varchars is inefficient so you could instead index TypeID (integer) to improve performance.  This would require a 3rd table to map TypeID to it's text name.  This table could be called Classes, so our structure would go Class-&gt;Object-&gt;Data.  My question is, is that a more efficient way of doing this, and is it OK or a bad idea to join these three tables in each query, where you say need to select all objects of class User (for example)?

3.  Obviously since this structure uses only 2 (or 3 if I make my Classes table) tables these tables (and their indices) are going to get monstrous.  What performance issues are there with tables so big? I'm pretty sure there is a max table size (even if it's the OS's maximum file size) so how do you split these up? When they do get split up, how do you determine what goes where (obviously it's up to the developer but what is the most optimal solution)?  And when things are split up across different boxes, how do you know where to look later?

4.  Is MySQL a suitable platform for this type of database? If so, what database engine would be most efficient?

I'm sorry for such wordy questions, I'm still turning the concept over in my mind.  It seems like such a simple solution to a complex problem but I worry about its limitations.  Million thanks to whoever helps shed some light on this.",,False,,t5_2qizd,False,,,True,t3_n4i9b,http://www.reddit.com/r/redditdev/comments/n4i9b/question_about_reddits_thingdata_database_design/,
1285892093.0,14,self.redditdev,dl9e2,Thoughts: How can we best approach handling comments in the 1000s? ,17,3,42,http://www.reddit.com/r/redditdev/comments/dl9e2/thoughts_how_can_we_best_approach_handling/,"I believe normally, the max is 500 comments to load at a time and for Gold members it is 1000 comments to load at a time. Then you could hit load more comments the bottom of the page. 

Navigating comment threads that are this large can be difficult from a user-interface stand point as well as far as server load. With a 1000 comments, it can take several minutes to load a page. For a self-post, like for IAMA posts, it can be extremely difficult to manage, especially just going through the inbox. 

So I wanted to hear your thoughts if there are steps we could take, either from an interface standpoint or loading time, that we could try to alleviate the concern. 

I believe as Reddit gets more traffic, these popular posts could easily hit the 10,000 mark. 

Maybe we could make an outside application, like a control panel, to aid the management of self.posts for the OP? Maybe we just need some CSS or a greasemonkey script from the user-side? Or maybe we need to edit some code? What about a new sort-method for comments (like with Best, New, Controversial, Top)? I don't really have any bright ideas at the moment. 


Edit: We might need to make some visual examples with some perf-testing to show how some of our ideas could work. Don't want to re-invent the wheel. 

&gt; &lt;@ketralnis&gt; Just a Listing(CommentsBuilder).render() in a loop

Also, the code in question:

&gt; * http://code.reddit.com/browser/r2/r2/models/_builder.pyx (CommentsBuilder)
&gt; * http://code.reddit.com/browser/r2/r2/controllers/api.py#L1554 (POST_morechildren)
&gt; * http://code.reddit.com/browser/r2/r2/lib/comment_tree.py

Edit2: This thread may be a good playground to come up with ideas and test (11,555 comments): 

http://www.reddit.com/r/blog/comments/d14xg/everyone_on_team_reddit_would_like_to_raise_a/",,False,,t5_2qizd,True,,,True,t3_dl9e2,http://www.reddit.com/r/redditdev/comments/dl9e2/thoughts_how_can_we_best_approach_handling/,
1251686608.0,13,self.redditdev,9fpco,Noob question: how do you join on so much data? ,13,0,16,http://www.reddit.com/r/redditdev/comments/9fpco/noob_question_how_do_you_join_on_so_much_data/,"When I work with web apps, one page can have a couple of queries with a couple of joins.

But with reddit, it seems like there is a mountain of information retrieved per user.  How is it done?

Is it done at the query level or mostly done by the application?

For example, just looking at the most visible stuff:

1. Per user, get the subreddits?
2. Per user, get the top links per the user selected subreddits
3. Have the ticker bar populate with the top links per the subreddits
4. Have the user selected last view links
5. Highlight the friends
6. Show the user's karma
7. Show the user's messages

Does some have a quick overview how reddit can do this?

",,False,,t5_2qizd,False,,,True,t3_9fpco,http://www.reddit.com/r/redditdev/comments/9fpco/noob_question_how_do_you_join_on_so_much_data/,
1374692309.0,11,self.redditdev,1iz2qe,API change: /by_id/ will no longer 404 if a single link ID is invalid,13,2,2,http://www.reddit.com/r/redditdev/comments/1iz2qe/api_change_by_id_will_no_longer_404_if_a_single/,"This one goes out to /u/radd_it, who I think may have smashed multiple innocent things due to it in the past. Once again, brought to you by /u/slyf:

---

/by_id will now only 404 if all links were not found. I noticed some users pointing out our inconsistent and counter intuitive behavior of /by_id. /by_id is an endpoint to get links by ID. However, if given an invalid link ID, the entire result would be a 404, even if some of the IDs were valid...giving no indication of which IDs were invalid. HOWEVER, if given an invalid id with a invalid prefix (eg ""/by_id/t3_1iy9ku,tomatosoup"") the endpoint would not 404 and would be very happy to just give you the valid items. This behavior was inconsistent and made it difficult to deal with having invalid ids. The new behavior is to only 404 if all of the ids were not found.

---

[View the code for this change on github](https://github.com/reddit/reddit/compare/9b037e0...c4fa6d3)",,False,,t5_2qizd,1374693321.0,,,True,t3_1iz2qe,http://www.reddit.com/r/redditdev/comments/1iz2qe/api_change_by_id_will_no_longer_404_if_a_single/,admin
1374136032.0,14,github.com,1ijq4u,Help complete jReddit - The Java wrapper for reddit API,18,4,6,http://www.reddit.com/r/redditdev/comments/1ijq4u/help_complete_jreddit_the_java_wrapper_for_reddit/,,,False,,t5_2qizd,False,,,False,t3_1ijq4u,https://github.com/thekarangoel/jReddit/blob/karan/implemented_methods.md,
1371996466.0,13,self.redditdev,1gwq0l,"Alternatives to CloudSearch via modular search handling? Core-devs, please advise!",16,3,13,http://www.reddit.com/r/redditdev/comments/1gwq0l/alternatives_to_cloudsearch_via_modular_search/,"I understand that Reddit uses Amazon CloudSearch for its indexing needs, however that doesn't work for the instance of Reddit I've stood up. 

I've a bit of experience with Elasticsearch, and I'm curious how the core Reddit devs would suggest I go about implementing integration. I would love to be able to contribute Elasticsearch support as a plugin, but it is clear that there's a little bit of work that would need to be done before Reddit could support alternatives to CloudSearch. 

Would it be acceptable for me to submit a pull-request that would make the search handling more modular and ""pluginified""? That would allow me to package Elasticsearch support as plugin, as well as allow others to provide their own plugins for their preferred search index system. 

I've spent a bit of time looking at this problem, and believe that it shouldn't be too complicated to abstract Reddit from CloudSearch, allowing for any indexing system to be dropped in place. I guess I just want to make sure that, should I do this, it would stand a good chance of being found useful by the Reddit core devs. 

Thoughts?",,False,,t5_2qizd,False,,,True,t3_1gwq0l,http://www.reddit.com/r/redditdev/comments/1gwq0l/alternatives_to_cloudsearch_via_modular_search/,
1371239926.0,12,self.redditdev,1gcxsf,Query about the t5 id in the side bar,12,0,4,http://www.reddit.com/r/redditdev/comments/1gcxsf/query_about_the_t5_id_in_the_side_bar/,"Wasn't sure where else to ask this.

In the sidebar there is a form that's the parent of the markdown area, [here](http://i.imgur.com/r5CNuv9.png). I've been using the id to achieve a pseudo-random tips bar at the top of /r/RESissues.

About 2 years ago or so, when I first noticed it, it would change every page refresh and so would the tip text. More recently I've noticed that it only changes every so often, but it can be anything from 5 minutes to 30 minutes. Doesn't seem to be any way to pin down why/when it changes.

Can anyone glean from the source as to what causes it to change and why it varies? Cheers.",,False,,t5_2qizd,False,,,True,t3_1gcxsf,http://www.reddit.com/r/redditdev/comments/1gcxsf/query_about_the_t5_id_in_the_side_bar/,
1365762108.0,11,self.redditdev,1c75dh,Lesson 1: How to get submission data from Reddit's API,16,5,12,http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/,"Hey guys!  I am going to put together a series of lessons for developers.  I will progress through the process of getting JSON data from Reddit's API, building a MySQL database and importing the Reddit data into a database.  With these lessons, you will be able to create a fully functional database using Reddit's API for your projects.

I've just started coding in Python (I was previously a PERL programmer).  If you see any errors in the code, please let me know.  

I hope these tutorials will help aspiring developers create awesome applications for other Redditors.  

Today's lesson is a simple one.  We will create a very basic Python script to retrieve Reddit submission data using the JSON format.  

Let's dive in!  For these lessons, I will assume you are using a linux distribution (Ubuntu, Fedora, etc.)  If you are running windows, I would recommend that you download Virtualbox (Free Oracle product) and install Ubuntu under Virtualbox.  Virtualbox is a nice virtual machine which allows you to run a complete OS under your existing OS.  

You can learn more about Virtualbox and download it from [here](https://www.virtualbox.org/).

You can download the install files (*both 32 and 64 bit versions*) for Ubuntu [here](http://www.ubuntu.com/download/desktop).

**Once you have Ubuntu installed, you can install MySQL (if it is not already installed) by using these commands:**

    sudo apt-get update
    sudo apt-get dist-upgrade
    sudo apt-get install mysql-server mysql-client

**You can install phpmyadmin for Ubuntu and MySQL using this command:**

    sudo apt-get install phpmyadmin

The following code is very simplistic.  It authenticates with Reddit's server and makes requests for submission data.  Each request will return up to 100 submissions.  It is important that you change the setting in your Reddit profile to receive 100 listings per request.  The other alternative is to pass a URL variable with your GET request using the limit variable.  In this script, I have hard-coded the limit to the maximum of 100. 

This is a very basic script with virtually no error control, so you may want to add additional error handling routines in your project.

**Here is the code for the Python script:**

    #!/usr/bin/python
    #
    # Script Name: getRedditJSONSubmissionData.py
    # Usage: ./getRedditJSONSubmissionData.py &gt; redditData.json
    # ----------------------------------------------------------------------------------------------------------------
    # This script will average one request every two seconds.  If the servers return data faster, you might
    # need to change the sleep time to avoid going over the API limits.  
    # Also, make sure you change the settings in your Reddit account to get 100 objects at a time.  You can
    # also use the URL variable ""limit=100"" (it might be count=100?)
    #
    # Also, the code to handle errors if a non-status 200 response is received should be improved to 
    # eventually stop requesting after X amount of failures -- this might happen if Reddit's servers go down
    # for an extended time period.
    # ----------------------------------------------------------------------------------------------------------------

    import requests
    import json
    import time
    import sys

    user_pass_dict = {'user': 'your_user_name',
                  'passwd': 'your_password',
                  'api_type': 'json'}
    s = requests.Session()
    s.headers.update({'User-Agent' : 'short_description_of_yourself user:your_user_name'})
    r = s.post(r'http://www.reddit.com/api/login', data=user_pass_dict)
    j = json.loads(r.content)
    after = """" # Set this to a T3 object to start at a specific point or leave blank to start with the most recent submissions

    while True:
        time.sleep(1) # Sleep for one second to avoid going over API limits
        url = ""http://www.reddit.com/r/all/new/.json?limit=100&amp;after="" + after
        html = s.get(url) # Make request to Reddit API
        if html.status_code != 200:  # This error handing is extremely basic.  Please improve.
            # Error handing block
            sys.stderr.write(str(html.status_code)) # Print HTTP error status code to STDOUT
            sys.stderr.write(url)
            continue
            # End Error handling block
        print html.content # Print the JSON object 
        after = decode['data']['after']  # Update after variable to receive the next batch of submissions in this loop
",,False,,t5_2qizd,1365777666.0,,,True,t3_1c75dh,http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/,
1363368466.0,13,self.redditdev,1aczcr,PRAW 2.0.13 adds basic wiki editing support,14,1,4,http://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/,"`pip install -U praw` will update you to the latest version that supports listing wiki pages via `r.get_wiki_pages('subreddit')`, creating / editing wiki pages via `r.edit_wiki_page('subreddit', 'page_title', 'content', 'reason')`. Read the [2.0.13 changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-13) for additional changes. If there is additional functionality that you would like to see added, please [create an issue](https://github.com/praw-dev/praw/issues?state=open).",,False,,t5_2qizd,False,,,True,t3_1aczcr,http://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/,
1363009095.0,11,self.redditdev,1a2x0a,Deleting annoying test-reddits,12,1,5,http://www.reddit.com/r/redditdev/comments/1a2x0a/deleting_annoying_testreddits/,"Even something completely understandable, as the decision to make subreddits undeletable can turn horrendous when developing. Does any of you know a practical way of removing them from the sr-bar?
Apparently they appear and disappear at will but there seems to be no way of making the changes persistent.",,False,,t5_2qizd,False,,,True,t3_1a2x0a,http://www.reddit.com/r/redditdev/comments/1a2x0a/deleting_annoying_testreddits/,
1351390607.0,13,self.redditdev,1279n0,Is reddit adding a wiki feature?,14,1,2,http://www.reddit.com/r/redditdev/comments/1279n0/is_reddit_adding_a_wiki_feature/,"Could have sworn I read somewhere that the reddit team is working on adding wikis for subreddits, but now my reddit and google fu are failing me and I can't find anything about it.  Did I just dream this, or was it cancelled, or still a wip?",,False,,t5_2qizd,False,,,True,t3_1279n0,http://www.reddit.com/r/redditdev/comments/1279n0/is_reddit_adding_a_wiki_feature/,
1340585330.0,14,self.redditdev,vjqb7,Python Reddit API Wrapper (PRAW) version 1.4.0 changes,16,2,1,http://www.reddit.com/r/redditdev/comments/vjqb7/python_reddit_api_wrapper_praw_version_140_changes/,"I updated the minor version of PRAW to account for some changes in the behavior of subreddit listing functions. The previous sort/filter defauts didn't make sense for the Subreddit instance functions `get_new`, `get_controversial` and `get_top`, thus I've made it such that these generic functions use either the reddit or account default for non-logged in and logged in users respectively. As part of the change I've added explicit functions for each of the possible listings. Those are:

* get_controversial_from_all
* get_controversial_from_day
* get_controversial_from_hour
* get_controversial_from_month
* get_controversial_from_week
* get_controversial_from_year
* get_new_by_date (this already existed)
* get_new_by_rising
* get_top_from_all
* get_top_from_day
* get_top_from_hour
* get_top_from_month
* get_top_from_week
* get_top_from_year

I recommend you use one of the explicit listings in your code. If you want to dynamically handle a filter the best way is to do something like:

    subreddit.get_top(url_data={'t': top})

where `top` is one of `all`, `day`, `hour`, `month`, `week`, or `year`.

[Here's](https://github.com/mellort/reddit_api/commit/447116bd25ae4404b1b5085e6f1d0a82ac2f0082) the relevant change for those interested.",,False,,t5_2qizd,False,,,True,t3_vjqb7,http://www.reddit.com/r/redditdev/comments/vjqb7/python_reddit_api_wrapper_praw_version_140_changes/,
1339543808.0,11,self.redditdev,uytxd,Search syntax update: Lucene is the default search syntax,13,2,0,http://www.reddit.com/r/redditdev/comments/uytxd/search_syntax_update_lucene_is_the_default_search/,"As [previously mentioned](http://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/), the search syntax now, by default, uses the same limited Lucene syntax from days of yore. API developers desiring to use CloudSearch syntax directly may continue to do so by adding `&amp;syntax=cloudsearch` to their queries (I recommend explicitly adding `&amp;syntax=lucene` for forwards compatibility, if you're utilizing the lucene syntax).

Errors should be reported to /r/bugs.

[/r/changelog post](http://www.reddit.com/r/changelog/comments/uytnm/lucene_search_syntax_has_returned/)",,False,,t5_2qizd,False,,,True,t3_uytxd,http://www.reddit.com/r/redditdev/comments/uytxd/search_syntax_update_lucene_is_the_default_search/,admin
1339349578.0,12,self.redditdev,uusnj,Why is it impossible to sort my comments by score ?,15,3,1,http://www.reddit.com/r/redditdev/comments/uusnj/why_is_it_impossible_to_sort_my_comments_by_score/,"Hi !

It's even more mysterious actually : sorting the month or day or hour comments by score works partially, sorting the week comments by score doesn't work at all, and sorting the all time comments by score actually sorts them by date.",,False,,t5_2qizd,False,,,True,t3_uusnj,http://www.reddit.com/r/redditdev/comments/uusnj/why_is_it_impossible_to_sort_my_comments_by_score/,
1338844941.0,12,self.redditdev,ukvpb,Possible to let people vote more than once?  ,15,3,9,http://www.reddit.com/r/redditdev/comments/ukvpb/possible_to_let_people_vote_more_than_once/,"I have a small reddit installation for an intranet.  There are so few people that only giving each person 1 vote per topic/comment provides really poor resolution to the point where it is almost useless.  It would be more useful to be able to vote up good topics say 10 times and mediocre topics 3 times, etc.  It's private so there isn't going to be abuse, but even 10 votes per voting opportunity would be sufficient.",,False,,t5_2qizd,False,,,True,t3_ukvpb,http://www.reddit.com/r/redditdev/comments/ukvpb/possible_to_let_people_vote_more_than_once/,
1330104060.0,13,self.redditdev,q49ml,What's the friendliest (to the reddit servers) way of downloading a significant amount of articles and comments from Reddit for an experimental search engine?,14,1,11,http://www.reddit.com/r/redditdev/comments/q49ml/whats_the_friendliest_to_the_reddit_servers_way/,"I work on experimental search technologies that I believe have better result ranking for domain-specific types of documents than general purpose search engines.   It's well tested in law enforcement crime data, recognizing things like how matching terms in a victim's description are more relevant terms than a vehicle's description a sexual assault; while the opposite is true for a vehicle theft document.   I'm interested to see how it does on other types of documents.

I believe reddit data would be an excellent data set for such an engine -- where things like upvotes, and the reputation of a commenter, and the amount of further discussion a comment inspires, etc. could all influence ranking.

To test this, I'd love to crawl (or suck through an API) quite a bit of reddit articles and comments.

Is there a recommended friendly way to do this?


If it matters, the search engine's F/OSS based (lucene extensions), and we'd have no problem releasing them - as well as any reddit-specific stuff on top of them - as open source.   We've happily submitted previous patches back to lucene for earlier extensions that proved useful to us.",,False,,t5_2qizd,False,,,True,t3_q49ml,http://www.reddit.com/r/redditdev/comments/q49ml/whats_the_friendliest_to_the_reddit_servers_way/,
1327519572.0,14,self.redditdev,owf76,How many requests can I make per minute before I'm banned?,16,2,5,http://www.reddit.com/r/redditdev/comments/owf76/how_many_requests_can_i_make_per_minute_before_im/,"I'm writing some mod bots for a couple default subreddits and one thing I may do is scan a users history before approving posts. I'm worried that scanning a couple hundred users every couple hours (in additional to doing other stuff) will get me banned for abuse.


",,False,,t5_2qizd,False,,,True,t3_owf76,http://www.reddit.com/r/redditdev/comments/owf76/how_many_requests_can_i_make_per_minute_before_im/,
1326862404.0,13,self.redditdev,olz33,So is new code being pushed or maintenance being done while the site is down for SOPA?,14,1,6,http://www.reddit.com/r/redditdev/comments/olz33/so_is_new_code_being_pushed_or_maintenance_being/,,,False,,t5_2qizd,False,,,True,t3_olz33,http://www.reddit.com/r/redditdev/comments/olz33/so_is_new_code_being_pushed_or_maintenance_being/,
1318991419.0,12,self.redditdev,lh0yv,Slow self-posts,13,1,5,http://www.reddit.com/r/redditdev/comments/lh0yv/slow_selfposts/,"Let me prepend this by saying I don't want to sound self-righteous or that I even know anything about Reddit arch at all.  I want to try to help make self posts faster.  I hate it when users try to tell me technically how to fix something when they have no idea :)

Could someone please explain why self-links are so slow?  For me, it's taking a whole 6 seconds just to fetch a little bit of text.  Is the actual operation expensive or are there other issues that are making it slow?  I can't see how looking up a simple key from a database could be slow, no joins or other craziness should be involved.  Also, does it really need to be a POST?  If you made it a GET instead of a POST, your caches might work better with it (and it doesn't seem like the post data is used anywhere).

Please excuse my ignorance, I just really think that self-posts should be near instant.  The reddit I love is being destroyed by the lack of decent self-post support, people just post it as an image to imgur.",,False,,t5_2qizd,False,,,True,t3_lh0yv,http://www.reddit.com/r/redditdev/comments/lh0yv/slow_selfposts/,
1317907327.0,13,self.redditdev,l2ymq,API to 'click' a story?,13,0,6,http://www.reddit.com/r/redditdev/comments/l2ymq/api_to_click_a_story/,"There's a attribute of a story that appears in the JSON response that I'm not sure how to manipulate via API or just using the site with my browser. Anyone know how to mark a Story as 'clicked'?
***
Here's a snippet of the JSON listing for the story:

    {
      ""kind"": ""t3"",
      ""data"": {
          ""domain"": ""self.subreddit"",
          ""media_embed"": {},
          ""levenshtein"": null,
          ...
          ""id"": ""my1id"",
    -&gt;    ""clicked"": false, &lt;-
          ""title"": ""Test"",
          ...
          ""created_utc"": 1317846372.0,
          ""num_comments"": 0,
          ""ups"": 1
      }
    }
",,False,,t5_2qizd,False,,,True,t3_l2ymq,http://www.reddit.com/r/redditdev/comments/l2ymq/api_to_click_a_story/,
1283520493.0,13,self.redditdev,d95ad,"Request: We need to work on a solution to the subreddit size-ratio problem. If we could fix search, then in Valve time, we can fix this! ",17,4,15,http://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/,"Some of you probably won't see this as a problem, and if you don't, I can respect that. I'm not saying smaller subreddits deserve to have more subscribers, but I think there needs to be a new method built into Reddit to allow that to happen, like a recommendation system, since the current method isn't working as well as it should. 


There are a couple of ways for people to find out about new reddits currently:

1. Post in /r/newreddits
2. Allow the reddit to be shown in the default set
3. search via /Reddits
4. Same links that can be shown to have been placed in different reddits
5. Links from generous mods in the subreddit description like so ----&gt;
6. Post topics in similar reddits and link off
7. Paid (sometimes free) Advertise on the site
8. [Randomit](http://userscripts.org/scripts/show/75183)

Some subreddits that are active, make good posts often, and can get an active following with good word of mouth, can expand greatly as far as subscribers. 

But for instance, KeyserSosa or Raldi said on [DrillDown](http://thedrilldown.com/2010/09/02/the-drill-down-151-surfing-with-the-aliens/) that they just heard of /r/zombies this past week, a community of ~8,500 for 2 years. There are about ~100 subreddits with over 10,000 subscribers and thousands of subreddits with less than 100 subscribers. 

Most have less than 100 for many reasons like not being active, no content, unusual topic, etc. This isn't about that, but about how to develop and spur activity more uniformly across smaller subreddits. 

Sites like [metareddit](http://metareddit.com/) or [subredditfinder](http://subredditfinder.com/) can help in their own way, but are not close to being efficient. 

Tags will not work and the Reddit Admins despise it with a passion. 

I was thinking of add a recommend button to /reddits to show reddits that you may like based off of the reddits you currently subscribe to and posts you may like. 

The issues I can see is not just creating an algorithm to analyze and recommend subreddits, but to reduce the server load to perform such an analysis as well. 

What do you developers think about creating that kind of system for reddit and taking on this massive project?

Or is this something better left for the Reddit Admins to decide once they have the resources to do so? 

Can you guys think of some other solutions?

Thanks for your input! I certainly would like to see more than 0.56% of /r/programming subscribed to /r/redditdev. 

**EDIT**: ketralnis made an announcement asking for help in this project and people to allow their data to be shared in preferences. http://www.reddit.com/r/announcements/comments/ddz0s/reddit_wants_your_permission_to_use_your_data_for/

**Thank you ketralnis! :)**",,False,,t5_2qizd,True,,,True,t3_d95ad,http://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/,
1279912498.0,13,self.redditdev,cszqw,[Patch Review] Save Hidden Comments across Page Loads,14,1,4,http://www.reddit.com/r/redditdev/comments/cszqw/patch_review_save_hidden_comments_across_page/,"I'd really appreciate it if any of you with the reddit VM could spare a couple of minutes and give my patch a whirl.

It's available here: http://code.reddit.com/ticket/940

Essentially it saves any collapsed comments into localStorage so that if you return to that particular page it remembers and auto-hides it when you come back. Make sure to use the last patch in the comment thread.",,False,,t5_2qizd,False,,,True,t3_cszqw,http://www.reddit.com/r/redditdev/comments/cszqw/patch_review_save_hidden_comments_across_page/,
1254990487.0,11,self.redditdev,9rz47,Would anyone else be interested in in depth user statistics as well as statistics on reddit users as a whole?,14,3,14,http://www.reddit.com/r/redditdev/comments/9rz47/would_anyone_else_be_interested_in_in_depth_user/,"I'm thinking along the lines of links clicked per hour/day, time spent on reddit each day/month/year, profile of an average redditor generated using the aggregated stats of each individual, male/female percentage, etc. Feel free to suggest other stats you would like to see.",,False,,t5_2qizd,False,,,True,t3_9rz47,http://www.reddit.com/r/redditdev/comments/9rz47/would_anyone_else_be_interested_in_in_depth_user/,
1249713971.0,13,self.redditdev,98omj,"This has been asked before, but it's never been as dire as it is now: Can we have the option to block certain users?",16,3,3,http://www.reddit.com/r/redditdev/comments/98omj/this_has_been_asked_before_but_its_never_been_as/,"georedd's been spamming the same nonsense over and over, and regardless of whether you agree with his political beliefs you have to admit that his constant ""vote up or the secret PR cabal will bury this story"" is getting extremely annoying. Even on his stories that are successful (end up in the positive point range), a lot of the comments are complaints about his style, but he refuses to stop. Since it's not appropriate to ban him, I feel it is more appropriate to allow us to block him. Maybe this could be implemented as an auto-hide feature, in which all of a blocked users stories are hidden by default.

http://www.reddit.com/user/georedd/submitted",,False,,t5_2qizd,False,,,True,t3_98omj,http://www.reddit.com/r/redditdev/comments/98omj/this_has_been_asked_before_but_its_never_been_as/,
1375745132.0,12,self.redditdev,1jrvzx,"API changes: mod permissions in /about/moderators.json, sticky-related updates",17,5,0,http://www.reddit.com/r/redditdev/comments/1jrvzx/api_changes_mod_permissions_in/,"Three minor updates to the API recently:

1. Mod permissions are now available for all of a subreddit's moderators in /r/something/about/moderators/.json, in the same format as [the previous addition to /subreddits/mine/moderator/.json](http://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/)
2. Submissions now have a `stickied` attribute that will be true if the link is [stickied in its subreddit](http://www.reddit.com/r/modnews/comments/1jr429/moderators_you_can_now_sticky_a_selfpost_to_the/).
3. Subreddit settings at /r/something/about/edit/.json now includes a `sticky_permalink` attribute that links to the currently-sticked post in that subreddit.",,False,,t5_2qizd,False,,,True,t3_1jrvzx,http://www.reddit.com/r/redditdev/comments/1jrvzx/api_changes_mod_permissions_in/,admin
1373771522.0,12,redditinsight.com,1i97ix,"Reddit Insight: An interactive analytics suite for Reddit.com using its public API, combined with real-time data analysis and d3 visualizations.",20,8,2,http://www.reddit.com/r/redditdev/comments/1i97ix/reddit_insight_an_interactive_analytics_suite_for/,,,False,,t5_2qizd,False,,,False,t3_1i97ix,http://www.redditinsight.com/,
1371625480.0,12,reddit.com,1gn3za,Announcement: ALTcointip bot is now open-source! [x-post from /r/ALTcointip],13,1,1,http://www.reddit.com/r/redditdev/comments/1gn3za/announcement_altcointip_bot_is_now_opensource/,,,False,,t5_2qizd,False,,,False,t3_1gn3za,http://www.reddit.com/r/ALTcointip/comments/1gn2bo/announcement_altcointip_bot_is_now_opensource/,
1370758967.0,12,self.redditdev,1fyxv7,Am I an evil bot? (Generates memes from comments),13,1,4,http://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/,"I am currently running [here](http://www.reddit.com/r/memeifier/), with an extremely long sleep time. Was built with PRAW and am using the PRAW defaults for timeouts.",,False,,t5_2qizd,False,,,True,t3_1fyxv7,http://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/,
1368986928.0,13,self.redditdev,1en471,Ideas for SMS uses of Reddit API?,14,1,30,http://www.reddit.com/r/redditdev/comments/1en471/ideas_for_sms_uses_of_reddit_api/,"Does anyone have any interesting ideas for projects that could involve the use of SMS (via Twilio) with the Reddit API in some way? Basically, if you stick Reddit and SMS together, what cool stuff could you make?",,False,,t5_2qizd,False,,,True,t3_1en471,http://www.reddit.com/r/redditdev/comments/1en471/ideas_for_sms_uses_of_reddit_api/,
1368323917.0,11,self.redditdev,1e5u0t,[Patch] Optimistic sort,12,1,3,http://www.reddit.com/r/redditdev/comments/1e5u0t/patch_optimistic_sort/,"Hi, I added a suggestion earlier on [ideasfortheadmins](http://www.reddit.com/r/ideasfortheadmins/comments/1aac47/use_upper_bound_of_wilson_score_confidence/), which I'd really love to see online, and it was quite well-received.

So I wrote a [patch](http://pastebin.com/K8YbqFTC)! What now?",,False,,t5_2qizd,False,,,True,t3_1e5u0t,http://www.reddit.com/r/redditdev/comments/1e5u0t/patch_optimistic_sort/,
1366882240.0,10,self.redditdev,1d2nr4,Multiprocess PRAW -- testing needed,11,1,1,http://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/,"Hey everyone,

I spent a little time over the last few days refactoring how PRAW actually handles requests including how PRAW performs both rate limiting and caching. The new approach is modular allowing you to more easily change PRAW's request handling behavior by providing your own handler class, or utilizing a non-default handler provided by PRAW.

One such handler I wrote is a multiprocess handler that interfaces with a request-handler server that is now included in PRAW. Before I push this code out with version 2.1.0 it would be awesome if some of the regular PRAW users could test the multiprocess handler with your programs.

The primary benefit of the multiprocess handler is you no longer have to worry about rate limiting when running multiple versions of your PRAW programs.

To get started, fetch PRAW from github. While using git is recommended, here's a [zip](https://github.com/praw-dev/praw/archive/master.zip) of the latest source. Installing PRAW this way requires you to run `python setup.py install` from the root of the source tree (where setup.py lives).

Once up and running give this script a try: https://gist.github.com/bboe/5458377

At this point you'll probably notice warnings like:

    Cannot connect to multiprocess server. Is it running? Retrying in 2 seconds.

From a separate terminal, start PRAW's multiprocess request handling server by running `praw-multiprocess`. From here you should be able to run as many PRAW instances you want so long as you pass in an instance of the `MultiprocessHandler` when creating the Reddit instance.

What I'm looking for is any issues you encounter. I've handled most of the obvious connection issues on my ubuntu OS, but perhaps the socket error messages are different on other OSes, or I completely neglected to test something. That's where you can help!

At the moment the only unresolved issue that I am aware of is when the request times out, the server fails to pickle the resulting TimeOut object. This in turn will cause an EOFError on the client that it already handles. However, after 3 consecutive EOFErrors PRAW will raise a ClientException.

Any feedback you have would be greatly appreciated. Thanks!",,False,,t5_2qizd,False,,,True,t3_1d2nr4,http://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/,
1366332464.0,10,self.redditdev,1cnagg,Play all videos in subreddit sequentially?,12,2,3,http://www.reddit.com/r/redditdev/comments/1cnagg/play_all_videos_in_subreddit_sequentially/,"So for music-related subreddits, the majority of the posts are typically youtube videos of songs.  Is there any application available to sequentially play all of these videos?  I'd like to make a playlist where I could listen to all the songs posted in a particular subreddit.  If it doesn't exist, would people be interested in using something like this? ",,False,,t5_2qizd,False,,,True,t3_1cnagg,http://www.reddit.com/r/redditdev/comments/1cnagg/play_all_videos_in_subreddit_sequentially/,
1366303575.0,11,self.redditdev,1cm5as,Advanced Reddit API Project -- Some questions for you guys.,11,0,7,http://www.reddit.com/r/redditdev/comments/1cm5as/advanced_reddit_api_project_some_questions_for/,"I've downloaded and archived around 65 million Reddit submissions.  I still have a few hundred million more to go before I have a fairly complete archive of submissions.  

A few of you have mentioned that it would be great if I created an API that you can make calls to and download data in bulk.  I am working on creating such an API right now and will have something available in the next week or so.

Here is my question -- I need a reliable dedicated server to handle the load.  I would like to get something with the following specifications (or something close):

8 gigs of memory (or 16 gigs if cheap enough)
128 gigabytes of SSD storage
1 terabyte of regular HD storage
Quad-core CPU (second or third generation Intel Core)
1,000 Mbps unmetered bandwidth (with option to go to 10Gpbs unmetered)
Ubuntu 12.10 64 bit server edition

The most important thing for this project is having a large enough pipe to handle the bandwidth requirements.  I want to at least guarantee 500 kilobytes a second downloads for the API.  

Q1)  What company would you recommend for getting a dedicated server with specifications similar to what I mentioned above?

Here is what the API will initially offer:

* Ability to make requests and have data returned in JSON format.  (XML and Google Bit Protocol will eventually be supported)
* Ability to request submissions by date range and subreddits.  
* Ability to return 1,000 submissions per API call.
* Allow 5 API requests per second.
* 1,000 API call limit per day for unregistered developers.
* 10,000 API call limit per day for registered developers.
* 100,000 API calls per day for partner developers.

----------------------------------------------------------------------------------------

An example API call:

To retrieve 100 submissions for the subreddit ""askreddit"" between two dates:

http://api.redditdata.com/submissions?subreddit=askreddit&amp;startdate=[unix timestamp]&amp;enddate=[unix timestamp]&amp;limit=100&amp;format=json

To retrieve 100 top level comments for a particular thread in the subreddit ""askreddit""

http://api.redditdata.com/comments?subreddit=askreddit&amp;id=z8492&amp;limit=100&amp;level=1&amp;format=json

To retrieve all Reddit submissions for April 18, 2013 for the subreddit ""askreddit""

http://api.redditdata.com/submissions?subreddit=askreddit&amp;date=20130418&amp;limit=0

To retrieve all Reddit submissions for the subreddit ""askreddit"" with a minimum score of at least 500 for the above date:

http://api.redditdata.com/submissions?subreddit=askreddit&amp;date=20130418&amp;limit=0&amp;minscore=500




**Here are the API fields that I am going to support:**

**limit** (specify the maximum number of returned objects)

**subreddit** (specify a particular subreddit or an array of subreddits)

**minscore** (specify the minimum score of a particular submission)

**mincomments** (specify the minimum number of comments)

**startdate** (the oldest date possible for objects)

**enddate** (the newest date possible for objects)

**date** (return objects with this exact date)

**author** (return objects submitted / commented by this particular author)

**q** (return objects that have this keyword or array of keywords using boolean logic -- AND, OR , etc.)

**domain** (return objects pertaining to a certain domain)

**over18** (return objects that are over18 only, both or none)

**id** (return objects with this id or an array of ids)


",,False,,t5_2qizd,1366319639.0,,,True,t3_1cm5as,http://www.reddit.com/r/redditdev/comments/1cm5as/advanced_reddit_api_project_some_questions_for/,
1360201367.0,10,self.redditdev,181cu9,A couple of questions,11,1,1,http://www.reddit.com/r/redditdev/comments/181cu9/a_couple_of_questions/,"Hey guys, 

I hope that you are able to help me with this.

1. During the creation of the new subreddit, is it possible to change the default value of thumbnails to the link, and make it ticked? So that way, when creating a new subreddit, it will have thumbnails shown by default for all users.

2. Is it possible to set default users from the admin interface? I am aware of the settings in the ini file, and I am wondering is that the only place or an admin can change it somewhere on the site.

3. Is it possible to delete the subreddit? Like, completely delete it and everything it contains? Or, can someone explain what the banning actually does, what happenes with the mods, users who are subscribed, top menu bar, search indices and so on, does the banned subreddit leave trails or its deteled-like, but not completely?

4. Somehow, my clone is not detecting new search submissions, so the data has to be pushed manualy to CloudSearch to Amazon. (via paster run r2/lib/cloudsearch.py -c ""rebuild_subreddit_index()"" and rebuild_link_index(). I've added a cron job for this, but this might pose issues later when (if) the sites become bigger, so can you suggest some potential solutions why the process_new_links() is not working (its not working when running directly via paster run, so its not the consumers issue)

I realize this is a lot of questions, so thanks in advance for any kind of help and assistance with either of those.",,False,,t5_2qizd,False,,,True,t3_181cu9,http://www.reddit.com/r/redditdev/comments/181cu9/a_couple_of_questions/,
1357244604.0,13,self.redditdev,15wmo2,Suggestion/Brainstorm: Premium API Access at Cost,15,2,7,http://www.reddit.com/r/redditdev/comments/15wmo2/suggestionbrainstorm_premium_api_access_at_cost/,"After reading /u/dakta's comment [here](http://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/c7q7lcy), it sounded like a decent revenue model for Reddit by allowing further limits on API calls for a cost.

Thinking about it from a service kind of level, what is currently being offered now is the free tier, while allowing for a ""pay for what you use as you use it"" type of model that AWS currently has.

Possible options could be:

- Rate limit bypass - allow the application to bypass current rate limits when the application has the need to. Price it at something like $0.001 (numbers completely made up) per rate limit bypass.

- Comments returned limit bypass (allow 2000 comments to be retrieved at a time instead of 1000).

There are a multitude of ways you could do this, perhaps even by *just* offering the rate limit bypass while keeping the other parameters the same (ex: you can't bypass the 1000 comments returned, but you can request 5000/sec and be charged rate limit charges).

**Please Note:** This is just a brainstorm of possibilities, and I do know it would take quite a bit to implement something like this. But from the dedicated Reddit community (especially the programming community) I'd like to think that some users *would* pay premium for something like this.",,False,,t5_2qizd,1357254630.0,,,True,t3_15wmo2,http://www.reddit.com/r/redditdev/comments/15wmo2/suggestionbrainstorm_premium_api_access_at_cost/,
1342574517.0,11,self.redditdev,wqg83,Reddit Alien Logo Copyright,13,2,6,http://www.reddit.com/r/redditdev/comments/wqg83/reddit_alien_logo_copyright/,"Hi Redditors,

I am writing a reddit metro app for Windows 8 and would like to know if there is any copyright on the Alien Logo/character, and any legalities (is that a word?) I should be aware of before using the feed or logo?

Thanks Much",,False,,t5_2qizd,False,,,True,t3_wqg83,http://www.reddit.com/r/redditdev/comments/wqg83/reddit_alien_logo_copyright/,
1338629331.0,11,self.redditdev,uh028,What does juryvote do ?,12,1,6,http://www.reddit.com/r/redditdev/comments/uh028/what_does_juryvote_do/,"Hi everyone,

I'm reading the API to create a bot, while reading the source code for the API I found POST_juryvote on line 1083 at the time of writing, so i'm wondering what is Jury Vote on reddit?, Because didn't see it before on the site",,False,,t5_2qizd,False,,,True,t3_uh028,http://www.reddit.com/r/redditdev/comments/uh028/what_does_juryvote_do/,
1335486367.0,13,self.redditdev,suh3z,Reddit api wrapper not_spammer=true flag. Answer would be greatly appreciated. ,21,8,2,http://www.reddit.com/r/redditdev/comments/suh3z/reddit_api_wrapper_not_spammertrue_flag_answer/,How do I add in the not_spammer=true flag into an upvote request in the reddit api wrapper. Please give me an example.,,False,,t5_2qizd,False,,,True,t3_suh3z,http://www.reddit.com/r/redditdev/comments/suh3z/reddit_api_wrapper_not_spammertrue_flag_answer/,
1294672674.0,10,self.redditdev,ezjwu,Did you guys build your spam filtering? Or are you using a an open source package?,13,3,11,http://www.reddit.com/r/redditdev/comments/ezjwu/did_you_guys_build_your_spam_filtering_or_are_you/,"I'm building a Q&amp;A site, and I would like to use Reddit's intelligent spam filtering system.  How do I get it, or how would I build it.",,False,,t5_2qizd,False,,,True,t3_ezjwu,http://www.reddit.com/r/redditdev/comments/ezjwu/did_you_guys_build_your_spam_filtering_or_are_you/,
1294515341.0,11,self.redditdev,eyl5x,Can someone direct me to the formula/algorithm for reddit's hot/new/rising feature,14,3,5,http://www.reddit.com/r/redditdev/comments/eyl5x/can_someone_direct_me_to_the_formulaalgorithm_for/,,,False,,t5_2qizd,False,,,True,t3_eyl5x,http://www.reddit.com/r/redditdev/comments/eyl5x/can_someone_direct_me_to_the_formulaalgorithm_for/,
1260786625.0,10,self.redditdev,aeg0n,"I just want to notify someone that the word ""message"" in the Swedish translation is spelled wrong.",13,3,0,http://www.reddit.com/r/redditdev/comments/aeg0n/i_just_want_to_notify_someone_that_the_word/,"Where it says ""Send message"", it says ""Skicka medellande"" in Swedish, but it's supposed to be ""Skicka meddelande"". :)",,False,,t5_2qizd,False,,,True,t3_aeg0n,http://www.reddit.com/r/redditdev/comments/aeg0n/i_just_want_to_notify_someone_that_the_word/,
1259601322.0,10,self.redditdev,a9jin,Is iReddit open for community development? How can I contribute to it?,11,1,5,http://www.reddit.com/r/redditdev/comments/a9jin/is_ireddit_open_for_community_development_how_can/,"iReddit is the ""official"" iPhone client for Reddit, right? Is the source available for submission/modification like the Reddit source itself?

I've been using iReddit pretty regularly and I have both some improvements to make and some features I'd like to add throughout the app. There are some portions that don't behave terribly iPhone-y that with some small changes could really benefit the experience. There are also some clear performance problems that I'd like to look at.

I am a professional iPhone developer (a reasonably successful one)  and can back up my technical and user experience know-how. Someone responsible can e-mail me for the more personal credentials.

Thanks. Sweet app so far, would love to see big improvements or even take over part of the development as a side project!",,False,,t5_2qizd,False,,,True,t3_a9jin,http://www.reddit.com/r/redditdev/comments/a9jin/is_ireddit_open_for_community_development_how_can/,
1214164152.0,11,downloadsquad.com,6oh8x,"""Digg has legions of followers. They're quite fanatical. The similar service Reddit doesn't have that type of following.""",16,5,7,http://www.reddit.com/r/redditdev/comments/6oh8x/digg_has_legions_of_followers_theyre_quite/,,,False,,t5_2qizd,False,,,False,t3_6oh8x,http://www.downloadsquad.com/2008/06/18/digg-this-kevin-rose-reddit-goes-completely-open-source/,
1367859904.0,11,self.redditdev,1dsvq5,"Does reddit have an equivalent to Twitters ""Firehose"" API?",13,2,5,http://www.reddit.com/r/redditdev/comments/1dsvq5/does_reddit_have_an_equivalent_to_twitters/,"I'm thinking about a way to gather some insightful statistics about subreddits and I'd thought about analysing comments for some specific metrics. E.g. [Flesh-Kincaid reading level](http://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_test) or just lengh. 

Is there a way to obtain all comments in a stream? There is http://www.reddit.com/comments.json that would be probably okay for a sample, but I'd rather don't want to hit the reddit servers each second or less. 

Twitter has an API endpoint that just keeps the connection opens and pushes new tweets to me: https://dev.twitter.com/docs/api/1.1/get/statuses/sample

Thanks for any hints!",,False,,t5_2qizd,False,,,True,t3_1dsvq5,http://www.reddit.com/r/redditdev/comments/1dsvq5/does_reddit_have_an_equivalent_to_twitters/,
1367837576.0,10,self.redditdev,1dsaoh,Anyone want to help a big subreddit make a bot? We're looking for a bot on /r/explainlikeimfive that will perform a variety of functions.,16,6,9,http://www.reddit.com/r/redditdev/comments/1dsaoh/anyone_want_to_help_a_big_subreddit_make_a_bot/,"**EDIT: We have someone working on it. Thanks /u/jonas747!**

On ELI5, we have one flair option. Users can mark their question as ""answered.""

Basically, the bot would have one task. When a post *that hasn't been given flair* received 25 comments, the bot would PM the OP and say:

&gt;Your post on /r/explainlikeimfive, ""[NAME OF POST],"" has received many responses. If you feel that your question has been adequately answered, click [here](link to reply to bot with a specific code) and hit ""reply"" to mark it as answered or do it manually.

&gt;I am a bot. Please do not respond to me other than through the ""mark answered"" link above. If you wish to communicate with the ELI5 moderators, click [here](http://www.reddit.com/message/compose?to=%2Fr%2Fexplainlikeimfive).

So basically, it would do a few things:

* alert users when their post that hasn't been marked as answered hits 20 comments through a PM

* link them to the post

* allow them to click a link that does it for them

Of course, we could add the bot account as a mod.

Thanks so much for your interest!

~123421 and the ELI5 modteam",,False,,t5_2qizd,1367843905.0,,,True,t3_1dsaoh,http://www.reddit.com/r/redditdev/comments/1dsaoh/anyone_want_to_help_a_big_subreddit_make_a_bot/,
1364756504.0,10,self.redditdev,1bde0f,Getting 409 error from Reddit login API.,14,4,1,http://www.reddit.com/r/redditdev/comments/1bde0f/getting_409_error_from_reddit_login_api/,"&gt;409 Conflict: Indicates that the request could not be processed because of conflict in the request, such as an edit conflict.

Getting a garbled response from the Reddit API whenever I try to login. Stuff like:

&gt;7IEjFy.z3""@H0

 Not sure what the issue could be?",,False,,t5_2qizd,False,,,True,t3_1bde0f,http://www.reddit.com/r/redditdev/comments/1bde0f/getting_409_error_from_reddit_login_api/,
1364165755.0,9,self.redditdev,1axt99,"I broke my reddit stack, how do I go about diagnosing it and fixing it.",10,1,8,http://www.reddit.com/r/redditdev/comments/1axt99/i_broke_my_reddit_stack_how_do_i_go_about/,"I wanted to explore the reddit stack so I started poking around. I enabled admin and tried to add an award, the picture of which was linked from an external source. I'm now stuck with a trace back in /admin/awards with the error 

     TypeError: not all arguments converted during string formatting

I can provide the traceback if necessary. Thanks.",,False,,t5_2qizd,False,,,True,t3_1axt99,http://www.reddit.com/r/redditdev/comments/1axt99/i_broke_my_reddit_stack_how_do_i_go_about/,
1363319642.0,11,self.redditdev,1abxgt,Is there a viewer that arranges comments as a graphical tree?,13,2,5,http://www.reddit.com/r/redditdev/comments/1abxgt/is_there_a_viewer_that_arranges_comments_as_a/,"I really like diagrams and graphs and I was thinking about writing a viewer that arranges comments like nodes on a [tree](http://en.wikipedia.org/wiki/Tree_(graph_theory\) ). However, it really seems like someone would have already made something like this and it would be silly to make something that already exists. Unfortunately, I don't know how to search for it, since ""tree"" means a lot of things.

I am asking in this sub-reddit because this seems like the most likely place where such a viewer would get posted.",,False,,t5_2qizd,False,,,True,t3_1abxgt,http://www.reddit.com/r/redditdev/comments/1abxgt/is_there_a_viewer_that_arranges_comments_as_a/,
1362823396.0,11,github.com,19yrny,Simple project to scrape /r/tldr using PRAW (learning python),13,2,0,http://www.reddit.com/r/redditdev/comments/19yrny/simple_project_to_scrape_rtldr_using_praw/,,,False,,t5_2qizd,False,,,False,t3_19yrny,https://github.com/shrayas/reddit-tldr,
1361107451.0,12,self.redditdev,18ov4m,Advice on running a Reddit bot written in Python on a shared server?,12,0,16,http://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/,"I want to use PRAW and do some automated data collection on a regular basis. I could run this from home but I'm pretty OCD and don't want an Internet hiccup, computer crash, or power outage to miss one of the polling times so ideally I could run this on a server. I have a cheap hosting account but I don't know if I can run a bot script from there. I'm fine writing PHP and JavaScript and stuff but running command line type stuff gives me pause especially when it involves importing stuff. I don't want the hosting company to get mad at me if I'm not releasing resources correctly.

So what are some options and best practices for writing a Reddit bot? Thanks!",,False,,t5_2qizd,False,,,True,t3_18ov4m,http://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/,
1359988517.0,10,self.redditdev,17v8gq,Age verification issue.,13,3,2,http://www.reddit.com/r/redditdev/comments/17v8gq/age_verification_issue/,"Hey guys! First post please be gentle.
There is an issue with the age verification page logic.
If you try to open a sub-reddit with adult content without logging in, reddit.com/r/&lt;subreddit&gt;, you get a page asking you to verify your age. But if you try to do the same using i.reddit.com/r/&lt;subreddit&gt;, you do not get the page asking you to verify age.
P.S. Don't ask me how I found it out.",,False,,t5_2qizd,False,,,True,t3_17v8gq,http://www.reddit.com/r/redditdev/comments/17v8gq/age_verification_issue/,
1359576451.0,11,self.redditdev,17kwf1,Adding an email notification feature. Best way to proceed?,12,1,5,http://www.reddit.com/r/redditdev/comments/17kwf1/adding_an_email_notification_feature_best_way_to/,"I am working on modifications to Reddit to support email notifications. I'm doing this primarily for a clone of the Reddit source we're evaluating for part of a larger system. I find that implementing a significant feature is the best way to learn the nuts and bolts of an architecture. 

I also would like to contribute something back to the community as part of our process, if possible.

So I'm not too worried about doing the feature, as I'm comfortable with complex systems and Reddit seems fairly approachable. I'm more concerned with wasting my time on a feature that won't end up being accepted for other reasons. If I know there is no chance the feature would be accepted, I'd code it differently than I might if it would likely be accepted, as there are likely Reddit-community requirements that greatly exceed ours for reasons of performance, scalability and dealing with what is likely to be a much higher flow of potential notifications for most people. For instance, people might want the option for a daily digest email on Reddit.com while on our future system, they'd likely just take the full feed all the time.

Since this is a feature that seems likely to have been considered many times before, and not implemented yet, I want to make sure I understand the reasons why this hasn't yet been done.

So I'd like to find out more about the informal or formal process for proposing, implementing, and then getting new features accepted into the main code line.

In this particular case, there are likely to be considerable cost from an operations perspective for the feature. It is also likely that people within Reddit have looked at this feature and decided it wasn't worth the tradeoff in benefits versus operational costs and time required to implement the feature.

So:

1) Does anyone know why email notifications specifically have not yet been implemented by the full-time programming staff? A search didn't turn up any discussion.

2) What are the operational parameters which enter into any equations about whether or not a feature like this would get accepted into the main code line?

3) How does one obtain architectural approval for a design ahead of implementing a feature like this? Is there a defined process? Who makes these sorts of decisions?

4) Email notification seems likely to increase the outbound email volume by several orders of magnitude. What is the specific email infrastructure I should target for sending email? It seems likely that this would change for just this feature alone as the only use of email I currently see is for confirmation of email addresses. After all, you don't even need an email address to get a Reddit account.",,False,,t5_2qizd,False,,,True,t3_17kwf1,http://www.reddit.com/r/redditdev/comments/17kwf1/adding_an_email_notification_feature_best_way_to/,
1358738637.0,10,self.redditdev,16yu6g,Simple Praw + Comment Question,10,0,2,http://www.reddit.com/r/redditdev/comments/16yu6g/simple_praw_comment_question/,"How do you get a comment given its submission.id and its comment.id ?  
This is how you do it with a submission but haven't been able to figure out how to get a comment. Im SURE its very simple and im just not seeing it.    
    
    submission = reddit.get_submission(submission_id=submissionid)

  ",,False,,t5_2qizd,False,,,True,t3_16yu6g,http://www.reddit.com/r/redditdev/comments/16yu6g/simple_praw_comment_question/,
1358712705.0,10,self.redditdev,16y13b,Pythonanywhere.com and PRAW,11,1,9,http://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/,"I keep getting a JSONDecodeError, anyone gotten PRAW to work on Pythonanywhere?
**EDIT:** Got past the first hurdle, but then experienced this error when using PRAW login.

    Traceback (most recent call last):
      File ""/usr/local/lib/python2.7/site-packages/flask/app.py"", line 1687, in wsgi_app
        response = self.full_dispatch_request()
      File ""/usr/local/lib/python2.7/site-packages/flask/app.py"", line 1360, in full_dispatch_request
        rv = self.handle_user_exception(e)
      File ""/usr/local/lib/python2.7/site-packages/flask/app.py"", line 1358, in full_dispatch_request
        rv = self.dispatch_request()
      File ""/usr/local/lib/python2.7/site-packages/flask/app.py"", line 1344, in dispatch_request
        return self.view_functions[rule.endpoint](**req.view_args)
      File ""/home/myapp.py"", line 21, in login
        return saved_links_page(fetch_Links(str(form.username.data), str(form.password.data)))
      File ""/home/myapp.py"", line 33, in fetch_Links
        r.login(str(username), str(password))
      File ""/home/.local/lib/python2.7/site-packages/praw/__init__.py"", line 804, in login
        self.request_json(self.config['login'], data=data)
      File ""/home/.local/lib/python2.7/site-packages/praw/decorators.py"", line 211, in error_checked_function
        return_value = function(cls, *args, **kwargs)
      File ""/home/.local/lib/python2.7/site-packages/praw/__init__.py"", line 375, in request_json
        response = self._request(url, params, data)
      File ""/home/.local/lib/python2.7/site-packages/praw/__init__.py"", line 266, in _request
        timeout=timeout)
      File ""/home/.local/lib/python2.7/site-packages/praw/decorators.py"", line 64, in __call__
        result = self.function(reddit_session, url, *args, **kwargs)
      File ""/home/.local/lib/python2.7/site-packages/praw/decorators.py"", line 155, in __call__
        return self.function(*args, **kwargs)
      File ""/home/.local/lib/python2.7/site-packages/praw/helpers.py"", line 149, in _request
        response.raise_for_status()
      File ""/home/.local/lib/python2.7/site-packages/requests/models.py"", line 638, in raise_for_status
        raise http_error
    HTTPError: 501 Server Error: Not Implemented",,False,,t5_2qizd,1358740773.0,,,True,t3_16y13b,http://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/,
1356303676.0,10,self.redditdev,15cfwf,JSON problems :(,13,3,12,http://www.reddit.com/r/redditdev/comments/15cfwf/json_problems/,"Hi, redditdev!

I'm trying to pull the first 500 posts of a subreddit in its JSON format in one call so as to limit the number of calls I send to reddit. I can append .json to the url and get the first 25 posts, but when I try to set the limit to ?limit=500 (which is supposedly the max), it will only return the first 100 posts. Since I figured this is built in, I tried to use ?count in conjunction with ?limit=100 to start at the 100th post and collect the posts from 100-200, and then repeat this for 200-300, etc, but this does not work and defaults me to the top 100 posts.

Here is my code:

    def datascrape(subreddit):
        import urllib2
        import simplejson


        temp = open(str(""tempfile.txt""), 'w')

        url = [""http://www.reddit.com/r/"" + subreddit + ""/.json?limit=100""]
        url.append(""http://www.reddit.com/r/"" + subreddit + ""/.json?count=100&amp;limit=100"")
        url.append(""http://www.reddit.com/r/"" + subreddit + ""/.json?count=200&amp;limit=100"")
        url.append(""http://www.reddit.com/r/"" + subreddit + ""/.json?count=300&amp;limit=100"")
        url.append(""http://www.reddit.com/r/"" + subreddit + ""/.json?count=400&amp;limit=100"")

        pos = 0

        while pos &lt; 4:
            req = urllib2.Request(url[pos], None, {'user-agent':'MrFanzyPanz_Data_Test'})
            opener = urllib2.build_opener()
            json_data = opener.open(req)

            temp.write(str(json_data.read()))
            pos += 1

        temp.close()

        return

The ""subreddit"" variable is currently set to ""pics"".  The API's time limitations make it really problematic to call entries individually or by lists, so I'm trying to avoid the API. Is there any way to extend the limit to 500, or to use ?count and ?limit together in order to retrieve blocks of 100 posts?

Thanks for your help!",,False,,t5_2qizd,False,,,True,t3_15cfwf,http://www.reddit.com/r/redditdev/comments/15cfwf/json_problems/,
1355932336.0,10,self.redditdev,1542h3,Logging in via the API is 503'ing for me.  I've not changed any code and I've tried machines on two different networks,10,0,3,http://www.reddit.com/r/redditdev/comments/1542h3/logging_in_via_the_api_is_503ing_for_me_ive_not/,"Logging in via the browser works fine.  I've tried changing my user-agent (except for copying a browser's, since that's a no-no on reddit).  I've tried two different accounts.  I've even tried a remote machine a few states away.  Nothing helps.  I've made no changes to my login code for months.

I guess I just want my /u/moderator-bot to be able to come back up and help moderating.


Edit: 

I am told that it might be an https issue.  I switched to http auth and all is fine.

",,False,,t5_2qizd,1355939824.0,,,True,t3_1542h3,http://www.reddit.com/r/redditdev/comments/1542h3/logging_in_via_the_api_is_503ing_for_me_ive_not/,
1350614080.0,11,self.redditdev,11q6ed,Documentation about operating and administering my clone,12,1,3,http://www.reddit.com/r/redditdev/comments/11q6ed/documentation_about_operating_and_administering/,"Is there any documentation out there?

I'm particularly interested in the admin functions.  So far, I don't see much, other than adding adverts and awards.  

What am I missing?  Surely there's more functionality available to admins, no?  Where's the ""shadowban this guy"" button, for example?",,False,,t5_2qizd,False,,,True,t3_11q6ed,http://www.reddit.com/r/redditdev/comments/11q6ed/documentation_about_operating_and_administering/,
1341113171.0,8,self.redditdev,vv4tg,Python Reddit API Wrapper package rename and repository move,12,4,2,http://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/,"Three things:

With /u/mellort's support, I have moved what was previously [mellort/reddit_api](https://github.com/mellort/reddit_api) to [praw-dev/praw](https://github.com/praw-dev/praw) on github. /u/mellort has made a clone from the new location, so that existing links to the repository are not broken.

Additionally I've official updated the name to __PRAW__ which is an acronym for ""Python Reddit API Wrapper"". The primary impetus for the name change was to resolve the confusion I see from many people regarding what to call the package. I admit it was very confusing that the repository was called `reddit_api` and the package was called `reddit`. Hopefully it should be clear what name to use now: PRAW.

Finally, to clearly distinguish the python package from reddit's own source, I have renamed the package from `reddit` to `praw` and restarted PRAWs version number at 1.0.

Aside from the readthedocs documentation, all the documentation (on github) should be updated to reflect the change.

Making the switch is pretty simple, here are the steps:

0. Install the `praw` package (instructions on github)

0. Replace `import reddit` with `import praw` in your code and of course update any `reddit.NAME` references with `praw.NAME` such as `reddit.Reddit` with `praw.Reddit`.

0. If you had a user-level, or script level `reddit_api.cfg` file, please replace that with `praw.ini`. There is more information [here](https://github.com/praw-dev/praw/wiki/The-Configuration-Files).

If you have any questions / comments please don't hesitate to ask / share.",,False,,t5_2qizd,False,,,True,t3_vv4tg,http://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/,
1341006904.0,8,self.redditdev,vtdwf,I'm a newbie developer. I want to do testing for your project.,12,4,2,http://www.reddit.com/r/redditdev/comments/vtdwf/im_a_newbie_developer_i_want_to_do_testing_for/,"The title says it all. I'm new to developing, but am highly interested in getting involved by testing any redditdev project your working on.

I have experience with the reddit bread 'n' butter (Python, HTML, PHP...), and A LOT more. I'm a second-year undergrad computer science student, but have been coding for over 6 years. I really just want to be involved with some developing.

I'm a 4-year-strong redditor. I know what I'm developing for.

Since I don't know much, I'd love to do some testing. It's simple, it'll expose me to code, and give me a chance to help out. Reply to the thread or PM me.

Awesome.

Thanks.",,False,,t5_2qizd,False,,,True,t3_vtdwf,http://www.reddit.com/r/redditdev/comments/vtdwf/im_a_newbie_developer_i_want_to_do_testing_for/,
1338534420.0,11,self.redditdev,ufauq,a working example in C# - CleanModQueue is a Windows Forms app that uses System.Net.Htttp.HttpClient to tickle reddit's API.,15,4,7,http://www.reddit.com/r/redditdev/comments/ufauq/a_working_example_in_c_cleanmodqueue_is_a_windows/,"http://cleanmodqueue.codeplex.com/

Source available. 


I wrote a little tool to do moderation of reddits for me. The tool or the code may be  interesting to other devs or moderators.

The basics:

- It's built on .NET 4.0, therefore runs on Windows only. Uses Windows Forms.

- It is graphical - not a headless script.

- works by periodically querying reddit for the contents of your aggregated modqueue, then applying rules to items in the queue. It can also monitor other more specific queues. 

- the rule flavors are: if the post is from a ""known bad"" domain, spam it.  If the post is from a ""known bad"" author, spam it. If the post is from a mod of the reddit, approve it.  If the post has N upvotes, approve it.  If the post is from a ""known good"" domain or user, approve it. 

- You can set the list of known bad domains and known bad authors.  Likewise known good domains and users. You can set the re-check interval, and the upvote threshold. You can set the relative priority of these constraints.  In other words, you can tell it to first delete known bad domains, then approve known good users, or vice versa. 

- It also presents a UI where you can tick checkboxes to explicitly specify  actions to take on particular items.

- Uses the ReadAsAsync&lt;T&gt; that is part of HttpClient to de-serialize JSON responses into C# classes.

- The reddit interaction is all wrapped up in a C# class called ""Reddit.Client"".  It exposes methods like Login, IsLoggedIn, Logout, GetModQueue, GetNewQueue, RemovePost, ApprovePost, ReportUserAsSpammer, GetUserRecentPosts, IsUserDead (shadow-banned), and so on.  This may be useful for other .NET developers.


All the code is released under the New BSD license.

I'm offering this in hopes that it may help someone else. 
",,False,,t5_2qizd,False,,,True,t3_ufauq,http://www.reddit.com/r/redditdev/comments/ufauq/a_working_example_in_c_cleanmodqueue_is_a_windows/,
1337783091.0,11,self.redditdev,u10w4,Backup/Restore Problem,11,0,3,http://www.reddit.com/r/redditdev/comments/u10w4/backuprestore_problem/,"In order to do backups I have been doing a simple pg_dump of the reddit database.  A couple days ago I tried restoring from this backup and ran in to a couple problems.  The frontpage and subreddit frontpages page no longer show any links.  I can confirm that the actual links exist by going to the specific link url.  Also, the moderatorion log is empty.  Everything else seems to be working fine (user logins, etc.).  I ran the cron scripts manually, though I don't think that should matter.  Any ideas?",,False,,t5_2qizd,False,,,True,t3_u10w4,http://www.reddit.com/r/redditdev/comments/u10w4/backuprestore_problem/,
1335834504.0,10,aws.amazon.com,t0r6a,ProTip: Run your bots on Amazon Web Services using the Free Usage Tier,12,2,12,http://www.reddit.com/r/redditdev/comments/t0r6a/protip_run_your_bots_on_amazon_web_services_using/,,,False,,t5_2qizd,False,,,False,t3_t0r6a,http://aws.amazon.com/free/faqs/,
1332178673.0,10,phreakocious.net,r3pkp,"I wrote a Gephi plugin to graph reddit.. If there's interest, I will release it.",17,7,4,http://www.reddit.com/r/redditdev/comments/r3pkp/i_wrote_a_gephi_plugin_to_graph_reddit_if_theres/,,,False,,t5_2qizd,False,,,False,t3_r3pkp,http://phreakocious.net//shreddit-20120317,
1329605875.0,10,self.redditdev,pvr52,"So, what should new reddit clones do since IndexTank signups are disabled?",14,4,5,http://www.reddit.com/r/redditdev/comments/pvr52/so_what_should_new_reddit_clones_do_since/,"What does the LinkedIn acquisition of IndexTank mean for new reddit clones? They very likely could never open up registrations again. Is there a patch out there to use the old search provider, or are we going to have to hack in support for other search providers since we aren't going to be able to use IndexTank?",,False,,t5_2qizd,False,,,True,t3_pvr52,http://www.reddit.com/r/redditdev/comments/pvr52/so_what_should_new_reddit_clones_do_since/,
1326296270.0,12,self.redditdev,ocffg,"Licensing question - What are the ""reddit logos""?",13,1,1,http://www.reddit.com/r/redditdev/comments/ocffg/licensing_question_what_are_the_reddit_logos/,"Let me know if this is the wrong place for this question. I am thinking of writing a reddit app, but I didn't want to deal with all the licensing stuff unless it gets decently popular. For that reason I was avoiding using the reddit name and alien logo, but can I use the orangered envelope? Is that considered a ""reddit logo""?",,False,,t5_2qizd,False,,,True,t3_ocffg,http://www.reddit.com/r/redditdev/comments/ocffg/licensing_question_what_are_the_reddit_logos/,
1325534663.0,10,self.redditdev,o040t,How can I mark messages as read via the API?,14,4,10,http://www.reddit.com/r/redditdev/comments/o040t/how_can_i_mark_messages_as_read_via_the_api/,"I did a google search and it appears that theere may be some [undocumented](https://github.com/reddit/reddit/wiki/_pages) API method called read_message, but that it may or may not work? And there is no one discussing how to actually use the method.

I am trying to write an app that will let people check their messages, but it seems to be not worthwhile if I can't clear the orangered when they do.

Edit: I have read through the python file and I believe I've constructed a similar function. The server returns 
""{}""  when I make the request, but my orangered stays and the message stays in unread.

Here the relevant code, if anyone could help...

**Double Edit** I think I may not be storing the cookie properly

    markMailRead: function(mailId, callback){
        var url = 'http://www.reddit.com/api/read_message/';
        postData =
        {
            id: mailId
            //uh: reddit.control.fetchModHash()
        };
        sendRequest(url, callback, postData);


    function sendRequest(url,callback,postData) 
    {
        var req = createXMLHTTPObject();
        if (!req) return;
        var method = (postData) ? ""POST"" : ""GET"";
        req.open(method,url,true);
        if (postData)
             req.setRequestHeader('Content-type','application/x-www-form-urlencoded');
        req.onreadystatechange = function () {
              if (req.readyState != 4) return;
              if (req.status != 200 &amp;&amp; req.status != 304) {
                  errorCode = 'HTTP error: ' + req.status;
                  callback(errorCode);
              }
              var response = JSON.parse(req.response)
              if(response.data &amp;&amp; response.data.modhash){
                  reddit.control.setModHash(response.data.modhash )
              }
              if(response.data &amp;&amp; response.data.children &amp;&amp; typeof callback =='function'){
                  callback(response.data.children);
              }else if(response.data &amp;&amp; typeof callback=='function'){
                  callback(response.data);
              }else if(typeof callback== 'function'){
                  callback(response);
              };
        }
        if (req.readyState == 4) return;
        var postString = parsePostData(postData);
        req.send(postString);
    };
",,False,,t5_2qizd,True,,,True,t3_o040t,http://www.reddit.com/r/redditdev/comments/o040t/how_can_i_mark_messages_as_read_via_the_api/,
1323911635.0,12,self.redditdev,nd521,List of existing Reddit API Wrappers?,13,1,18,http://www.reddit.com/r/redditdev/comments/nd521/list_of_existing_reddit_api_wrappers/,"Could we get a list somewhere of implementations of the Reddit API in different programming languages?

That way, people who just want to get developing without having to muck about with http requests can pick a project and get working, while those who want to either learn how API wrappers work or want to develop their own have some references to go off of.

Obviously the list wouldn't be limited to one single implementation per language, but here's a list of all the wrappers I've found. I can't vouch the others, but as a user/contributor to Mellort's Python one, I can confirm that it is being actively developed:

C#:  
http://z3rb.net/reddit-c-api/  
http://code.google.com/p/reddit-api/source/browse/#hg%2FCSharp  
https://github.com/pressf12/reddit (from the comments)  

Clojure:  
http://sunng.info/blog/2011/07/reddit-clj-clojure-wrapper-for-reddit-api/

Perl:  
https://github.com/three18ti/Reddit.pm | [CPAN Link](http://search.cpan.org/~jon/Reddit-0.11/lib/Reddit.pm)

PHP:  
https://github.com/jcleblanc/reddit-php-sdk  
http://code.google.com/p/reddit-api/source/browse/#hg%2Fphp  

Python:  
https://github.com/mellort/reddit_api  
https://github.com/fsuarez2005/reddit-api-wrapper-python3  
https://github.com/derv82/reddiwrap

Ruby:  
https://rubygems.org/gems/ruby_reddit_api

Java:  
https://bitbucket.org/_oe/jreddit  
https://github.com/talklittle/reddit-is-fun (an app rather than a wrapper, but should contain enough to get started)

Any other implementations are welcome!",,False,,t5_2qizd,1354842639.0,,,True,t3_nd521,http://www.reddit.com/r/redditdev/comments/nd521/list_of_existing_reddit_api_wrappers/,
1304908349.0,9,self.redditdev,h70v1,"How much extra load would this put on Reddit, and would it be worthwhile?",20,11,17,http://www.reddit.com/r/redditdev/comments/h70v1/how_much_extra_load_would_this_put_on_reddit_and/,"Basic outcome: When viewing all the comments on a submission, the colour of the comment reflects that user's response to the submission. If they upvote, their comment has a faint green background; if they downvote, it's faint orange. No vote lodged, no alterations to the colour of the comment's background.

Possible implementation: I'm not an expert on the Reddit code, so this might be totally retarded - just based on my guesswork at how the existing system works. Requires no database modification but does require a few queries that probably aren't there - esp if caching upvotes and downvotes.

When loading a comments page...

    $submissionUpvotes = mysql_query(""SELECT * FROM votes WHERE submission = '$currentSubmission' AND voteType = 'upvote'"");
    while($row = mysql_fetch_array($submissionUpvotes)) {
    	$arrayUpvotes[] = $row['userID'];	
    }
        
    $submissionDownvotes = mysql_query(""SELECT * FROM votes WHERE submission = '$currentSubmission' AND voteType = 'downvote'"");
    while($row = mysql_fetch_array($submissionDownvotes)) {
    	$arrayDownvotes[] = $row['userID'];	
    }
    
    $submissionPoints = count($arrayUpvotes) - count($arrayDownvotes);
        
    // here's all the other stuff to output submission points, image thumb, etc etc etc
        
    // when looping through comments:
    
    if (in_array($commentSubmitter, $arrayUpvotes)) {
        echo ""class = 'upvoted'"";
    }
    
    if (in_array($commentSubmitter, $arrayDownvotes)) {
        echo ""class = 'downvoted'"";
    }

As above - I haven't had a chance to look at the Reddit code yet so not sure on the implementation part. But could this be done, and does anybody else think it would be kinda neat? Gauging feasability of code before I wander over to /r/ideasfortheadmins",,False,,t5_2qizd,False,,,True,t3_h70v1,http://www.reddit.com/r/redditdev/comments/h70v1/how_much_extra_load_would_this_put_on_reddit_and/,
1296148810.0,10,self.redditdev,fa4w8,Possible (interim) solution for subreddit badges: Is there a way to look up usernames by ID?,11,1,8,http://www.reddit.com/r/redditdev/comments/fa4w8/possible_interim_solution_for_subreddit_badges_is/,"I moderate /r/hockey and give people team crests.  There are a number of subreddits that do this, and it has become a huge hassle for big subreddits now...  Yes, we've made that hassle ourselves by starting to offer this when traffic was lower... not complaining in that regard...

However, two key concerns:

1) It's a cumbersome process to add people's crests, and I'm trying to come up with a solve (via UserJS) to at least simplify the process for moderators...

2) Subreddits using the a.author[href=/username]:before method are often running out of space.  The alternative is to use shorter declarations that involve the user's reddit-ID (found in their about.json) such as: .id-t2_[userid]:after  Problem with that is once you put it in you never know who the heck it is/was if you need to do some editing...

It would be great if there were a way (maybe there is one already?) to look up a username if you know their userid... So, mine is 2539s ... can I query a URL somewhere and get back 'honestbleeps' from that?

If so, I'd build a tool for managing crests in a more efficient way...

I was going to kill work on this because an admin (I believe ketralnis) had said it was a feature that you guys might be working on adding... but I was later informed that it had dropped off the radar due to other higher priorities (totally understandable!)...

Thanks...",,False,,t5_2qizd,False,,,True,t3_fa4w8,http://www.reddit.com/r/redditdev/comments/fa4w8/possible_interim_solution_for_subreddit_badges_is/,
1294441316.0,11,self.redditdev,ey6b7,Not a big complaint but...why does it take so long to view my messages and user home page?,12,1,13,http://www.reddit.com/r/redditdev/comments/ey6b7/not_a_big_complaint_butwhy_does_it_take_so_long/,"I am just wondering.  As of right now its not too bad I don't really mind.  I am just interested in why this is the case.  Is there some underlying meaning, do you go through a separate server that is slower running? What do?  

Also, is it a feature that everytime I try to verify I am a human the first verification never works?  Is this standard or am I just screwing up somehow every single time?  Again, not a hassle just wondering.

Edit: Also if anyone knows please for the sake of everything good in the world let me know how to get **r/politics** off of my front page.  I have removed the frontpage status but it still shows for some reason.",,False,,t5_2qizd,True,,,True,t3_ey6b7,http://www.reddit.com/r/redditdev/comments/ey6b7/not_a_big_complaint_butwhy_does_it_take_so_long/,
1289322426.0,12,self.redditdev,e3jpm,Graceful failure with the reddit JSON[P] API,15,3,9,http://www.reddit.com/r/redditdev/comments/e3jpm/graceful_failure_with_the_reddit_jsonp_api/,"I would have posted a ticket for this, but I'm not allowed to do so. I recall Chris saying awhile back that it had something to do with not having enough karma. Oh well. :)

I've been tinkering around with the JSONP api for the first time and I've hit a brick wall when it comes to handling errors. Here's some examples. Some apply to the JSON api as well.

    tl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json
     L}P1n0
    SS\CW;vu A8""#7f8-@OCgfe/_C4)Y:?o
                      j1m kS `
                                                 i$vJo{5/=)D
                                                                                              
    # When viewed from a brower this will redirect to an HTML page
    # regardless of whether its an api request
    # Better response: {""error"":404}
    
                                                 
    tl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json?jsonp=fish
     L}P1n0
    SS\CW;vu A8""#7f8-@OCgfe/_C4)Y:?o
                      j1m kS `
                                                 i$vJo{5/=)D
                                                 
    # Same as above + ignores jsonp
    # Better response: fish({""error"":404})
    
    
    tl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json
    {error: 404}
    
    # Perfect :), although missing proper JSON quotes
    
    
    tl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json?jsonp=fish
    {error: 404}
    
    # Ignores jsonp
    # Better response: fish({""error"":404})


Also, a feature request for the API would be to add awards to /user/{username}/about.json. Or at the very least, is_gold to tell if a user is a current gold subscriber. Some of the apps and visualizations I create could potentially be resource intensive for reddit. If I could easily tell if a user is a gold subscriber I can give them full[er] features while limiting normal users to less resource intensive methods.

Thanks.",,False,,t5_2qizd,False,,,True,t3_e3jpm,http://www.reddit.com/r/redditdev/comments/e3jpm/graceful_failure_with_the_reddit_jsonp_api/,
1286768009.0,10,stackoverflow.com,dpjoj,Can anyone track down any extra bits of Reddit API documentation?(self - submit),13,3,3,http://www.reddit.com/r/redditdev/comments/dpjoj/can_anyone_track_down_any_extra_bits_of_reddit/,,,False,,t5_2qizd,False,,,False,t3_dpjoj,http://stackoverflow.com/questions/3900404/help-with-reddit-api-documentation,
1278664940.0,10,self.redditdev,cnmkr,json feed of comments view returns HTTP status 200 but mangled json sometimes,10,0,6,http://www.reddit.com/r/redditdev/comments/cnmkr/json_feed_of_comments_view_returns_http_status/,"Sometimes when I'm downloading the json feed of a comments view of a post, I get an HTTP result code of 200 back, but then the HTTP content is this:

    {error: 503}
    {'content-length': '12', 'access-control': 'allow &lt;*&gt;', 'vary': 'Accept-Encoding', 'server': ""'; DROP TABLE servertypes; --"", 'connection': 'keep-alive', 'date': 'Fri, 09 Jul 2010 08:35:09 GMT', 'content-type': 'application/json; charset=UTF-8'}

Is reddit trying to do a sql-injection attack on a bad json parser?

I searched through the source code and I don't see anything that is generating this on purpose. It must be some weird bug or a very obfuscated piece of code.

My bot retries the download 5 times and sometimes all 5 attempts at downloading will receive that same response. It's very weird.

Thoughts?",,False,,t5_2qizd,False,,,True,t3_cnmkr,http://www.reddit.com/r/redditdev/comments/cnmkr/json_feed_of_comments_view_returns_http_status/,
1269785103.0,9,self.redditdev,bjb09,Is there a page to see all friend submissions + comments? Can one be added?,12,3,18,http://www.reddit.com/r/redditdev/comments/bjb09/is_there_a_page_to_see_all_friend_submissions/,"Currently, you can see all friend submissions on /r/friends. But there doesn't seem to be a place where you can see all friend submissions + comments, or even a place to see all friend comments -- this limits the usefulness of its integration in clients like Gwibber, as discussed [here](http://www.reddit.com/r/Ubuntu/comments/bj008/upgraded_to_lucid/c0n1gf2?context=1).

So, is there a page where you can see friend submissions + comments (and possibly + liked, but I'm not a fan of that one)? Would it be accepted if someone added it? Is it a feature that wasn't considered, or one ignored because of load/storage issues, like saving comments?",,False,,t5_2qizd,False,,,True,t3_bjb09,http://www.reddit.com/r/redditdev/comments/bjb09/is_there_a_page_to_see_all_friend_submissions/,
1263710961.0,10,self.redditdev,aql1m,"Some questions on ""morechildren"" api or ""load more comments"" link.",10,0,1,http://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/,"I have asked the following questions at reddit-dev google group, but there was no reply.
I'd like to get some answers here.

#1.
How can I get children id's to fetch the hidden comments in json response using ""morechildren"" api?
Html responses for ""comments"" api contains all the hidden children id's but json responses don't contain the id's. 
For example, the response for the request ""http://www.reddit.com/r/politics/comments/aoz20"" contains the following snippet for the last ""load more comments"" link.

onclick=""return morechildren(this, 't3_aoz20', 'c0iowz2,c0iowtu,c0iowl9,c0iowhk,c0iowg5,c0iotl9,c0iorj3,c0iorhk,c0iomme,c0ip4kg,c0ioqng,c0ip00i,c0ioq2j,c0ip0r3,c0iospy,c0iovqu',0)

However, the response for the request ""http://www.reddit.com/r/politics/comments/aoz20/.json"" contains only the next snippet for the last ""load more comments"" link.

{
    ""kind"":""more"",
    ""data"":{
        ""name"":""t1_c0iowz2"",
        ""id"":""c0iowz2""
     }
}


#2.
It seems that the responses for ""morechildren"" api request contain html code to insert. 
Can I get the informations on the fetched comments in json format ?

#3.
If possible, I'd like to fetch all the comments for a link at once without the ""load more comments"".
Is it possible?
",,False,,t5_2qizd,False,,,True,t3_aql1m,http://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/,
1253004476.0,9,self.redditdev,9kovq,I saw lots of changes to Reddit in the last few weeks. When are they going to be pushed to git master?,12,3,9,http://www.reddit.com/r/redditdev/comments/9kovq/i_saw_lots_of_changes_to_reddit_in_the_last_few/,,,False,,t5_2qizd,False,,,True,t3_9kovq,http://www.reddit.com/r/redditdev/comments/9kovq/i_saw_lots_of_changes_to_reddit_in_the_last_few/,
1252206754.0,9,github.com,9hqrz,Android reddit app open source under GPL,13,4,11,http://www.reddit.com/r/redditdev/comments/9hqrz/android_reddit_app_open_source_under_gpl/,,,False,,t5_2qizd,False,,,False,t3_9hqrz,http://github.com/talklittle/reddit-is-fun/tree,
1246134336.0,9,imgur.com,8w96r,How I wish the compressed link display looked.,11,2,3,http://www.reddit.com/r/redditdev/comments/8w96r/how_i_wish_the_compressed_link_display_looked/,,,False,,t5_2qizd,False,,,False,t3_8w96r,http://imgur.com/GxeiD.png,
1214203228.0,11,reddit.com,6oj2c,One question all reddit devs need to ask themselves [bestof],14,3,1,http://www.reddit.com/r/redditdev/comments/6oj2c/one_question_all_reddit_devs_need_to_ask/,,,False,,t5_2qizd,False,,,False,t3_6oj2c,http://www.reddit.com/info/6oi6p/comments/c04g2wf?context=1,
1213885148.0,10,reddit.com,6o2no,"Pythonista observations on the reddit source, reposted to redditdev",13,3,1,http://www.reddit.com/r/redditdev/comments/6o2no/pythonista_observations_on_the_reddit_source/,,,False,,t5_2qizd,False,,,False,t3_6o2no,http://www.reddit.com/info/6nwgk/comments/,
1375995248.0,10,self.redditdev,1jzca9,How to best run a bot on a schedule (for free)?,11,1,8,http://www.reddit.com/r/redditdev/comments/1jzca9/how_to_best_run_a_bot_on_a_schedule_for_free/,"I have a Python bot that I'd like to run, automatically, once a week.  What are the best practices?

Heroku has a scheduler add-on, from what I can tell of reading the documents it basically requires a dyno to be running full-time, which will eat up all of the free hours they give you.

I read a blog post saying to use Google App Engine essentially as my cron-job app.  So I'd write a simple app in GAE that'd wake up my Heroku bot to run once weekly.  Is this really the best way to handle this problem?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1jzca9,http://www.reddit.com/r/redditdev/comments/1jzca9/how_to_best_run_a_bot_on_a_schedule_for_free/,
1374112564.0,7,self.redditdev,1ij39q,Sites using reddit.com's code?,11,4,14,http://www.reddit.com/r/redditdev/comments/1ij39q/sites_using_redditcoms_code/,"I've never seen a working/live site using reddit.com's code. 

Are there any such sites? Why no list of sites, or is there one? I'm amazed there are so little ""reddit clones"" with it being so popular.",,False,,t5_2qizd,False,,,True,t3_1ij39q,http://www.reddit.com/r/redditdev/comments/1ij39q/sites_using_redditcoms_code/,
1373474975.0,9,self.redditdev,1i0mu9,A small tool to pull images from any subreddit and set them as user's desktop background. Any critique?,9,0,9,http://www.reddit.com/r/redditdev/comments/1i0mu9/a_small_tool_to_pull_images_from_any_subreddit/,"Was trying to find a tool or app for windows to automatically cycle my desktop background through images from subreddits such as ""EarthPorn"" or ""wallpapers"" etc.

Couldn't find anything that did what I was looking for, so wrote a small Java program to do it for me.

It turned out alright so I added a GUI and exe wrapper and thought I would see what others think.

Check out the code and download it if you feel like it. [https://github.com/Exote/RedditWallpaper](https://github.com/Exote/RedditWallpaper)

Let me know what you think and if anyone has any idea how to handle some of the other image types/hosts such as flickr (who hide the .jpg behind all sorts of javascript) let me know.",,False,,t5_2qizd,False,,,True,t3_1i0mu9,http://www.reddit.com/r/redditdev/comments/1i0mu9/a_small_tool_to_pull_images_from_any_subreddit/,
1371751511.0,10,self.redditdev,1gqown,Can I get every single possible submission from a subreddit?,10,0,6,http://www.reddit.com/r/redditdev/comments/1gqown/can_i_get_every_single_possible_submission_from_a/,"Hi. Just decided I wanna write a reddit bot today. What I want to be able to do is from the API go to a subreddit like askreddit and get every single post that is on there. Basically what I would get if I went to the subreddit and clicked next until that ran out.

It looks like I should be able to do it with the ""fullname"" feature of the API but not sure how to work that. And can't find anyone that has done this.

Any thoughts?",,False,,t5_2qizd,False,,,True,t3_1gqown,http://www.reddit.com/r/redditdev/comments/1gqown/can_i_get_every_single_possible_submission_from_a/,
1366908100.0,11,github.com,1d3bkl,jReddit - Java Wrapper for Reddit API (Continuing work),12,1,10,http://www.reddit.com/r/redditdev/comments/1d3bkl/jreddit_java_wrapper_for_reddit_api_continuing/,,,False,,t5_2qizd,False,,,False,t3_1d3bkl,https://github.com/thekarangoel/jReddit,
1362713155.0,10,self.redditdev,19w24u,"Not a programmer, just curious: why can't reddit mobile clients utilize push notifications?",12,2,6,http://www.reddit.com/r/redditdev/comments/19w24u/not_a_programmer_just_curious_why_cant_reddit/,,,False,,t5_2qizd,False,,,True,t3_19w24u,http://www.reddit.com/r/redditdev/comments/19w24u/not_a_programmer_just_curious_why_cant_reddit/,
1360446350.0,10,self.redditdev,187o5m,"Results of submit POST queries sometimes get wrapped in Jquery crap, sometimes they don't.  Why?",15,5,4,http://www.reddit.com/r/redditdev/comments/187o5m/results_of_submit_post_queries_sometimes_get/,"When my bot submits something, I usually get a result like this:

    ""{""json"": {""errors"": [], ""data"": {""url"":
    ""http://www.reddit.com/tb/187xxx.json"", ""id"": ""187xxx"", ""name"":
    ""t3_187xxx""}}}""

Which is fine and dandy.

But sometimes, when the exact same running code during the exact same session
submits a post, I get this:

    ""{""jquery"": [[0, 1, ""call"", [""body""]], [1, 2, ""attr"", ""find""], [2, 3,
    ""call"", ["".status""]], [3, 4, ""attr"", ""hide""], [4, 5, ""call"", []], [5, 6,
    ""attr"", ""html""], [6, 7, ""call"", [""""]], [7, 8, ""attr"", ""end""], [8, 9, ""call"",
    []], [1, 10, ""attr"", ""attr""], [10, 11, ""call"", [""target"", ""_top""]], [1, 12,
    ""attr"", ""redirect""], [12, 13, ""call"",
    [""http://www.reddit.com/tb/187xxx.json""]]]}""

Note: My post request has the following:

     extension =&gt; json
     kind =&gt; self
     sr =&gt; subreddit 
     text =&gt; text 
     then =&gt; tb
     title =&gt; title 
     uh =&gt; modhash 
     api_type =&gt; json

Now I would rather not have the Jquery bullshit wrapping the post result, but I
can deal with it if needed.

However it's really annoying to get inconsistent results.  Sure I can work my
way around it, but what's the problem here?

**EDIT (2013-02-16): Turns out this was a bug in the libcurl bindings I was using.**",,False,,t5_2qizd,1361028820.0,,,True,t3_187o5m,http://www.reddit.com/r/redditdev/comments/187o5m/results_of_submit_post_queries_sometimes_get/,
1360353347.0,11,self.redditdev,185e3c,Little site to attempt to parse out OP answers to questions,12,1,9,http://www.reddit.com/r/redditdev/comments/185e3c/little_site_to_attempt_to_parse_out_op_answers_to/,"I wrote this because of the top 500 ama post the other week and how I thought it was difficult to go through and just find responses to questions by the OP

http://rnm.gauzza.com

It uses bootstrap and the c# reddit api library that I had to tinker with a bit to get 100% working for what I needed.",,False,,t5_2qizd,False,,,True,t3_185e3c,http://www.reddit.com/r/redditdev/comments/185e3c/little_site_to_attempt_to_parse_out_op_answers_to/,
1359403541.0,9,self.redditdev,17g3uw,Please update /r/foo/about/stylesheet.json to allow non-moderators to view it,10,1,0,http://www.reddit.com/r/redditdev/comments/17g3uw/please_update_rfooaboutstylesheetjson_to_allow/,"I think this would be nice after [this change](http://www.reddit.com/r/changelog/comments/10l7zl/reddit_change_a_new_place_to_see_the_full_source/). I'd certainly make my life easier.

EDIT: http://git.io/WkLHHA",,False,,t5_2qizd,1359486014.0,,,True,t3_17g3uw,http://www.reddit.com/r/redditdev/comments/17g3uw/please_update_rfooaboutstylesheetjson_to_allow/,
1358401362.0,9,self.redditdev,16qju9,Any way to authorize a bot to send private messages?,9,0,3,http://www.reddit.com/r/redditdev/comments/16qju9/any_way_to_authorize_a_bot_to_send_private/,"I'm working on a bot that will automate many of the moderation functions of a -swap style subreddit. I'd really like to not have all the bot's command-and-control messages sitting out in public threads, and would prefer to use the private messaging system to pass information back and forth and control authentication. I can receive messages to the bot, but I can't have the bot reply to them because the compose API demands a CAPTCHA.

Is there anything else I can do to get the same effect? Is there a way to get a bot certified as not-spammy to be allowed to bypass that? Any other advice?
",,False,,t5_2qizd,False,,,True,t3_16qju9,http://www.reddit.com/r/redditdev/comments/16qju9/any_way_to_authorize_a_bot_to_send_private/,
1357993918.0,9,self.redditdev,16fnly,load more comments (/api/morechildren) returns 403 Forbidden - Firefox 18,9,0,3,http://www.reddit.com/r/redditdev/comments/16fnly/load_more_comments_apimorechildren_returns_403/,"I turned off all reddit-related firefox extensions. Clicking load more comments just yields the red text ""loading...""

Pasting from firebug:

Request Headers:

    POST /api/morechildren HTTP/1.1
    Host: www.reddit.com
    User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:18.0) Gecko/20100101 Firefox/18.0
    Accept: application/json, text/javascript, */*; q=0.01
    Accept-Language: en-US,en;q=0.5
    Accept-Encoding: gzip, deflate
    DNT: 1
    Content-Type: application/x-www-form-urlencoded; charset=UTF-8
    X-Requested-With: XMLHttpRequest
    Referer: http://www.reddit.com/r/AskReddit/comments/16fe4d/what_company_has_forever_lost_your_business_why/
    Content-Length: 207
    Cookie: &lt;noneofyourbusiness&gt;
    Connection: keep-alive
    Pragma: no-cache
    Cache-Control: no-cache

Response Headers:

    HTTP/1.1 403 Forbidden
    Cache-Control: no-cache
    Content-Type: text/html
    Vary: Accept-Encoding
    Date: Sat, 12 Jan 2013 12:20:14 GMT
    Transfer-Encoding: chunked
    Connection: close, Transfer-Encoding",,False,,t5_2qizd,False,,,True,t3_16fnly,http://www.reddit.com/r/redditdev/comments/16fnly/load_more_comments_apimorechildren_returns_403/,
1356372685.0,10,self.redditdev,15dsju,"Automate new thread at midnight, every night.",11,1,14,http://www.reddit.com/r/redditdev/comments/15dsju/automate_new_thread_at_midnight_every_night/,"Hello Reddit Developer Community!

I have what should be a simple question, I'm the founder of /r/DiaryOfEarth, which requires a new post at midnight each night. So far, I've been manually creating this post. I'm wondering what, in your professional opinions, you believe to be the best way to automate this process. I imagine I must leave a machine on somewhere which runs a job at 11:55p each night, that logs into an account and posts the thread via the Reddit API. Correct? Can you point me in the right direction?

Any help is appreciated, much thanks!

-Eric",,False,,t5_2qizd,False,,,True,t3_15dsju,http://www.reddit.com/r/redditdev/comments/15dsju/automate_new_thread_at_midnight_every_night/,
1354475374.0,10,github.com,145sxg,Is this doing what I think it is doing?,15,5,16,http://www.reddit.com/r/redditdev/comments/145sxg/is_this_doing_what_i_think_it_is_doing/,,,False,,t5_2qizd,False,,,False,t3_145sxg,https://github.com/reddit/reddit/commit/db7e5e9ac66ae4b5527c606f32819c9d9fa930e5,
1352924321.0,11,github.com,1375la,Simple Reddit App for Google Chrome,13,2,0,http://www.reddit.com/r/redditdev/comments/1375la/simple_reddit_app_for_google_chrome/,,,False,,t5_2qizd,False,,,False,t3_1375la,https://github.com/DaGoodBoy/reddit-chrome-app,
1350094574.0,9,self.redditdev,11ebmc,When user agent can't be changed for API calls?,13,4,3,http://www.reddit.com/r/redditdev/comments/11ebmc/when_user_agent_cant_be_changed_for_api_calls/,"I'm toying with a Chrome plugin. I'm grabbing JSON API data, but I can't change the user agent because of browser restrictions. Trying to change it triggers the error ""Refused to set unsafe header 'User-Agent'"". Item two in the API wiki states ""Change your client's User-Agent string to something unique and descriptive, preferably referencing your reddit username."".

For now I'm setting a header value for X-User-Agent, which the browser does permit. What is the ""right thing"" that I should do? Has anyone thought of this from a browser security context?

Thanks!",,False,,t5_2qizd,False,,,True,t3_11ebmc,http://www.reddit.com/r/redditdev/comments/11ebmc/when_user_agent_cant_be_changed_for_api_calls/,
1348954947.0,8,self.redditdev,10omtd,PRAW: when are requests being made? (code included),10,2,3,http://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/,"Hi all,

I'm trying to implement global ratelimiting for my bot so I can run multiple PRAW bots in parallel, but apparently I'm not catching every time PRAW makes a request to reddit. Can someone please clarify how many requests are being made in the following code snippets?

I count this as 1 request:

    r.get_subreddit(""all"").get_top(limit=100)

I count r.get_front_page() as 1 request, then each submission.comments as another request.

    posts = []

    for submission in r.get_front_page():
        for comment in submission.comments:
            posts.append(comment)

I count this as 1 request:

    r.get_redditor(user).get_overview(limit=2000)",,False,,t5_2qizd,False,,,True,t3_10omtd,http://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/,
1345055976.0,10,self.redditdev,y9vz3,Would anybody be interesting in helping me create a reddit client for Google Chrome?,16,6,18,http://www.reddit.com/r/redditdev/comments/y9vz3/would_anybody_be_interesting_in_helping_me_create/,"I'm currently developing a client for Chrome and could use a partner who really knew what they were doing with JavaScript and the reddit API. I'm an interface designer and while I've been practicing my hand at js development, I feel a little out of my depth at times.

I have the basic site up. Currently you can view stories from any subreddit and switch subreddits. Here is a screenshot: http://i.imgur.com/ACyWq.png

It's slow going but I intend to stick to it. There is a lot of work to be done, comments page, ability to login and post, for example. It would be fantastic to have a js developer to work with on this project.

I don't believe a reddit client for a browser is pointless. It's always nice to give the user a choice. I created a reddit ""app"" for the Chrome webstore, which was basically a redirect to reddit.com. It was the first reddit app on the webstore and as such, it generated quite a few thousand installs. Unfortunately, this past week it was taken down. I can resubmit the app, but only if it does more than redirect a user. I figure this is a great time to release a reddit client, because we have a guaranteed userbase there. As soon as the app is resubmitted, it will update automatically for all users.

Sorry if I posted this in the wrong subreddit. I was looking to speak with people who have reddit API experience. Thanks for reading. I welcome your feedback.

Edit: Link title fail. Would anybody be *interested*...",,False,,t5_2qizd,False,,,True,t3_y9vz3,http://www.reddit.com/r/redditdev/comments/y9vz3/would_anybody_be_interesting_in_helping_me_create/,
1342804212.0,9,self.redditdev,wvrc1,Is there a way I can download every comment I ever posted?,11,2,15,http://www.reddit.com/r/redditdev/comments/wvrc1/is_there_a_way_i_can_download_every_comment_i/,"I kinda need to find a comment I posted perhaps a year or so ago.

I've posted quite a bit, so going ""next"", ""next"" is possible if tedious, but I'm curious if it will give them all to me, or will cut out after a while.

Theoretically I could write a web crawler to do that ""next"" thing for me - but the:

     Disallow: /*after=

in robots.txt tells me not to.

I tried the search feature, but it seems near useless for comments.


Is there any other way?   Any existing API for this?  If it doesn't exist already, would reddit accept a patch that said ""let me download all my own comments""?
",,False,,t5_2qizd,False,,,True,t3_wvrc1,http://www.reddit.com/r/redditdev/comments/wvrc1/is_there_a_way_i_can_download_every_comment_i/,
1337438082.0,10,self.redditdev,tuq5l,Is Cassandra always this fat or did I do something wrong?,11,1,9,http://www.reddit.com/r/redditdev/comments/tuq5l/is_cassandra_always_this_fat_or_did_i_do/,"I'm setting up my own Reddit instance on Ubuntu 12.04 LTS using [this guide](https://github.com/reddit/reddit/wiki/Install-guide). Right now I've gotten to the point where all the [dependencies](https://github.com/reddit/reddit/wiki/Dependencies) are installed, and I've started configuring everything.

When setting up Cassandra, I found Cassandra wasn't actually running and eventually found out the 512 MB I allocated to this VM wasn't enough. I kicked it up to 2 GB and it runs now, but [Cassandra is uing over half of the available ram](http://i.imgur.com/pCFEU.png).

This seems ridiculous considering there isn't even any information in the database. Setting up Cassandra was a little sketch since the ppa listed in the dependencies guide doesn't seem to work anymore.

Did I set up Cassandra wrong or is this just something I have to deal with? Since Reddit primarily uses Cassandra for caching (according to the Wiki), can I just disable caching and therefore disable Cassandra?",,False,,t5_2qizd,False,,,True,t3_tuq5l,http://www.reddit.com/r/redditdev/comments/tuq5l/is_cassandra_always_this_fat_or_did_i_do/,
1337116106.0,11,self.redditdev,toto8,API: Last edited time of post/comment available,11,0,0,http://www.reddit.com/r/redditdev/comments/toto8/api_last_edited_time_of_postcomment_available/,"The attribute is stored as ""edited"" and the value is same as for created_utc - seconds since epoch.

Important note: For older posts, the value returned will be a boolean true/false, since those posts did not store the edit time.

See also: http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/

(And you can use that post as an example: http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/.json)",,False,,t5_2qizd,False,,,True,t3_toto8,http://www.reddit.com/r/redditdev/comments/toto8/api_last_edited_time_of_postcomment_available/,
1336004192.0,10,self.redditdev,t4841,Modify frontpage?,12,2,2,http://www.reddit.com/r/redditdev/comments/t4841/modify_frontpage/,"Is it possible to make the frontpage sorted by a different method (e.g. top) by default?

What about not even listing any links at all, rather just a static home page that links to subreddits?  I'm half tempted to look in to doing this on another level (e.g. apache mod_rewrite) and just directing it to a different page, but I'd rather keep it within the reddit code so that it works regardless of rendering method.

It seemed like this should be simple, but I seem to be missing how the controller actually works...",,False,,t5_2qizd,False,,,True,t3_t4841,http://www.reddit.com/r/redditdev/comments/t4841/modify_frontpage/,
1334691853.0,8,self.redditdev,sep1w,"Creating a Reddit App, hit a little speed bump",10,2,2,http://www.reddit.com/r/redditdev/comments/sep1w/creating_a_reddit_app_hit_a_little_speed_bump/,"Hey everyone,

I'm in the midst of developing an app for Reddit and have been using the API extensively. I am trying to pull the list of subreddits that you see in the subreddit bar up on the top of the page. After scouring the internet, I could not find anything. Does anyone know how I can do this? Without crawling the page of course.

Thanks",,False,,t5_2qizd,False,,,True,t3_sep1w,http://www.reddit.com/r/redditdev/comments/sep1w/creating_a_reddit_app_hit_a_little_speed_bump/,
1334069962.0,10,blog.fruiapps.com,s2ll2,How/Why i extended reddit's ranking system to rate players for my multiplayer game. ,11,1,1,http://www.reddit.com/r/redditdev/comments/s2ll2/howwhy_i_extended_reddits_ranking_system_to_rate/,,,False,,t5_2qizd,False,,,False,t3_s2ll2,http://www.blog.fruiapps.com/2012/04/Chose-a-mathematical-model-to-rate-players,
1330854180.0,9,self.redditdev,qh192,API Documentation - It sucks hard.,16,7,11,http://www.reddit.com/r/redditdev/comments/qh192/api_documentation_it_sucks_hard/,"I am trying to write a web app that supports users logging in, upvoting, commenting, submitting new self posts, edits user flair, a few mod actions too.

Well I got the logging in working and submitting new threads, however when it does work no JSON gets returned with the thread url like the API Doc says.

~~Then if it fails requiring captcha, there is no doc saying how to do that.~~ Figured that out thanks to the freenode channel. The API Returns captcha with a value you construct a url with. http://reddit.com/captcha/xxxxxxx.png

So for the guys who do know the api, please spend a weekend improving the doc, provide detailed explanations and code samples in various languages or even pseudo code.

Everytime I try to do something that involves the api it takes me hours to get it working because lack of documentation and I am about to rip my damn hair out.

Here is my OSS code if anyone cares: 

http://code.google.com/p/playitforward/source/list",,False,,t5_2qizd,True,,,True,t3_qh192,http://www.reddit.com/r/redditdev/comments/qh192/api_documentation_it_sucks_hard/,
1329077213.0,9,github.com,pmdtt,Some documentation for sciteit code (and by extension reddit),10,1,2,http://www.reddit.com/r/redditdev/comments/pmdtt/some_documentation_for_sciteit_code_and_by/,,,False,,t5_2qizd,False,,,False,t3_pmdtt,https://github.com/constantAmateur/sciteit/wiki,
1328806410.0,9,self.redditdev,phycj,Cassandra problems setting up a reddit clone,10,1,3,http://www.reddit.com/r/redditdev/comments/phycj/cassandra_problems_setting_up_a_reddit_clone/,"I've been trying to set up reddit on a test machine using Ubuntu 11.04. The installation script runs fine without any errors (save for the PostgreSQL encoding, that defaults to ANSI and has to be changed).

The site runs fine for a while, but then Cassandra freezes (can't even restart the service):

&gt; pycassa.pool.AllServersUnavailable: An attempt was made to connect to each of the servers twice, but none of the attempts succeeded. The last failure was timeout: timed out

The logs in /var/log/cassandra show no errors before the freeze. Rebooting the machine makes it work again for a few minutes.


Any ideas on how to troubleshoot this?",,False,,t5_2qizd,False,,,True,t3_phycj,http://www.reddit.com/r/redditdev/comments/phycj/cassandra_problems_setting_up_a_reddit_clone/,
1327381496.0,9,self.redditdev,ou1xv,Is there any way to find out which user is responsible for banning an item using the API?,11,2,4,http://www.reddit.com/r/redditdev/comments/ou1xv/is_there_any_way_to_find_out_which_user_is/,"I can't find it in the json returned by a by_id request and it's not in the listing at /r/whatever/about/spam.json. I suppose I could scrape HTML for the ""removed by"" text but that's a bit hackish.

Grepping through the source tree, it looks like that info isn't publicly exposed anywhere except the HTML interface, but I might be wrong.",,False,,t5_2qizd,False,,,True,t3_ou1xv,http://www.reddit.com/r/redditdev/comments/ou1xv/is_there_any_way_to_find_out_which_user_is/,
1326941050.0,9,self.redditdev,omlmg,How can I get the reddit ID of a subreddit if I have the url? for example /r/aww,11,2,2,http://www.reddit.com/r/redditdev/comments/omlmg/how_can_i_get_the_reddit_id_of_a_subreddit_if_i/,"How can I get the reddit ID of a subreddit if I have the url? 

For example /r/aww",,False,,t5_2qizd,False,,,True,t3_omlmg,http://www.reddit.com/r/redditdev/comments/omlmg/how_can_i_get_the_reddit_id_of_a_subreddit_if_i/,
1325955604.0,7,self.redditdev,o6v24,PHP flair bot guy here... I did it! Let me know what you think,11,4,25,http://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/,"**EDIT:** Working on [a better solution](http://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/c3ev5zy) that doesn't require the user's password.

**EDIT2:** I did it again! Same links as before. Now when you fill out the form it sends a message to the user with a link. I had to use tinyurl because for some reason the reddit api for composing messages was cutting of my php variables after the first one. Check it out and let me know what you think, and thanks for all the help!

So I had the problem about verifying a user is who they say they are. [zjs](http://www.reddit.com/user/zjs) had [some good ideas](http://www.reddit.com/r/redditdev/comments/o5vi3/creating_a_web_form_to_let_users_edit_their_flair/c3em9qa) but I decided that for now I will just take the user's name and password, and if people don't like it I'll change it in the future. Here is the [web form](http://pastebin.com/Z1WQZ0Tq), and the [php script](http://pastebin.com/NxgQH93F) that does all the work. If you're interested in something like this it should be really easy to just modify that to do what you need to do. Thanks to those who responded to my questions over the past few days, and a special shout out to [bboe](http://www.reddit.com/user/bboe) who [helped me solve a problem that had be banging my head against the wall for a few hours](http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/c3egg80). If you feel like testing it out, you can sub to /r/killobyte and get to the form [here](http://www.quicklookbusy.com/flairbot/flairform.php). Right now it's all about Drexel because I want to use it there, but I'm testing it in /r/killobyte first.",,False,,t5_2qizd,True,,,True,t3_o6v24,http://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/,
1316805696.0,10,self.redditdev,kpcui,"Fixing a bug in the reddit is fun android app, need help naming a variable :p",14,4,10,http://www.reddit.com/r/redditdev/comments/kpcui/fixing_a_bug_in_the_reddit_is_fun_android_app/,"There are a few ""special"" subreddits that aren't actually subreddits- you can browse them but not submit to them, forexample /r/all and /r/random. What are these called in the reddit code base? ",,False,,t5_2qizd,False,,,True,t3_kpcui,http://www.reddit.com/r/redditdev/comments/kpcui/fixing_a_bug_in_the_reddit_is_fun_android_app/,
1315280542.0,9,self.redditdev,k61iq,Reddit Comment Enumeration,11,2,3,http://www.reddit.com/r/redditdev/comments/k61iq/reddit_comment_enumeration/,"I'm not sure if this is the /r/ to post this to, if not, please send me the right way. :)

I'm using mellort's reddit [API](https://github.com/mellort/reddit_api) for python as a wrapper.

What I'm trying to do: Given an id for a submission, retrieve all comments for that submission. But the problem is that it's slow... I just tried to retrieve 400 comments from a submission, and it took close to a minute to complete.

It seems the way the reddit api works is through links to ""more"" results for each comment, and the API wrapper has to retrieve all of those links over multiple requests. Is there a way to tell the reddit API to give me all the results, or more results in one go, rather than having to enumerate through all of the ""more"" links?

I am using this small project as a way to learn Python, so please be nice, but here's the code that i'm working with (snipped):

    import reddit
    
    def get_comments(submission):
        all_comments_list = []
        
        comments = submission.comments
        for comment in comments:
            _get_all_comments(comment, all_comments_list)
            
        return all_comments_list
    
    def _get_all_comments(cur_comment, list):
        replies = None
        
        if type(cur_comment) == reddit.comment.Comment:
            list.append(cur_comment)
            
            if hasattr(cur_comment, 'replies'):
                replies = cur_comment.replies
        else:       # It's a MoreComments object then.
            if hasattr(cur_comment, 'comments'):
                replies = cur_comment.comments
        
        if replies:
            for comment in replies:
                _get_all_comments(comment, list)

    r = reddit.Reddit(user_agent='niothiel')
    submission = r.get_submission_by_id('dnjoe')
    print get_comments(submission)
                

My information for this problem comes from the reddit API docs located [here](https://github.com/reddit/reddit/wiki/API). Specifically under the **Fetching More** section.

So, is there a better way of doing this? And thank you in advance for your answers. :)",,False,,t5_2qizd,False,,,True,t3_k61iq,http://www.reddit.com/r/redditdev/comments/k61iq/reddit_comment_enumeration/,
1312320757.0,9,self.redditdev,j7373,Is there a howto guide on setting up a reddit clone?,12,3,6,http://www.reddit.com/r/redditdev/comments/j7373/is_there_a_howto_guide_on_setting_up_a_reddit/,"I'd like to set up a local clone running on my linux box just to satisfy my curiosity on how it all works. There must be some sort of guide to do this? I've already forked the git and have the code sitting here, I just don't know what to do next.",,False,,t5_2qizd,False,,,True,t3_j7373,http://www.reddit.com/r/redditdev/comments/j7373/is_there_a_howto_guide_on_setting_up_a_reddit/,
1308850527.0,10,self.redditdev,i7bxj,Clarification of the 1 request per 2 seconds rule,15,5,5,http://www.reddit.com/r/redditdev/comments/i7bxj/clarification_of_the_1_request_per_2_seconds_rule/,"So I was looking to make an app that would consume the reddit API, when it dawned on me that if my app somehow got up into a significant number of users, the requests would easily be flowing in more than once per 2 seconds.

My question is, then, what are the boundaries of once per 2 seconds? If I distribute a client app, is it once per 2 seconds per user?  What about a centralized web app? Is that still user based, or is the entire web app restricted from making more than 1 request per 2 seconds?",,False,,t5_2qizd,False,,,True,t3_i7bxj,http://www.reddit.com/r/redditdev/comments/i7bxj/clarification_of_the_1_request_per_2_seconds_rule/,
1307417993.0,9,reddit.com,htfnq,Designing the 'tagging' feature for subreddits (cross-post),11,2,1,http://www.reddit.com/r/redditdev/comments/htfnq/designing_the_tagging_feature_for_subreddits/,,,False,,t5_2qizd,False,,,False,t3_htfnq,http://www.reddit.com/r/modhelp/comments/ht45x/dear_mods_help_me_design_a_new_feature_for/,
1298179614.0,8,self.redditdev,fouzs,"Is it possible to lookup comments using only the base-36 id for the comment, and no article name/id?",11,3,4,http://www.reddit.com/r/redditdev/comments/fouzs/is_it_possible_to_lookup_comments_using_only_the/,"I built a collection of comments scraped from reddit.com/comments as they were posted. My goal is to look them up 48 hours or more later and get updated karma details for them. However, the json for /comments doesn't give the article id.

Well, if it's a root comment for an article, it gives the article id as the parent_id. However, if it's a child to another comment, there doesn't appear to be any way to look it up.

Have I missed something? Is there a way to do this lookup? Even /api/morechildren requires link_id with the article's ID.

Any help would be much appreciated. :)

EDIT: I'm a dumbass. /comments has link information. I repurposed code and didn't verify its absence on the page. Thanks chime.",,False,,t5_2qizd,True,,,True,t3_fouzs,http://www.reddit.com/r/redditdev/comments/fouzs/is_it_possible_to_lookup_comments_using_only_the/,
1289449221.0,9,self.redditdev,e4ezp,Smartypants support?,10,1,4,http://www.reddit.com/r/redditdev/comments/e4ezp/smartypants_support/,"A while ago, I had whipped up a small greasemonkey script to fulfill the role of [SmartyPants](http://daringfireball.net/projects/smartypants/) for my reddit needs. Among many other issues with Microsoft's fine operating system, there's no easy way to type in en-dashes and em-dashestheir solution is to fake it in their products by substituting instances of `--` surrounded by words with em-dashes. As you might imagine, this is kind of irritating as the em-dash is an incredibly common form of punctuation. SmartyPants also adds many other features, including TeX-like quotation marks, (c) and (r) symbol replacement, proper ellipsis support, etc. And there are really no downsides! It was *designed* to be integrated in with markdown.

I found myself with a dearth of  projects to do and I was bored, so I thought about giving back to the community by integrating some form of SmartyPants's functionality into reddit's markdown implementation. When I looked at the source, however, I discovered that there was already support there, but reddit had [intentionally disabled it](http://code.reddit.com/browser/r2/r2/lib/c/reddit-discount-wrapper.c#L70)! (*gasp here*.)

This isn't really a complaint so much as my wondering why you disabled it. I can't imagine that it's considered intrusive, like images or HTML, or that it's a safety concern, like the `SAFELINK` flag. Are you guys planning on releasing the individual features (like the superscript feature that is in SmartyPants!) as gold features slowly? seeing `&amp;emdash;` littered throughout my normally beautiful comments makes me a little sad.

Anyway, here's a convenient patch to reenable it:

	diff --git a/r2/r2/lib/c/reddit-discount-wrapper.c b/r2/r2/lib/c/reddit-discount-wrapper.c
	index d95b46f..a852315 100644
	--- a/r2/r2/lib/c/reddit-discount-wrapper.c
	+++ b/r2/r2/lib/c/reddit-discount-wrapper.c
	@@ -67,7 +67,7 @@ reddit_discount_wrap(const char * text, int nofollow, const char * target,
	 
	   mmiot = mkd_string((char *) text, strlen(text), 0);
	 
	-  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOPANTS | MKD_NOHEADER |
	+  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOHEADER |
	                      MKD_NO_EXT | MKD_AUTOLINK | MKD_SAFELINK);
	 
	   mkd_e_flags (mmiot, &amp;cb_flagmaker);


A man can hope.",,False,,t5_2qizd,False,,,True,t3_e4ezp,http://www.reddit.com/r/redditdev/comments/e4ezp/smartypants_support/,
1288262205.0,8,self.redditdev,dxn4z,Any idea why I've started losing rows?,12,4,3,http://www.reddit.com/r/redditdev/comments/dxn4z/any_idea_why_ive_started_losing_rows/,"I have a low-activity reddit installation that sees 10-30 new links a day.  This reddit was running fine on a VM on a provider but about a week ago I transferred the entire installation (code + db) to another VM running on a different provider.

Ever since I did the move I've started to get random reddit crashes (about once every 12 hours) and seem to be caused when people add links.  What happens is that a row for the link is created in the reddit_thing_link table but an incomplete or no row is created in the reddit_data_link table.  As a consequence, the link has no sr_id and I get a hard crash with no further pages served until I manually delete the rows.

I am pretty sure this is a problem with my configuration as this installation has run fine for over 9 months on the first provider.

I have vacuumed the database but the only discernible differences in the configuration are that my original VM was running a 64-bit Ubuntu (postgres version 8.4.4) while the new one is running a 32-bit Ubuntu (postgres version 8.4.5).  I see no clues in the postgres log but obviously data corruption/loss is occurring.  

Does anybody have any ideas how I could proceed on this?
",,False,,t5_2qizd,False,,,True,t3_dxn4z,http://www.reddit.com/r/redditdev/comments/dxn4z/any_idea_why_ive_started_losing_rows/,
1287074005.0,10,self.redditdev,dr81o,Looking into improving friends list impact on performance,14,4,11,http://www.reddit.com/r/redditdev/comments/dr81o/looking_into_improving_friends_list_impact_on/,"Although I have only just begun to look through the reddit codebase to determine
exactly how to do this I'd like to get some imput on it, and ideally some
pointers at where the changes might be made to make this happen (I've tracked it
as fare as r2.models.account.Account.friends, but I can't find where
Account.friend_ids() is defined)

Anyway, here's the idea

Where the current comment tagline contains something like

    &lt;a href=""http://www.reddit.com/user/ketralnis"" class=""author submitter id-t2_nn0q friend""&gt;ketralnis&lt;/a&gt;

and reddit.css contains 
    .tagline .friend   { color:orangered }

Whether or not `friend` ends up in the class is AFAICT related to a test in
 r2.models.builder.wrap_items line 128, this test would be entirely removed if
 this proposal or something like it were implemented.


The idea is to take advantage of CSS caching in browsers. That is, supply a custom
CSS file per user (in a manner which can be invalidated to force a recache) e.g.
from reddit.com/bjartr.css?&lt;some sort of uniquifier&gt; which would contain e.g

    .tagline .id-t2_nn0q { color:orangered }
    .tagline .&lt;another friend's id&gt; { color:orangered }

and so on.

I still have to research where the CSS generation routine would be appropriate
and how to do it in such a way that fits the style of the rest of reddit's code.

The reason to use CSS directly instead of JS to achevie this is to leverage the
rendering engine of the client's browser which will be faster than using JS to
modify a page and force subsequent re-renderings.

Worst case is a browser doesn't cache CSS at all (rare AFAIK) and the CSS file
is served from a reddit side cache (flag on account says whether or not friends
list has changed since it was last generated) and so the actual list of friends
would only ever be queried on first visit or when the friends list changes or
they view their friends list in their preferences. Even if the CSS file is reserved 
in its entirety every time it only causes two queries (one to see if it 
needs to be regenerated and one to get the CSS text) which would mean for
a logged in user with zero friends that is in the quite unusual position of having 
browser which does not cache CSS there may be an extra query compared to
that same user in the current system. I do not believe this is problematic due to
the rarity of such a browser configuration.


Comments, advice, and critiques are welcome and encouraged. ",,False,,t5_2qizd,True,,,True,t3_dr81o,http://www.reddit.com/r/redditdev/comments/dr81o/looking_into_improving_friends_list_impact_on/,
1287042735.0,8,imgur.com,dr2h9,uhm what's this?,14,6,6,http://www.reddit.com/r/redditdev/comments/dr2h9/uhm_whats_this/,,,False,,t5_2qizd,False,,,False,t3_dr2h9,http://imgur.com/chsuy.png,
1286293996.0,9,self.redditdev,dn556,Getting a large amount of data from reddit,12,3,8,http://www.reddit.com/r/redditdev/comments/dn556/getting_a_large_amount_of_data_from_reddit/,"Say I wanted to try to write, for example, a reddit search engine.  I would want the title of each post (ignoring comments) and some additional information (such as submission time, subreddit, etc.) to populate my own local database.  Is there any way to get this information without spidering reddit at a rate of one request every two seconds (which would never finish)?

Yes, I'm aware that this dataset would probably be at least tens of gigabytes uncompressed (hundreds, maybe?).  I'm just guesstimating the order of magnitude based on the size of Wikipedia dumps.",,False,,t5_2qizd,False,,,True,t3_dn556,http://www.reddit.com/r/redditdev/comments/dn556/getting_a_large_amount_of_data_from_reddit/,
1276899622.0,9,self.redditdev,cgkig,We are building a team of developers and testers for Radio Reddit mobile apps. Inquire within.,12,3,13,http://www.reddit.com/r/redditdev/comments/cgkig/we_are_building_a_team_of_developers_and_testers/,"Yesterday came the great news of the [Reddit iphone app going open source](http://blog.reddit.com/2010/06/weve-open-sourced-ireddit.html).  That being the case, we fellow redditors over at [Radio Reddit](http://www.radioreddit.com) wanted to jump on the opportunity to use this to build a Radio Reddit app for the iphone, as well as a foundation for other mobile environments.  

A general concept is as follows:

On RadioReddit.com, users that are artists or are parts of bands upload their music into our site with all relevant information going into our database.  We then have playlists generated automatically based on genre and [schedule](http://www.radioreddit.com/schedule) of said genre.  Users can see what song is playing at the moment on the website and vote it up or down.  An example of the outcome can be seen at [/r/radioreddit](/r/radioreddit).  

We are thinking that we can adapt the code to incorporate a stream of Radio Reddit and information of what is currently playing while allowing for voting in real time. Our site and database also holds tags, all id3 info, etc so maybe something unique can be done there(thank you [octatone](http://www.reddit.com/user/octatone) and [johncub](http://www.reddit.com/user/johncub), you are awesome). As we also offer [podcasts](http://www.radioreddit.com/podcasts) of every show, we think this might be worth incorporating as well.

If you are interested in joining in the fun, you can find us in our [chat room](http://www.radioreddit.com/chat) or on irc at irc.makethemusic.org in #radioreddit-dev and #radioreddit.  

 ",,False,,t5_2qizd,True,,,True,t3_cgkig,http://www.reddit.com/r/redditdev/comments/cgkig/we_are_building_a_team_of_developers_and_testers/,
1274316975.0,10,self.redditdev,c64hb,While trying to use the reddit VM I had a problem with  networking not working. Here's how I solved it.,10,0,2,http://www.reddit.com/r/redditdev/comments/c64hb/while_trying_to_use_the_reddit_vm_i_had_a_problem/,"Commands such as sudo aptitude update wouldn't work. I'd get an error like ""Temporarily cannot resolve [urlhere]"". After trying ifconfig eth0 and getting nothing. I finally tried eth1 just to be sure. eth1 was working,  I changed etc/network/interfaces so that everything with eth0 now said eth1 and then rebooted the machine. Everything worked flawlessly after that.

    eth1

    cd etc

    cd network
For some reason, it wouldn't let me use cd if I tried cd'ing through more than one directory in one command.
    sudo nano interfaces

(All eth0 to eth1)

    ctrl + X

    y

At this point I just forced the VM to close. You probably shouldn't do it like that. I just like to live dangerously.



I'm not sure how likely this is to happen to anyone else or why it actually happened. I'm am however proud that I managed to solve a problem like this by myself. I don't get to use linux much. So I thought I'd post here in case I can actually help someone out.",,False,,t5_2qizd,False,,,True,t3_c64hb,http://www.reddit.com/r/redditdev/comments/c64hb/while_trying_to_use_the_reddit_vm_i_had_a_problem/,
1273512061.0,8,self.redditdev,c27ez,Chrome broke reddit (again),12,4,16,http://www.reddit.com/r/redditdev/comments/c27ez/chrome_broke_reddit_again/,"Probably old news and part of ongoing pattern but perhaps of interest.  
  
I've got the dev version of Chrome and had an update to ver 5.0.396.0 with the result that the ""reply"" to comment link no longer opens a comment box.  
  
The error was reported &amp; I expect it'll be addressed.",,False,,t5_2qizd,False,,,True,t3_c27ez,http://www.reddit.com/r/redditdev/comments/c27ez/chrome_broke_reddit_again/,
1268308125.0,9,self.redditdev,bc165,There's a small problem with how awards are calculated,10,1,12,http://www.reddit.com/r/redditdev/comments/bc165/theres_a_small_problem_with_how_awards_are/,"It seems the award system currently checks comments in the last 24 hours and awards to the best ones. However if a comment is posted say one hour before this check it is completely dismissed in the shadow of the winning comment for that day and ignored in the next day.

Can anyone point to where the code that calculates this is so I can have a go at working on a fairer method?",,False,,t5_2qizd,False,,,True,t3_bc165,http://www.reddit.com/r/redditdev/comments/bc165/theres_a_small_problem_with_how_awards_are/,
1263393056.0,8,self.redditdev,ap45m,Setting up reddit on fresh install of ubuntu-server 9.10? i'm stuck. Command history shown in below:,11,3,15,http://www.reddit.com/r/redditdev/comments/ap45m/setting_up_reddit_on_fresh_install_of/,"Hi redditdev, I just tried to install a local version of reddit on a fresh install ubuntu-server 9.10. As a disclaimer, I'll be honest, as far as linux dev work is concerned, I am a beginner. 
I used the guide found at [RedditStartToFinish](http://code.reddit.com/wiki/RedditStartToFinishIntrepid) however I ran into problems so I then used some of SystemicPlural's suggestions found in this redditdev [post](http://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/) 

Right after a fresh install of ubuntu-server 9.10, here is my entire command history:

----------

    sudo apt-get install curl gcc gettext git-core libfreetype6 libfreetype6-dev -y
    sudo apt-get install libjpeg62 libjpeg62-dev libpng12-0 libpq-dev memcached -y
    sudo apt-get install postgresql-8.3 python python-dev python-setuptools subversion -y
    git clone http://code.reddit.com/repo/reddit.git
    sudo chmod 777 -R ~/reddit
    cd reddit
    git pull
    cd r2
    sudo apt-get install libxslt1-dev -y
    sudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/
    make
    sudo mkdir /usr/local/pgsql
    sudo mkdir /usr/local/pgsql/data
    sudo chown postgres /usr/local/pgsql/data
    sudo passwd postgres
    su postgres
    cd /home/gadgetsolo/reddit/r2/
    /usr/lib/postgresql/8.3/bin/initdb -D /usr/local/pgsql/data
    createdb -E utf8 newreddit
    createdb -E utf8 changed
    createdb -E utf8 email
    createdb -E utf8 query_queue
    createdb -E utf8 awards
    createdb -E utf8 award
    createdb -E utf8 authorize
    psql newreddit &lt; ../sql/functions.sql
    createuser -P ri
    paster shell example.ini


----------

After running the last command, I get this error:

    sqlalchemy.exc.ProgrammingError: (ProgrammingError) function hot(integer, integer, timestamp with time zone) does not exist
    LINE 1: ...idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, d...
    HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
    'create index idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, downs, date), date)' {}

***
If anyone can spot any errors in my command history, I would really appreciate any feedback. 
Also, if anyone has a working version of local reddit running on a fresh install of ubuntu 9.10, it would be great if you could share your command history from start to end. 

**EDIT1**, Fixed formatting of command history
",,False,,t5_2qizd,True,,,True,t3_ap45m,http://www.reddit.com/r/redditdev/comments/ap45m/setting_up_reddit_on_fresh_install_of/,
1263097077.0,9,self.redditdev,anq3l,Authenticating to Reddit API,11,2,5,http://www.reddit.com/r/redditdev/comments/anq3l/authenticating_to_reddit_api/,"Hey Reddit Dev people. Sorry if this isn't the right place to post this, but I'm just getting this started and don't know the meta side of Reddit too well yet. I'm looking to create a Reddit inbox notifier for the Mac, ideally as a menu bar item. I see that you can access http://www.reddit.com/message/inbox/.json or .xml to view inbox data. My question is if there's a good way to programmatically log in to Reddit and fetch this while logged in. I suppose I can always use WebKit and scrape the loginbox, but I was hoping for something a bit more elegant.",,False,,t5_2qizd,False,,,True,t3_anq3l,http://www.reddit.com/r/redditdev/comments/anq3l/authenticating_to_reddit_api/,
1226101312.0,9,reddit.com,7c2e7,"""My Reddits""?! This is why I love you guys.",12,3,0,http://www.reddit.com/r/redditdev/comments/7c2e7/my_reddits_this_is_why_i_love_you_guys/,,,False,,t5_2qizd,False,,,False,t3_7c2e7,http://www.reddit.com/reddits/,
1214232208.0,9,self.redditdev,6okvi,i'm really new to all of this.  where is the color scheme and logos defined in the live code once i have the website running?,9,0,5,http://www.reddit.com/r/redditdev/comments/6okvi/im_really_new_to_all_of_this_where_is_the_color/,,,False,,t5_2qizd,False,,,True,t3_6okvi,http://www.reddit.com/r/redditdev/comments/6okvi/im_really_new_to_all_of_this_where_is_the_color/,
1376659689.0,8,self.redditdev,1khiqw,I want to build an app for my windows 8 rt for reddit using the api. Is this allowed?,10,2,5,http://www.reddit.com/r/redditdev/comments/1khiqw/i_want_to_build_an_app_for_my_windows_8_rt_for/,Seemed like a good idea. But then I realised the company makes money advertising etc. Would I be stepping on toes if I did this?,,False,,t5_2qizd,False,,,True,t3_1khiqw,http://www.reddit.com/r/redditdev/comments/1khiqw/i_want_to_build_an_app_for_my_windows_8_rt_for/,
1376493158.0,7,self.redditdev,1kco9e,System/hosting requirements for a reddit clone?,8,1,11,http://www.reddit.com/r/redditdev/comments/1kco9e/systemhosting_requirements_for_a_reddit_clone/,"I have been having troubles finding out what is necessary to run the reddit source code. What I found on various sites is that (obviously) root access is required, and it is easier to implement with a Linux OS. But as far as the actual hardware goes, how much RAM and CPU is needed?

One VPS host I found has a cheap option with only a 500Mhz CPU limit, and 512MB dedicated/1GB burst RAM. But it is obviously upgradable. I just don't want to over spend.

I tried using the search feature to find out these details, but I couldn't find what I was looking for. I apologize if this has actually been asked before and I missed it.",,False,,t5_2qizd,False,,,True,t3_1kco9e,http://www.reddit.com/r/redditdev/comments/1kco9e/systemhosting_requirements_for_a_reddit_clone/,
1374174556.0,9,self.redditdev,1iktjd,How would I start setting up and coding a comment response bot?,11,2,23,http://www.reddit.com/r/redditdev/comments/1iktjd/how_would_i_start_setting_up_and_coding_a_comment/,"I have an idea for a comment response bot (providing basic facts for a specific subject).

I have a little coding background (aka beginner).  Where should I start and what steps do I need to take?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1iktjd,http://www.reddit.com/r/redditdev/comments/1iktjd/how_would_i_start_setting_up_and_coding_a_comment/,
1373163591.0,9,self.redditdev,1hs5j9,Reddit OAuth as identity provider?,10,1,2,http://www.reddit.com/r/redditdev/comments/1hs5j9/reddit_oauth_as_identity_provider/,"I'd like to offer users the ability to ""sign in with reddit,"" as opposed to facebook/twitter/google.

Is reddit's implementation intended to be used that way? Regarding rate limiting, would it be 30/minute globally or per user? I'd only need the 1 /me.json after auth for each login.",,False,,t5_2qizd,False,,,True,t3_1hs5j9,http://www.reddit.com/r/redditdev/comments/1hs5j9/reddit_oauth_as_identity_provider/,
1373063895.0,8,self.redditdev,1hptt3,Large Reddit Research Project,10,2,9,http://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/,"Hello, Redditdev!

I am a UCLA student, and my roommates and I are conducting a large research project on reddit communities. We are planning on taking data hourly from reddit using praw and about 10 VMs for 1 month. The code will not upvote or downvote anything, or interact with reddit in any way other than simply pulling data.

The user-agent names will follow the format

    user_agent = 'UCLA Reddit Research Project: __'

with the __ being filled by the VM number.

I just wanted to let you guys know that these bots are harmless, and also to say thank you so much for your help over the last 6 months as I worked on getting this off the ground! You guys are the best!",,False,,t5_2qizd,False,,,True,t3_1hptt3,http://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/,
1372622574.0,10,vimeo.com,1hdqny,Lessons Learned while at Reddit,12,2,0,http://www.reddit.com/r/redditdev/comments/1hdqny/lessons_learned_while_at_reddit/,,,False,,t5_2qizd,False,,,False,t3_1hdqny,http://vimeo.com/10506751,
1372328982.0,8,self.redditdev,1h65v0,Request: Don't abandon the returned results because one ID is missing in a /by_id/ request.,9,1,11,http://www.reddit.com/r/redditdev/comments/1h65v0/request_dont_abandon_the_returned_results_because/,"When getting specific submissions by id, the entire fetch fails if just one id is ungettable. 

For instance, the reddit id ""1ghhps"" is not present on the system, and including that id with a list of others in a call to http://www.reddit.com/by_id/t3_1ghhps,t3_1ghhpt will cause that call to fail even though the second id is present in the system.

The system should either fail silently on the missing id and not include it in the returned JSON or include in the JSON wrapper that the id is not present or has been deleted.

Thanks!

EDIT:  The most efficient approach that I can come up with in the event that an error 404 is returned when requesting a batch of 100 id's is ""divide and conquer.""  That means splicing the original request array of ID's in half and making a new request for those ID's.  Then continue to split the array until the bad ID is the only one left in a request.  The performance for such a situation is probably O(log N) if there is one bad ID out of 100.  That means making a dozen or so requests to get the original batch of 100 ID's successfully instead of just making 1 request.

Thank you again!",,False,,t5_2qizd,1372336789.0,,,True,t3_1h65v0,http://www.reddit.com/r/redditdev/comments/1h65v0/request_dont_abandon_the_returned_results_because/,
1372225940.0,9,dev.redditanalytics.com,1h3ba4,Using Javascript and the Reddit API to bring interaction to you users via JSONP requests.,14,5,2,http://www.reddit.com/r/redditdev/comments/1h3ba4/using_javascript_and_the_reddit_api_to_bring/,,,False,,t5_2qizd,False,,,False,t3_1h3ba4,http://dev.redditanalytics.com/hottest.php,
1372097604.0,10,self.redditdev,1gzdxj,Keep getting locked out for too many login requests. Anything I can do to get around this whilst I'm developing?,12,2,3,http://www.reddit.com/r/redditdev/comments/1gzdxj/keep_getting_locked_out_for_too_many_login/,"I'm currently writing a script to allow a user to log in and list their subreddits (basic, I know) but as I'm very new to Python and PRAW I'm having to re-run my script a lot to debug my code. The problem I'm having is that Reddit is detecting too many login requests and locking me out for periods of time (just had to wait an hour to post this) by restricting my IP address. Is there anyway I can get around this by somehow signalling that I am developing and not a spammer?",,False,,t5_2qizd,False,,,True,t3_1gzdxj,http://www.reddit.com/r/redditdev/comments/1gzdxj/keep_getting_locked_out_for_too_many_login/,
1367693089.0,8,self.redditdev,1dov8h,Can't vote using oauth. Should I just abandon oauth for reddit? Is it not ready yet?,8,0,8,http://www.reddit.com/r/redditdev/comments/1dov8h/cant_vote_using_oauth_should_i_just_abandon_oauth/,"using this url : https://oauth.reddit.com/api/vote

with these parameters  : id=[a link id]&amp;dir=1 

and a POST request

I get a ""{}"" response with an OK http status but the vote is not getting registered with reddit.

I have successfully logged on, grabbed listings etc. I can't vote however. The vote endpoint mentions a modhash that I can't grab from anywhere using oauth. Is it required?",,False,,t5_2qizd,False,,,True,t3_1dov8h,http://www.reddit.com/r/redditdev/comments/1dov8h/cant_vote_using_oauth_should_i_just_abandon_oauth/,
1367592448.0,9,self.redditdev,1dmdcz,PRAW and HTTPError handling,10,1,2,http://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/,"I'm writing a small bot to monitor selected threads and then send me an email when a new comment/post is made. ATM, it's running as a cron job that checks every 5 minutes, prints to standard out which then gets emailed to me. 

 

I get error like these, which kills the thread and then sends me email of the error dumps. 



raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 504 Server Error: Gateway Time-out


raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 502 Server Error: Bad Gateway


and the like


What I want


a)At least a way to quiet these errors so my inbox doesn't get flooded.

b)Even better would be to somehow handle the error and tell it to retry the request.  I don't know how to send a signal back down to the appropriate function/class(?) that is doing the work. 




I've tried putting stuff in try: and except: and ""pass""-ing on errors. It seems to work for some errors, but it seems that /some/ exceptions occur anyway. 
",,False,,t5_2qizd,1367637901.0,,,True,t3_1dmdcz,http://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/,
1367506259.0,8,self.redditdev,1djxf4,Using reddit data for text-mining project,8,0,10,http://www.reddit.com/r/redditdev/comments/1djxf4/using_reddit_data_for_textmining_project/,"Hello friends,

For a university project, we are trying to retrieve a lot of reddit data from /r/all. Using this data, we want to apply machine learning and text mining techniques to hopefully come to some fun conclusions and possibly predict whether or not a post will become popular. ;)

It does, however, seem impractical to get a big database of posts.

We experimented with the csharp wrapper 'RedditSharp' for the reddit API, but we cannot figure out how to retrieve multiple pages.

Our next step was to write our own fetcher, but we still struggle with the retrieval of multiple pages.

Any advice you guys can give would be appreciated.",,False,,t5_2qizd,False,,,True,t3_1djxf4,http://www.reddit.com/r/redditdev/comments/1djxf4/using_reddit_data_for_textmining_project/,
1367212227.0,9,self.redditdev,1dbjvx,What are the rules for bots?,11,2,9,http://www.reddit.com/r/redditdev/comments/1dbjvx/what_are_the_rules_for_bots/,Are there any official rules for bots on top of the [API rules] (https://github.com/reddit/reddit/wiki/API#wiki-rules) and the general [rules of reddit](http://www.reddit.com/rules)?,,False,,t5_2qizd,False,,,True,t3_1dbjvx,http://www.reddit.com/r/redditdev/comments/1dbjvx/what_are_the_rules_for_bots/,
1366436841.0,8,self.redditdev,1cq7g1,How to know if subreddit exists?,10,2,8,http://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/,"I've been using PRAW to crawl through all of the subreddits, but I've found that some subreddits have been banned (thereby not existing), abruptly stopping the crawling process via a 404 error. Is there a way to test whether a subreddit exists?",,False,,t5_2qizd,False,,,True,t3_1cq7g1,http://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/,
1365209344.0,8,reddit.com,1brr3o,Using the api to mark a message as read doesn't seem to change the has_mail attribute from me.json. Any way around this?,9,1,7,http://www.reddit.com/r/redditdev/comments/1brr3o/using_the_api_to_mark_a_message_as_read_doesnt/,,,False,,t5_2qizd,False,,,False,t3_1brr3o,http://www.reddit.com/r/redditdev/comments/1022k3/using_the_api_to_mark_a_message_as_read_doesnt/,
1365176680.0,8,self.redditdev,1bqlqv,Is there a solution (e.g. irssi script or bitblee plugin) to have Reddit inbox and messages in an IRC client?,10,2,5,http://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/,Else what about using https://github.com/praw-dev/praw as a Jabber bot like https://github.com/fritzy/SleekXMPP/wiki ?,,False,,t5_2qizd,False,,,True,t3_1bqlqv,http://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/,
1364880549.0,7,self.redditdev,1bhyg8,Is the 30 requests per minute calculated by user? Or by IP Address?,8,1,0,http://www.reddit.com/r/redditdev/comments/1bhyg8/is_the_30_requests_per_minute_calculated_by_user/,,,False,,t5_2qizd,False,,,True,t3_1bhyg8,http://www.reddit.com/r/redditdev/comments/1bhyg8/is_the_30_requests_per_minute_calculated_by_user/,
1364579122.0,8,self.redditdev,1b94zr,"Hitting feed every 10 minutes, still it gives me 429 error most of the time.",11,3,2,http://www.reddit.com/r/redditdev/comments/1b94zr/hitting_feed_every_10_minutes_still_it_gives_me/,"I have [a bot](https://github.com/JKirchartz/yinzbot) that sends links to reddit posts, it checks for updates every 10 minutes on a json feed (http://reddit.com/r/redditdev/.json, for example) but it keeps giving me [429 errors](http://httpstatus.es/429). I saw that reddit prefers a username in the user-agent, so I added mine and I still get 429 errors. 

Do I really need to use oAuth just to get this feed? Or is there another way to avoid 429's?",,False,,t5_2qizd,False,,,True,t3_1b94zr,http://www.reddit.com/r/redditdev/comments/1b94zr/hitting_feed_every_10_minutes_still_it_gives_me/,
1364421580.0,9,self.redditdev,1b4ywk,"Calling API, but getting Curl Error: ""Couldn't resolve host"".  Any thoughts",9,0,1,http://www.reddit.com/r/redditdev/comments/1b4ywk/calling_api_but_getting_curl_error_couldnt/,"Curl error: Couldn't resolve host 'www.reddit.com'

Calling reddit with php api wrapper.  I've modified the user-agent call to be unique.  I'm using the api to get the saved.json data.  The call goes through sometimes but then stops for a while.  Is there a limit that I'm surpassing?  Any advice would be appreciated.",,False,,t5_2qizd,False,,,True,t3_1b4ywk,http://www.reddit.com/r/redditdev/comments/1b4ywk/calling_api_but_getting_curl_error_couldnt/,
1363617812.0,7,self.redditdev,1aixtk,Reddit API Keyword,11,4,10,http://www.reddit.com/r/redditdev/comments/1aixtk/reddit_api_keyword/,"So I'm reallllllly new to dev work. I want to just do a simple HTTP GET and search based on a keyword. Does this require Auth or should I be able to do it from a ""public"" point-of-view? What would the proper call be?",,False,,t5_2qizd,False,,,True,t3_1aixtk,http://www.reddit.com/r/redditdev/comments/1aixtk/reddit_api_keyword/,
1362624200.0,8,self.redditdev,19tiyz,Using PRAW to search posts with a title keyword,8,0,4,http://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/,"I am getting started w/ praw-dev and from other posts it looks like there is a limit of 1000 results that cannot be violated. What I am trying to do is to crawl for posts with title string containing key word (say ""happy dog"") along with an image. I don't need the comments. Can anyone please recommend (or point me to sample code) for this ? I am wondering if there is a way to get just the title + image associated without burdening the backend (and hopefully somehow get past the 1000 limit). Thanks! 

I don't need to do this real-time - so pacing requests are perfectly fine.",,False,,t5_2qizd,1362625676.0,,,True,t3_19tiyz,http://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/,
1362121650.0,9,self.redditdev,19g943,Any way to glean comments from a link?,9,0,2,http://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/,"Hello, Redditdev!

I want to pull the score and author of the comments attached to a link. Preferably on a timely basis. Can get_comments be used on a submission? Is there a way to do this using PRAW that I'm missing?

Thanks! :D

**P.S.** Shout out to bboe and Deimorz for all your help!",,False,,t5_2qizd,False,,,True,t3_19g943,http://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/,
1361832159.0,8,self.redditdev,197x36,Using OAuth to send valid requests,8,0,9,http://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/,"I'm trying to build an OAuth library for Android and I'm starting off OAuth2 with Reddit. I have successfully gotten an access token but I can not seem to figure out how to send this to validate requests. I'm starting on the simple https://oauth.reddit.com/api/me.json endpoint and I'm putting as Post Data access_token [my access token]. I always receive a 403 forbidden ""request forbidden by administrative rules"". Are there other parameters I need to send along or am I doing something completely wrong?",,False,,t5_2qizd,False,,,True,t3_197x36,http://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/,
1360250961.0,8,self.redditdev,182ggq,Installation script for CentOS?,8,0,3,http://www.reddit.com/r/redditdev/comments/182ggq/installation_script_for_centos/,"Hey all,

We want to do some experiments with a fresh installation of reddit, and our sysadmin is asking if we have an equivalent of this script: https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu

... but for CentOS?

I'm guessing it can be adapted but maybe someone has a script that is already written/tested?

Cheers,",,False,,t5_2qizd,False,,,True,t3_182ggq,http://www.reddit.com/r/redditdev/comments/182ggq/installation_script_for_centos/,
1359944609.0,8,self.redditdev,17ubp3,"More on srutils - extended to support backup/restore, to install prepackaged themes, etc",9,1,0,http://www.reddit.com/r/redditdev/comments/17ubp3/more_on_srutils_extended_to_support_backuprestore/,"Hi there! I recently [posted about](http://www.reddit.com/r/redditdev/comments/17rxm1/srutils_manipulate_your_subreddits_from_the/) srutils, but I wanted to stop by again and mention an important new feature: the ability to backup and restore subreddits. You can use it to create a local zip archive of:

* CSS
* Subreddit images
* Flair templates
* Settings
* Sidebar

And then you can later restore this is something nasty happens. You can also create backups of subreddits you don't moderate, if that's your kind of thing.

Additionally, you can use this tool to produce pre-packaged subreddit themes, like /r/edurne, and distribute them to other moderators, who can then easily apply those themes to their subreddits.

To compliment these changes, I also added the ability to specify the username/password of a moderator from the command line arguments when you run it, in case you want to put it in cronjob or something similar. See the readme for details.

[Information and source code](https://github.com/SirCmpwn/srutils)

[Direct download](http://sircmpwn.github.com/srutils/srutil.exe)

Let me know what you think, feedback is very welcome. If you can think of other nifty features, I'd love to add them.",,False,,t5_2qizd,False,,,True,t3_17ubp3,http://www.reddit.com/r/redditdev/comments/17ubp3/more_on_srutils_extended_to_support_backuprestore/,
1359739053.0,7,self.redditdev,17pbiv,Where do i find my History or Gold Credit spending? ,8,1,0,http://www.reddit.com/r/redditdev/comments/17pbiv/where_do_i_find_my_history_or_gold_credit_spending/,"Where do i find a log of my gold credit spending, i could see sent messages and all sorts up to my ip address but couldn't find it, Thanks!",,False,,t5_2qizd,False,,,True,t3_17pbiv,http://www.reddit.com/r/redditdev/comments/17pbiv/where_do_i_find_my_history_or_gold_credit_spending/,
1359558189.0,7,self.redditdev,17ka6l,oauth: invalid redirect_uri parameter,8,1,6,http://www.reddit.com/r/redditdev/comments/17ka6l/oauth_invalid_redirect_uri_parameter/,"Hi - I'm working on a website and I'd like to allow users to log in with reddit.  I'm using the ruby library jackdempsey/omniauth-reddit (github).  I get to the login screen ok, but after I log in, I see ""invalid redirect_uri parameter"".  The URL itself seems to work fine: http://comments-enabled.com/auth/reddit/callback

The only thing I can think of is that the URL is not https.  Is that a requirement?  I don't see that documented anywhere.

Thanks!",,False,,t5_2qizd,False,,,True,t3_17ka6l,http://www.reddit.com/r/redditdev/comments/17ka6l/oauth_invalid_redirect_uri_parameter/,
1359484589.0,9,self.redditdev,17ibuu,Is there a subreddit to test posts using the API?,9,0,4,http://www.reddit.com/r/redditdev/comments/17ibuu/is_there_a_subreddit_to_test_posts_using_the_api/,"I want to test my iOS application (a whisky app that can submit reviews in the format expected on /r/scotch) and was looking for a subreddit that I could test my posts on. Is there such an animal?

I hope this is the right place to post this and if not is there a suggestion for a better subreddit to target?",,False,,t5_2qizd,False,,,True,t3_17ibuu,http://www.reddit.com/r/redditdev/comments/17ibuu/is_there_a_subreddit_to_test_posts_using_the_api/,
1359428108.0,8,self.redditdev,17h0al,Enable multiple background-image URLs in CSS,9,1,2,http://www.reddit.com/r/redditdev/comments/17h0al/enable_multiple_backgroundimage_urls_in_css/,"Could the subreddit stylesheet validation be relaxed to permit [multiple background images](http://www.css3.info/preview/multiple-backgrounds/)?

That would allow a small number of sprites to be combined in a single fake link to form many possible arrangements. Each arrangement would need a single CSS declaration but would not need its own unique sprite.

A simple example, with 2 layers built selected from 3 sprites (a, b, c) in a single PNG:

    a[href=""#ab""]{
      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);
      background-position: -100px -100px, -200px -100px;
    }
    a[href=""#ac""]{
      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);
      background-position: -100px -100px, -300px -100px;
    }
    a[href=""#bc""]{
      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);
      background-position: -200px -100px, -300px -100px;
    }

With corresponding definitions for `#a`, `#b`, `#c`, just 3 base sprites could be composed into 6 combinations, instead of needing 6 different precomposed sprites. (If the layer ordering matters, then 3 permuted sprites would enable up to 9 distinct effects.)

The flexibility and relative efficiency would be even greater with more sprites or more background layers.

This would allow commenters to build complex characters from simple fake links.",,False,,t5_2qizd,False,,,True,t3_17h0al,http://www.reddit.com/r/redditdev/comments/17h0al/enable_multiple_backgroundimage_urls_in_css/,
1359163692.0,9,self.redditdev,17aiml,[PRAW] Getting newest posts to show up as quickly as  possible,9,0,6,http://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/,"Hi guys,

I've just started using PRAW today and I'm absolutely loving it.

I did run into some issues while writing a bot. It's supposed to check a specific subreddit for the newest posts, read their contents and alert me whenever it finds something that matches a certain regular expression within them.

I'm checking for new submissions every 10 seconds, by calling s.get_new_by_date(limit=5) on my subreddit's Subreddit object. The problem is that after the initial call, new posts are usually only fetched after anywhere from 30 seconds to a few minutes AFTER they were created, where I expect them to show up within a maximal amount of 10 seconds from the moment they were created. They do appear when I refresh the page in my browser.

Things I tried doing to resolve this:

- My user agent follows the API guidelines and isn't generic.
- Setting my local praw.ini file so that cache_timeout=5 to ensure I'm not reading from the cache.
- Calling s.refresh() on my Subreddit object after every sleep(10).

Do you guys have any idea what might be causing this to happen? Is it because pages are cached server-side, and so I cannot force a refresh on the user's side? If so, why do pages usually take more than a minute (which is far more than the forced 30-second limitation if there is in fact one) to be detected by my algorithm?


Thanks for reading, I appreciate your time.

Icy",,False,,t5_2qizd,False,,,True,t3_17aiml,http://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/,
1358577246.0,6,self.redditdev,16v5yg,how does 'top' item sort?,10,4,1,http://www.reddit.com/r/redditdev/comments/16v5yg/how_does_top_item_sort/,"The confidence sort seem to rank comments, dose 'top' item use the same algorithm? If not, how does 'top' item sort? 
I'm not familiar with python, your reply can save my time. so thanks in advance",,False,,t5_2qizd,False,,,True,t3_16v5yg,http://www.reddit.com/r/redditdev/comments/16v5yg/how_does_top_item_sort/,
1357950089.0,8,self.redditdev,16esdb,Language specific syntax highlighting in &lt;code&gt; blocks. Options?,8,0,3,http://www.reddit.com/r/redditdev/comments/16esdb/language_specific_syntax_highlighting_in_code/,"This feature is available in about/stylesheet, but afaik it's CSS only and entirely server side. It would be nice if we could enable this on ALL code blocks.

I have noticed there is some remnants of adding this functionality in older commits in the Snudown repository, but it's been removed for whatever reason. Is it worth discussing how we might do this?

The simplest solution would be to include a client-side JS highlighter in this repo. in the Snudown repo, the changes will be very minimal involving the inclusion of a class directive in the &lt;pre&gt; or &lt;code&gt; blocks 
    
    &lt;code class=""prettyprint""&gt;...&lt;/code&gt;

The big issue is the impact the extra overhead incurred by loading a client side highlighter.

Thoughts?",,False,,t5_2qizd,1357951000.0,,,True,t3_16esdb,http://www.reddit.com/r/redditdev/comments/16esdb/language_specific_syntax_highlighting_in_code/,
1356894247.0,7,self.redditdev,15olbr,"Adding ~2000 WikiContributors - Can't use API; simple client side script running into ""SUBREDDIT_RATELIMIT""",9,2,5,http://www.reddit.com/r/redditdev/comments/15olbr/adding_2000_wikicontributors_cant_use_api_simple/,"I'm trying to add in an automated fashion ~2000 approved wiki contributors to our wiki.  (At askscience we want all ~2000 or so panelists to be able to edit the wiki, but not have general users edited it.)

First, I have tried doing it through the API, but couldn't figure out how to do it to add as wikicontributor to a subreddit not to tied to specific wiki page and was getting 403 errors.  Details on my API attempt at the bottom.

Next, I tried doing this through client-side javascript.  Go to the add wiki contributors page manually, logged in as myself, populate an array of users to add (which we maintain in a spreadsheet) and then use the following to add the users (with a delay of adding 1 user every 2 seconds originally; and 5 seconds on my second attempt).  (Yes jQuery could do this more concisely, but basically this just types the name into each form and then simulates a click of the button with a delay of two seconds - using an IIFE and setTimeout).

    for(i=0; i&lt; users.length; i++) {
    setTimeout(
       (function(user) {
           return function() {
              var a = document.getElementById('name');
              a.value = user;
              var b = document.getElementsByClassName('btn')[0];
              var event = document.createEvent('UIEvents');
              event.initUIEvent('click', true, true, window, 1);
              b.dispatchEvent(event);
          }
       })(users[i]), 80000*i);
    }

However, after I added about ~80 contributors last night (at a rate of 1 every 2seconds), I got SUBREDDIT_RATELIMIT error message.  So this morning I tried again, and was able to get another ~80 or so in before the same error message popped up.  This time adding at a rate of 1 every ~~5 seconds~~, ~~10 seconds~~, ~~30 seconds~~, *80 seconds is fine, as is everything over 72 seconds*,

Is there any published limits to avoid this SUBREDDIT_RATELIMIT?  I really want to add ~2000 or so users.

### API Attempt:

Using http://www.reddit.com/dev/api#POST_api_wiki_alloweditor_:act
I tried something like (using python requests module):

     import requests
     headers = {'User-Agent': 'djimbob setting adding ask science panelists'}
     requests.post('http://www.reddit.com/api/wiki/alloweditor/add', data=dict(act='add', page='/r/askscience/wiki/', uh=modhash, username='asksci_throwaway'), headers=headers)

but kept getting 403's ""forbidden - forbidden (reddit.com); you are not allowed to do that - wiki_disabled."" (even though if I went to http://www.reddit.com/r/askscience/about/wikicontributors/
I could add contributors).   I think this is because it was trying to add editors only to a specific page; which is not what I want.  We want the index only editable by mods; the other pages edited by all panelists.",,False,,t5_2qizd,1356974905.0,,,True,t3_15olbr,http://www.reddit.com/r/redditdev/comments/15olbr/adding_2000_wikicontributors_cant_use_api_simple/,
1356846633.0,7,self.redditdev,15nuau,Praw + CSS,8,1,4,http://www.reddit.com/r/redditdev/comments/15nuau/praw_css/,Could someone be so kind as to direct me to any documentation that may exist for using praw to create a new or even write to an existing CSS of a subreddit?,,False,,t5_2qizd,False,,,True,t3_15nuau,http://www.reddit.com/r/redditdev/comments/15nuau/praw_css/,
1356713602.0,7,self.redditdev,15ku6v,Please add /r/subreddit/about/stylesheet to OAuth!,8,1,1,http://www.reddit.com/r/redditdev/comments/15ku6v/please_add_rsubredditaboutstylesheet_to_oauth/,Surely this isn't that hard to do? I need to receive the images a subreddit's using and without OAuth that goes fine (trough /r/subreddit/about/stylesheet) but when trying to get it trough oauth (https://oauth.reddit.com/r/subreddit/about/stylesheet.json) results in a 400 error (which seems to be the 404 of the OAuth domain),,False,,t5_2qizd,False,,,True,t3_15ku6v,http://www.reddit.com/r/redditdev/comments/15ku6v/please_add_rsubredditaboutstylesheet_to_oauth/,
1356425998.0,7,self.redditdev,15eydf,"Video embed meta tags that reddit can parse (such as done with youtube, soundcloud, bandcamp)",10,3,8,http://www.reddit.com/r/redditdev/comments/15eydf/video_embed_meta_tags_that_reddit_can_parse_such/,"When submitting a youtube, soundcloud, or bandcamp link reddit parses the site's meta tags to scrape thumbnail and video url info.  Is it possible for me to implement this on my own website? ",,False,,t5_2qizd,False,,,True,t3_15eydf,http://www.reddit.com/r/redditdev/comments/15eydf/video_embed_meta_tags_that_reddit_can_parse_such/,
1355788772.0,6,self.redditdev,150sws,Awards Page Fix,10,4,1,http://www.reddit.com/r/redditdev/comments/150sws/awards_page_fix/,"*Not sure if this is where to put it, but I found a coding issue in the site's HTML:*

In the awards help page, the table in the bottom just under:
""There are also some special trophies that are rarer/better than awards:"" is broken.

Removing these:
&gt;      &lt;pre class=""wiki""&gt;#
&gt;      &lt;/pre&gt;

Then fixing the table's &lt;tr&gt;'s and &lt;td&gt;'s will fix the issue with the table not displaying.


",,False,,t5_2qizd,1355789476.0,,,True,t3_150sws,http://www.reddit.com/r/redditdev/comments/150sws/awards_page_fix/,
1352689588.0,9,self.redditdev,131k4a,Subreddit rates of update ,10,1,3,http://www.reddit.com/r/redditdev/comments/131k4a/subreddit_rates_of_update/,"Hi Devs, forgive me if this is an imprecise question:  I'm posing a question for a more technical colleague from whom English is a second language:

1.  How frequently are subreddits are updated?
2.  Does the update frequency vary by the sort function?  For example, does new update more frequently than whats hot?

We've been hunting around for documentation but haven't found any.",,False,,t5_2qizd,False,,,True,t3_131k4a,http://www.reddit.com/r/redditdev/comments/131k4a/subreddit_rates_of_update/,
1348117712.0,7,self.redditdev,106frx,"Is there any way to use reddit accounts on another website, rather then having to create and manage yet-another account. (""Sign in with Reddit"")",9,2,5,http://www.reddit.com/r/redditdev/comments/106frx/is_there_any_way_to_use_reddit_accounts_on/,"I'd like to authenticate reddit users on a different website, without asking for their password. It would be nice to make a website where redditors can go and retain their username, and not have to make a different account. 

For example, a fake stock market for redditors. It would be nice if people could retain their identity and not have to make a new account (which wouldn't make it ""for redditors"").

I looked at the API but I don't see anyway to _Sign in with Reddit_.",,False,,t5_2qizd,False,,,True,t3_106frx,http://www.reddit.com/r/redditdev/comments/106frx/is_there_any_way_to_use_reddit_accounts_on/,
1347477544.0,8,self.redditdev,zs41u,How does Reddit deal with bots?,9,1,3,http://www.reddit.com/r/redditdev/comments/zs41u/how_does_reddit_deal_with_bots/,"I'm currently in the process of building a content aggregator somewhat like Reddit but a bit more specialised, however at the moment there's a huge problem with the number of bots that are on the site. I've implemented time between registration and captchas much like Reddit does but I'm still having the same problem.
I'm not sure this is the right subreddit for this question, if it's not, then please point me in the direction where I can hopefully can some answers :) Thanks!",,False,,t5_2qizd,False,,,True,t3_zs41u,http://www.reddit.com/r/redditdev/comments/zs41u/how_does_reddit_deal_with_bots/,
1346886540.0,9,self.redditdev,zf378,Possible API patch: Batch GET command by_id?,9,0,4,http://www.reddit.com/r/redditdev/comments/zf378/possible_api_patch_batch_get_command_by_id/,"Hello, Dev!

I'm an intern for CenSoC (The Center for the Study of Choice) at the University of Technology, Sydney. We are currently doing a project involving various structures that implement ""Follow the Leader"" type dynamics. We look to see if the choices that people make are dependent on the influence of ""leaders"" in their community, and try to analyze why this is/is not an attribute of the community.

We would like to do some simple data mining through the Reddit API, since Reddit is already effectually a simple choice experiment (upvotes/downvotes). We were planning on collecting content and user data to see how they influence each other, however, we hit a snag:

If we try to request user info one at a time, with a 2 second delay between each, the time taken to collect all of the data would be overdrawn. We have been considering restructuring the data mining approach, but we also wanted to know if a patch would be simple enough to allow GET requests by_id. Something of this nature:
    


in the file which implements GET username about API:    

    /r2/r2/controllers/listingcontroller.py, line ~679

to go from something like this:
    

    def GET_about(self, vuser):

        ...

        return Reddit(content = Wrapped(vuser)).render()

        ...

to something like the following, where the username(s) url portion is split using regex (much like in the 'by_id' api code):

    reddit.com/user/userA,userB,userC/about.json

... and then looped on the server-side appending to the returned http response without having to re-initiate the http request/response every time:

    def GET_about(self, usernames):

        ...    
        splitter = re.compile('[ ,]+')    
        return Reddit(content = [Wrapped(x) for x in in splitter.split(usernames)]).render()    
        ...
    

obviously, the correct Python-syntax and various variable-validation in reddit arch. will be necessary, but the logic of the above, albeit incorrect Python-syntax-wise, remains.

    

Would something like this be feasible? We currently don't have any team members who are well-versed in Python, and we also don't know how things look on Reddit's end.

Thanks, Reddit! :D",,False,,t5_2qizd,1346886921.0,,,True,t3_zf378,http://www.reddit.com/r/redditdev/comments/zf378/possible_api_patch_batch_get_command_by_id/,
1346069892.0,8,self.redditdev,ywevx,"Does the reddit source API deny heavy usage, like reddit.com?",9,1,11,http://www.reddit.com/r/redditdev/comments/ywevx/does_the_reddit_source_api_deny_heavy_usage_like/,"My primary usage for the reddit source would be via the fantastic API features, but I recall reading about denied service from the reddit.com API when used regularly (I think it was over 30 calls a minute from the same address, maybe even much less).

Does anybody know if the source API is also restricted in such a way? I sure hope not, or if there is a workaround.",,False,,t5_2qizd,False,,,True,t3_ywevx,http://www.reddit.com/r/redditdev/comments/ywevx/does_the_reddit_source_api_deny_heavy_usage_like/,
1344395421.0,9,self.redditdev,xv08c,Is Reddit updating post rankings in real-time or is it using a background job to update them?,9,0,2,http://www.reddit.com/r/redditdev/comments/xv08c/is_reddit_updating_post_rankings_in_realtime_or/,"As the rankings depend on time, number of votes, they change constantly over time. So I was wondering if Reddit is updating the rankings constantly over time, in a background process?",,False,,t5_2qizd,False,,,True,t3_xv08c,http://www.reddit.com/r/redditdev/comments/xv08c/is_reddit_updating_post_rankings_in_realtime_or/,
1342738881.0,7,self.redditdev,wud8k,Use API to create white label site?,8,1,6,http://www.reddit.com/r/redditdev/comments/wud8k/use_api_to_create_white_label_site/,"I took a look around the API docs and the usage guidelines, but couldn't find any information addressing what I'd like to do.

I'm working on a site that allows users to create bits of content. Aside from that I'd like the site to have every piece of reddit functionality (voting, commenting). I was thinking it might be neat to create a subreddit, and then use the API to leverage reddits voting and commenting engine.

Is this sort of usage discouraged? Is there a better way to do it than using the API?",,False,,t5_2qizd,False,,,True,t3_wud8k,http://www.reddit.com/r/redditdev/comments/wud8k/use_api_to_create_white_label_site/,
1342093129.0,7,self.redditdev,wfqr9,Is there a subreddit for posting reddit installation-related queries?,10,3,3,http://www.reddit.com/r/redditdev/comments/wfqr9/is_there_a_subreddit_for_posting_reddit/,"I've posted queries on this subreddit before, but haven't got much replies. I know this is a subreddit for the reddit API client and the reddit source code.  But my first assumption was that we could post queries here if we had problems with our reddit installation. I'm not cribbing but that's what I assumed. 

Anyway, I went through [Raerth's Moderation Guide] (http://www.reddit.com/r/raerth/comments/f2xrr/raerths_moderation_guide/) but couldn't find any mention of a subreddit where you could post queries related to installing reddit - stuck with an installation, installation gone wrong, features not working after a new installation - a place where you could say, *""Help reddit, I just installed my reddit site, but I'm stuck with this problem""* etc. 

So, getting back to the topic, is there such a subreddit where fellow redditors help out each other with reddit installation problems?",,False,,t5_2qizd,1342103339.0,,,True,t3_wfqr9,http://www.reddit.com/r/redditdev/comments/wfqr9/is_there_a_subreddit_for_posting_reddit/,
1338283085.0,8,self.redditdev,ua3gk,API access to /new{.json/.xml} disabled?,10,2,10,http://www.reddit.com/r/redditdev/comments/ua3gk/api_access_to_newjsonxml_disabled/,"It seems that you can no longer access the json or xml version of /new for any subreddit. It works fine in the browser, but I have tested multiple scripts from multiple domains with no luck.

Am I the only one having this issue?",,False,,t5_2qizd,False,,,True,t3_ua3gk,http://www.reddit.com/r/redditdev/comments/ua3gk/api_access_to_newjsonxml_disabled/,
1336318098.0,8,self.redditdev,t9r5i,"Is there a way to do nameglobs for subreddits? 
Trying to figure out a way to do autocomplete for 
subreddit search",8,0,10,http://www.reddit.com/r/redditdev/comments/t9r5i/is_there_a_way_to_do_nameglobs_for_subreddits/,"I currently have Opera set up so that one of my custom search engines is http://www.reddit.com/r/%s - this means if I type ""r redditdev"" in my address bar I get straight to http://www.reddit.com/r/redditdev.

I thought it might be cool to extend this to have autocomplete, so I could type ""r Ask"" it pops up AskReddit, AskScience, etc. To do this, I need a URL which when passed ""Ask"" returns a JSON list of subreddits whose names start with ""Ask"". However, AFAIK, no such URL exists in the API.

I could host the URL myself, but first I need a way to get a machine readable list of **all** subreddits. I could grab them from http://www.reddit.com/reddits, but that only returns them 25 at a time, and includes a whole lot of extra data besides the name (which is all I need), so it's a bit impractical, especially to keep the list up-to-date.

UPDATE: I'm running a perl script to scrape http://www.reddit.com/reddits.json, but you can only get 25 results per request. It's been running for over 15 minutes now, and 15,600 reddits. I might be able to run it as a cron job, but probably no more than once a day. I guess the list doesn't need to be up-to-the-minute. I'll publish a link to the list in case anyone else needs it, to save reddit from being scraped by too many robots.

UPDATE2: OK, got it working eventually, thanks to ketralnis. I was able to get the data from /api/search_reddit_names.json, but it wasn't directly compatible with Opera search suggestions because a) It requires POSTed params and b) the JSON object wasn't quite in the correct format. I wrote a perl script to act as a gateway, which you can see [here](http://pastebin.com/83RkaNtX) if you are interested. [Here](http://pastebin.com/WXZjKbrD) is the relevant section of %APPDATA%\Opera\search.ini",,False,,t5_2qizd,True,,,True,t3_t9r5i,http://www.reddit.com/r/redditdev/comments/t9r5i/is_there_a_way_to_do_nameglobs_for_subreddits/,
1336229381.0,9,self.redditdev,t8dbj,Cleaning and Backing up Database?,10,1,1,http://www.reddit.com/r/redditdev/comments/t8dbj/cleaning_and_backing_up_database/,"If I wanted to delete everything (all submissions, users, etc.), would the best way be to just delete the postgresql ""reddit"" database, then recreate it and set it up using the sql/functions.sql file?  Is that safe / complete?

Similarly, if I wanted to back up everything then should I just dump or replicate the ""reddit"" database?  Would that miss anything?  (Presuming my code/config is already saved elsewhere.)",,False,,t5_2qizd,False,,,True,t3_t8dbj,http://www.reddit.com/r/redditdev/comments/t8dbj/cleaning_and_backing_up_database/,
1336228423.0,8,self.redditdev,t8ctu,Reddit Deployment,9,1,1,http://www.reddit.com/r/redditdev/comments/t8ctu/reddit_deployment/,"So I have (mostly) completed my modifications and would like to deploy my reddit to a production server.  Ideally, I would like to have it running on an existing apache/php server (since it is serving the domain I would like to use with related content).  It looks like the best? way to do this is use mod_wsgi, then configure the virtualhost to serve both.  I've run in to a couple questions trying to do this:

1) Is there a good way to package the code for production on a different machine?  I used the [Ubuntu 11.04 install script](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) which worked great.  I was thinking I may want to just use that again, then drop in my modified code...  is that the best way to go?  I know typically you can package it as an egg, but it seems like that would be more difficult (and may not be optimized).

2)  What all does the `make` do?  It looks like it is compiling the python to c for optimization?  How do I ensure that this is the optimized/compiled code being used?  It appears that the uwsgi server that was configured by the install script uses the same run.ini as my development paster server (which I can tell uses the regular .py files).  

3)  Will reddit have problems running in a sub-directory on the apache2/mod_wsgi server?  Will any of the internal links, etc. break?

4)  Does anyone have any tips for running php and wsgi on the same apache server?  The closest I could find was [this](http://serverfault.com/questions/226449/how-can-django-wsgi-and-php-share-on-apache).

5)  What exactly are the SECRET, MODSECRET, FEEDSECRET, ADMINSECRET, and traffic_secret in the .ini?  They look important, and like I should change them...

6) Any other tips or suggestions?


Thanks!",,False,,t5_2qizd,False,,,True,t3_t8ctu,http://www.reddit.com/r/redditdev/comments/t8ctu/reddit_deployment/,
1333402630.0,8,self.redditdev,rq4ok,Login cookie/modhash expiration time and is consistently having ~3 small reqs in a second bad if they are spaced out by ~hr plus?,8,0,6,http://www.reddit.com/r/redditdev/comments/rq4ok/login_cookiemodhash_expiration_time_and_is/,"Hi, I'm developing an browser extension to help us AskScience Mods and have a few questions.

1. How often do session cookies / modhashes expire?  Could I login once (or once an hour/day/week/month) and then continue to use the same modhash/cookie?  Looking at the python source the session cookie doesn't appear to ever expire (the check of the cookie `datetime+user+sha(datetime+user+secret)` reads datetime+user from the cookie and then compares to the secret).  However, the modhash for CSRF protection seems to just be a placeholder in the reddit source.

2. I plan on giving mods a few browser buttons that will combine a couple separate requests into a single action; e.g., mod presses a button that uses the API to 'post a (brief) comment', 'send a (brief) message', and 'change user's flair'.  Should I wait 2 seconds between the three separate requests or can I send request 1, wait for response 1; then send request 2, wait for response 2; then send request 3, wait for response 3)?  I expect these actions to happen rarely after testing finishes, less than 20 times a day (60 total requests daily) -- nowhere near the limit of 60 reqs/minute.  So, I'd prefer not to have to wait 4+ seconds to finish the combined requests, so they can get quicker feedback of success/failure.  Is say 3 requests in say a second bad if the next request isn't for tens of minutes later?  (This is also part of the reason I want to login once; so its not four requests).",,False,,t5_2qizd,False,,,True,t3_rq4ok,http://www.reddit.com/r/redditdev/comments/rq4ok/login_cookiemodhash_expiration_time_and_is/,
1332267227.0,7,self.redditdev,r5e5l,Python Reddit API Developers: Python 2/3 hybrid beta testing needed,9,2,5,http://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/,"Edit: 1.3.0 is official now. `pip install reddit` or `pip install --upgrade reddit` will install six if it is not already installed.

I recently finished adding hybrid support for both Python 2.6+ and 3.2+ and I would really appreciate if some PRAW users would upgrade to the 1.3.0dev version of PRAW to ensure everything works as expected. I have extended PRAW's tests to include unicode testing and thus given that all the tests pass on both 2.6, and 3.2 I don't expect there to be any issues. Nevertheless it doesn't hurt to be sure.

1.3.0dev is not available via pip, thus you'll need to manually fetch it from [github](https://github.com/bboe/reddit_api/tree/python3). Going forward PRAW will depend on the `six` module which can be obtained via `pip install six`. To make sure you are running the correct version you can run:

`python -c 'import reddit; print(reddit.VERSION)'`

The output should be `1.3.0dev`.

If you run into any issues, please follow up either here, or on #reddit-dev in irc.freenode.net. Thanks!",,False,,t5_2qizd,True,,,True,t3_r5e5l,http://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/,
1327909202.0,9,self.redditdev,p2rqf,I want to see all of the subreddits!,10,1,16,http://www.reddit.com/r/redditdev/comments/p2rqf/i_want_to_see_all_of_the_subreddits/,"So, just for fun, I want to make a reddit profile that is subscribed to all of the subreddits.  You're probably asking ""Why don't you just use the ""All"" tab?"" Well my goal is to manually unsubscribe from boring/bad/big subreddits as I browse so I can find some hidden gems.

I understand that I need to use subscribe(&lt;subreddit id&gt;), so to subscribe to Pics would be ""subscribe('t5_2cneq')""

Two problems I see so far is the request limit, and how to compile a list of all the public subreddits.

The 1 request every two seconds wouldn't be hard to program, but will just mean the script will take a long time. 

Getting a list of every subreddit seems like it could be very hard.  Luckily metareddit, [post the source code of their spider](https://github.com/modemuser/metareddit), so I might be able to modify and simplify that.  Or I could just parse every page of [www.reddit.com/reddits](www.reddit.com/reddits).

So I was looking for some input on my idea, has it been done before?  Any ideas about compiling a list of subreddits?  Thanks a lot.",,False,,t5_2qizd,False,,,True,t3_p2rqf,http://www.reddit.com/r/redditdev/comments/p2rqf/i_want_to_see_all_of_the_subreddits/,
1323360577.0,8,self.redditdev,n4y4v,Understanding how comments are rendered...,11,3,5,http://www.reddit.com/r/redditdev/comments/n4y4v/understanding_how_comments_are_rendered/,"Hello,

I've been trying to understand how reddit renders the comments tree and was hoping someone could help me out.  What I'm specifically trying to do is to have a comment not be displayed when a flag is set.  This isn't for any specific purpose, I'm just trying to understand the code at this point.  I'm not familiar with pylons/mako (although I know the basics), but am pretty experienced with using python.

The best i've been able to do so far is a crude hack of the template comment_skeleton.html, where I define blocks as empty if the flag is set.  When I've tried digging deeper into how things are done, I end up tumbling down the rabbit hole of inherited classes without much insight.  For example, I tried following the simple route from the /c/COMMENT_ID to the comment_by_id action of the front controller.  If you follow it far enough, you end up at the _render function of Templated class, which I think probably calls something else to do the rendering depending on the context.  In principal, the right way to implement my if flag, no print action would be to change the python code rather than the template (at least I think it is), but I'm utterly lost as to how to do it.

tl;dr  If anyone can explain to me how comments are rendered and where the best place to change how they're rendered is, I would be eternally gratefu",,False,,t5_2qizd,False,,,True,t3_n4y4v,http://www.reddit.com/r/redditdev/comments/n4y4v/understanding_how_comments_are_rendered/,
1322643057.0,8,self.redditdev,mujsd,Is reddit's API open and where can I find it?,9,1,6,http://www.reddit.com/r/redditdev/comments/mujsd/is_reddits_api_open_and_where_can_i_find_it/,,,False,,t5_2qizd,False,,,True,t3_mujsd,http://www.reddit.com/r/redditdev/comments/mujsd/is_reddits_api_open_and_where_can_i_find_it/,
1319216749.0,7,self.redditdev,lk3n2,Is it possible to use the reddit comment system as the comment system for a wordpress blog?,12,5,6,http://www.reddit.com/r/redditdev/comments/lk3n2/is_it_possible_to_use_the_reddit_comment_system/,"I'm guessing you could skin a custom reddit install to look like a blog, then use self posts as your blog posts and let people comment/vote as usual.

But could you somehow integrate just the comment/voting system used by reddit into a wordpress site?",,False,,t5_2qizd,False,,,True,t3_lk3n2,http://www.reddit.com/r/redditdev/comments/lk3n2/is_it_possible_to_use_the_reddit_comment_system/,
1317231980.0,8,i.imgur.com,kuagz,Installed reddit 2 times on debian with no problem. I now get this error after installing on CentOS. It happens randomly when I browse through the site. Have to re-GET several times.,12,4,5,http://www.reddit.com/r/redditdev/comments/kuagz/installed_reddit_2_times_on_debian_with_no/,,,False,,t5_2qizd,False,,,False,t3_kuagz,http://i.imgur.com/slOR3.png,
1314889497.0,7,github.com,k1cy1,Teaching myself Python. Made a script to sync friends across multiple reddit accounts. Constructive criticism appreciated.,9,2,11,http://www.reddit.com/r/redditdev/comments/k1cy1/teaching_myself_python_made_a_script_to_sync/,,,False,,t5_2qizd,False,,,False,t3_k1cy1,https://github.com/gehsekky/reddit-scripts/blob/master/reddit_friend_sync.py,
1312736179.0,9,self.redditdev,jbmph,I have fixed an error in the german translation. Is it alright to issue a pull request?,12,3,1,http://www.reddit.com/r/redditdev/comments/jbmph/i_have_fixed_an_error_in_the_german_translation/,"The translations for ""what's new"" and ""what's hot"" are exchanged in the german translation. I forked [reddit-i18n](https://github.com/reddit/reddit-i18n) and fixed it. Should I just issue a pull request or is there another way to fix the translations? ([this](http://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/) suggests that there is a translations interface somewhere, but my google-fu seems weak)",,False,,t5_2qizd,False,,,True,t3_jbmph,http://www.reddit.com/r/redditdev/comments/jbmph/i_have_fixed_an_error_in_the_german_translation/,
1312233622.0,9,self.redditdev,j61st,Is there a limit on what /messages/unread.json will return?,11,2,4,http://www.reddit.com/r/redditdev/comments/j61st/is_there_a_limit_on_what_messagesunreadjson_will/,"I'm wondering what would happen in the case where my user has **a lot** of messages waiting in his inbox and I GET /messages/unread.json??

Will I get all of the messages or just a portion of them?  If I only get a portion, how do I get the remaining messages?

And if I only get a portion of the messages, when I GET /messages/inbox/ does that mark **all** unread messages read or just the portion of them?",,False,,t5_2qizd,False,,,True,t3_j61st,http://www.reddit.com/r/redditdev/comments/j61st/is_there_a_limit_on_what_messagesunreadjson_will/,
1309357913.0,8,self.redditdev,icbh1,How do I use the 'more' entries in the json reply to load more comments? ,8,0,23,http://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/,"I'm trying to load all comments in a thread, and I'm having a hard time figuring out how to fetch more than the standard reply. For instance, when I load this story: 

    http://www.reddit.com/r/technology/comments/ib83z/us_navy_bought_59000_microchips_that_turned_out/.json

I get children with the following format (the formatting is my own): 

    ['kind'] = 'more' --- &lt;type 'unicode'&gt;
    ['data']['name'] = 't1_c22d3fb' --- &lt;type 'unicode'&gt;
    ['data']['id'] = 'c22d3fb' --- &lt;type 'unicode'&gt;

When I follow the instructions for loading additional comments, I call: 

    http://reddit.com/comments/ib83z/c22d3fb.json

and pretty much the original page. What gives? ",,False,,t5_2qizd,False,,,True,t3_icbh1,http://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/,
1308738317.0,7,self.redditdev,i635r,"I want to help you out with the german translation, but I don't know anything about github. Could somebody give me a brief introduction?",11,4,4,http://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/,,,False,,t5_2qizd,False,,,True,t3_i635r,http://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/,
1299369099.0,8,techcrunch.com,fy53l,Revealed my start up on TechCrunch! Inspired by reddit but as an LBS. The original proof of concept was a reddit fork called Geodit.,17,9,4,http://www.reddit.com/r/redditdev/comments/fy53l/revealed_my_start_up_on_techcrunch_inspired_by/,,,False,,t5_2qizd,False,,,False,t3_fy53l,http://techcrunch.com/2011/03/05/view/,
1299025402.0,7,self.redditdev,fvf8u,reddit downtime?,8,1,0,http://www.reddit.com/r/redditdev/comments/fvf8u/reddit_downtime/,"I'm trying to figure out some statistics I'm getting from an index I'm building in solr. I am crawling /r/worldnews.json every 15 minutes and there are 25 headlines in there. I should have 2400 (25*4*24) items in solr per day but I have a variety of numbers instead. Is it me, is it reddit? I don't know.

My lowest number is 303 on Feb 26th 2011.

EDIT: I just realized that my own hourly statistics contradicts the daily statistics so I'm confused even more (hopefully no one saw the original and clearly stupid post :( ).

Is there a json equivalent of the reddit is down cute image? Perhaps an http error 500 code?

--thanks",,False,,t5_2qizd,False,,,True,t3_fvf8u,http://www.reddit.com/r/redditdev/comments/fvf8u/reddit_downtime/,
1292206403.0,8,self.redditdev,ekrig,Logging in via POSTs to /api/login causes account lockout?,8,0,12,http://www.reddit.com/r/redditdev/comments/ekrig/logging_in_via_posts_to_apilogin_causes_account/,"Hi all,

To start with, I'm very new to using the Reddit API so I apologize for my  naivet. I'm currently writing an app which allows the user to log in. I'm doing a POST to http://www.reddit.com/api/login/&lt;username&gt; with the following as the request body:

op=login-main&amp;user=&lt;username&gt;&amp;passwd=&lt;password&gt;&amp;id=%23login_login-main&amp;r=redditdev&amp;renderstyle=html

I believe I copied that string out of the regular login requests that I was sniffing. Admittedly, I have little idea what most of those params mean (op, id, renderstyle).

In any case, these logins are generally successful for around 10 times. After that, it seems that my account is basically locked out of reddit. In other words, I can't log in via my app or via reddit.com. The only way out of this seems to be to reset my password on reddit and log in again. Even then, after the inital reset of the password, I'm still unabel to log in to reddit again.

Any idea what I might be doing wrong? I've looked around /redditdev and the API docs, but I haven't found anything. I did find a post from a while ago complaining about a similar problem when using the iPhone app, but there was no resolution to the problem in the thread.

Thanks for your help!

**EDIT**: Just got a raging clue from this code:

    if login_throttle(username, wrong_password = form.has_errors(""passwd"",
                                                     errors.WRONG_PASSWORD)):
            VRatelimit.ratelimit(rate_ip = True, prefix = 'login_', seconds=1)

            c.errors.add(errors.WRONG_PASSWORD, field = ""passwd"")

I'm guessing I've just tried to log in too many times from the same IP in the last couple hours. Does this explain it?",,False,,t5_2qizd,True,,,True,t3_ekrig,http://www.reddit.com/r/redditdev/comments/ekrig/logging_in_via_posts_to_apilogin_causes_account/,
1284129968.0,9,self.redditdev,dc3is,Any universities running a local reddit site?,10,1,2,http://www.reddit.com/r/redditdev/comments/dc3is/any_universities_running_a_local_reddit_site/,Or have a dedicated sub-reddit?,,False,,t5_2qizd,False,,,True,t3_dc3is,http://www.reddit.com/r/redditdev/comments/dc3is/any_universities_running_a_local_reddit_site/,
1276564318.0,8,pastebin.com,cf1ar,Search reddit and sort by karma - proof of concept,11,3,4,http://www.reddit.com/r/redditdev/comments/cf1ar/search_reddit_and_sort_by_karma_proof_of_concept/,,,False,,t5_2qizd,False,,,False,t3_cf1ar,http://pastebin.com/919p9VEq,
1269990779.0,10,self.redditdev,bkepo,Clearing up #subreddit message confusion,10,0,0,http://www.reddit.com/r/redditdev/comments/bkepo/clearing_up_subreddit_message_confusion/,"After some recent confusions, the labeling in the #subreddit (moderator) messages should probably look more like:


* When a user sends a message to a mod inbox, it should say from [RANDOM\_USER] to [#subreddit]

* When a moderator replies it should say from [CAPTAIN\_MODERATOR] via [#subreddit] to [RANDOM\_USER], [#subreddit]

* When a user responds to a moderator response: from [RANDOM\_USER] to [CAPTAIN\_MODERATOR], [#subreddit]


I think this will clarify that it's basically a group inbox where all messges are visible to all respondents.
",,False,,t5_2qizd,False,,,True,t3_bkepo,http://www.reddit.com/r/redditdev/comments/bkepo/clearing_up_subreddit_message_confusion/,
1263920229.0,7,self.redditdev,arip5,Reddit DB dump,8,1,11,http://www.reddit.com/r/redditdev/comments/arip5/reddit_db_dump/,"I found this link.

http://www.reddit.com/r/programming/comments/6q0oq/ask_reddit_where_to_download_a_db_dump_of_reddit/

They said that there would be a reddit db dump.
Is there the db dump of reddit really now?
Actually, I'd like to get DB dumps for only politics and WTF subreddit.
Is it possible?",,False,,t5_2qizd,False,,,True,t3_arip5,http://www.reddit.com/r/redditdev/comments/arip5/reddit_db_dump/,
1262637018.0,8,github.com,alial,I just wrote up a quick tool to export reddit listings to HTML,8,0,7,http://www.reddit.com/r/redditdev/comments/alial/i_just_wrote_up_a_quick_tool_to_export_reddit/,,,False,,t5_2qizd,False,,,False,t3_alial,http://github.com/ketralnis/redditexporter,
1262586322.0,9,self.redditdev,al9qc,"Reddit, I have a feature request.  I want to collapse threads, follow a link, and then click back.",12,3,6,http://www.reddit.com/r/redditdev/comments/al9qc/reddit_i_have_a_feature_request_i_want_to/,"If this worked the way I want, it would make reddit a lot more usable.  Say I open the comments page and start reading comments.  When I am done with a comment thread I collapse it.  After collapsing the first 20 threads, I encounter a link I would like to follow in the 21st thread.  I click the link and it turns out to be boring.  I click back in my browser, and all the comment threads are now re-expanded.

I guess I could ""open link in new tab/window"" but I'm really a one window one tab browsing kind of guy and I never think of this at the time.  If there is a way to preserve the state of the comments page when I go back to it, that would be very cool.",,False,,t5_2qizd,False,,,True,t3_al9qc,http://www.reddit.com/r/redditdev/comments/al9qc/reddit_i_have_a_feature_request_i_want_to/,
1251057930.0,8,self.redditdev,9ddj0,Ask: Why isn't Tinypic blocked?,12,4,1,http://www.reddit.com/r/redditdev/comments/9ddj0/ask_why_isnt_tinypic_blocked/,"It's well known that Tinypic blocks incoming links from Reddit after a cap has been reached, why isn't it just blocked to divert submitters to an alternate image host?",,False,,t5_2qizd,False,,,True,t3_9ddj0,http://www.reddit.com/r/redditdev/comments/9ddj0/ask_why_isnt_tinypic_blocked/,
1245933953.0,8,self.redditdev,8vjrj,Ask: do you know any hosting plan that meets reddit's dependencies?,8,0,10,http://www.reddit.com/r/redditdev/comments/8vjrj/ask_do_you_know_any_hosting_plan_that_meets/,"I'm planning on starting a reddit implementation as a sort of forum for my comic. Every comic strip will have a reddit post, and people can mod it up and down and comment. But my current hosting doesn't meet the dependencies. Besides, every how-to I've seen kind of assumes you're on a dedicated server.
Do you know of any hosting service that will make my reddit work out of the box? (I can't afford a dedicated server).",,False,,t5_2qizd,False,,,True,t3_8vjrj,http://www.reddit.com/r/redditdev/comments/8vjrj/ask_do_you_know_any_hosting_plan_that_meets/,
1230586212.0,7,developer.mozilla.org,7mamu,I noticed that reddit does not have one of these...,10,3,9,http://www.reddit.com/r/redditdev/comments/7mamu/i_noticed_that_reddit_does_not_have_one_of_these/,,,False,,t5_2qizd,False,,,False,t3_7mamu,https://developer.mozilla.org/en/Creating_OpenSearch_plugins_for_Firefox,
1224797198.0,8,code.reddit.com,78zhe,reddit internal API Documentation,9,1,1,http://www.reddit.com/r/redditdev/comments/78zhe/reddit_internal_api_documentation/,,,False,,t5_2qizd,False,,,False,t3_78zhe,http://code.reddit.com/docs/,
1224770384.0,8,reddit.com,78w7b,"Introducing: The greasemonkey subreddit, for reddit-related userscripts.",10,2,2,http://www.reddit.com/r/redditdev/comments/78w7b/introducing_the_greasemonkey_subreddit_for/,,,False,,t5_2qizd,False,,,False,t3_78w7b,http://www.reddit.com/r/GreaseMonkey,
1376801549.0,8,self.redditdev,1kla7e,do i need to check 'needs_captcha' each and everytime something is being posted?,8,0,1,http://www.reddit.com/r/redditdev/comments/1kla7e/do_i_need_to_check_needs_captcha_each_and/,or do i need to do it only once(upon login),,False,,t5_2qizd,False,,,True,t3_1kla7e,http://www.reddit.com/r/redditdev/comments/1kla7e/do_i_need_to_check_needs_captcha_each_and/,
1376715553.0,7,self.redditdev,1kj9zg,Compiling and using metareddit source?,7,0,0,http://www.reddit.com/r/redditdev/comments/1kj9zg/compiling_and_using_metareddit_source/,"Has anyone compiled the metareddit source / figured out how to get it to run? The source is up at https://github.com/modemuser/metareddit

It's written in python, which I'm familiar with, but using frameworks that I haven't done much in (sqlalchemy, werkzeug). 

I've gotten in far in trying to get it to run, but in the end cant get it all the way through, I get stumped at trying to actually init my DB - it seems like I cant get the correct version of alchemy that was initially used, despite having tried them all.

Does anyone have any suggestions or info on how to run this?

thanks",,False,,t5_2qizd,False,,,True,t3_1kj9zg,http://www.reddit.com/r/redditdev/comments/1kj9zg/compiling_and_using_metareddit_source/,
1376349543.0,6,self.redditdev,1k8ocm,Is there any way to easily host a reddit bot using praw on Google App Engine?,8,2,7,http://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/,"I wrote a simple bot for reddit which needs to run every ten minutes or so to adequately perform its job. As I didnt want to leave my computer turned on forever in order to let the script I decided to try hosting it on google app engine.

Now I'm a newb to both app engine and praw, so bear with me here.

When I uploaded my script to the app engine it threw me an error saying that it could not find/import any packages named ""praw."" That's fine. To fix this, and all the later dependency errors that came up, I just ran the following commands in a python shell

    import dependencypackage
    print(dependencypackage)
    
I then copied all the files/folders that the shell spit out into my app's src directory. By doing this a few times I managed to trade one error for another, but now I'm running into the following error:

        Traceback (most recent call last):
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/SpelunkyDailyBot.py"", line 2, in &lt;module&gt;
        import praw
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/praw/__init__.py"", line 34, in &lt;module&gt;
        from praw import decorators, errors
    ImportError: cannot import name decorators

I'm stumped by this one, because when I check the praw folder in my app's src dir there exist both a decorators.py and a decorators.pyc. 

So the question here is twofold-

1) What am I doing wrong here? Is the decorators module not supported by App Engine or something? Can I even use praw in a GAE script? I'd really love to get this bot on the cloud, so this question is the main one

2) Is there any easier way to do this? I feel like the process I've followed to get all the dependencies in the src is one of the least efficient possible. Is there some sort of tool I can use, some sort of magic button I can use in my IDE (pycharm or pydev for eclipse) which will take care of uploading all these dependencies for me?

Answers and explanations are welcome (duh)

Looking forward to your answers!

EDIT: I made some changes to how the packages were imported, and now I'm getting the following error

        Traceback (most recent call last):
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/SpelunkyDailyBot.py"", line 2, in &lt;module&gt;
        import praw
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/praw/__init__.py"", line 43, in &lt;module&gt;
        from update_checker import update_check
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py"", line 87, in &lt;module&gt;
        class UpdateChecker(object):
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py"", line 93, in UpdateChecker
        @cache_results
      File ""/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py"", line 43, in cache_results
        filename = os.path.join(gettempdir(), 'update_checker_cache.pkl')
      File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py"", line 45, in PlaceHolder
        raise NotImplementedError(""Only tempfile.TemporaryFile is available for use"")
    NotImplementedError: Only tempfile.TemporaryFile is available for use",,False,,t5_2qizd,1376352070.0,,,True,t3_1k8ocm,http://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/,
1375555569.0,8,self.redditdev,1jmzm7,[PRAW] Can a bot give gold?,9,1,16,http://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/,"Didn't find anything about it in the reddit API or PRAW documentation.

Is this something the admins object to?",,False,,t5_2qizd,False,,,True,t3_1jmzm7,http://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/,
1374584753.0,6,self.redditdev,1ivr0u,SSL problems on login,7,1,13,http://www.reddit.com/r/redditdev/comments/1ivr0u/ssl_problems_on_login/,"I'm trying to log in via POST to `https://ssl.reddit.com/api/login/` and I'm getting an SSL handshake error on ruby:

`OpenSSL::SSL::SSLError: SSL_connect returned=1 errno=0 state=SSLv3 read server hello A: sslv3 alert handshake failure`

This has worked 2 days ago, but not anymore. Has your certificate changed?",,False,,t5_2qizd,False,,,True,t3_1ivr0u,http://www.reddit.com/r/redditdev/comments/1ivr0u/ssl_problems_on_login/,
1373696606.0,8,self.redditdev,1i7fma,Been messing around with praw but I've been running into issues with the RateLimitExceeded.,8,0,8,http://www.reddit.com/r/redditdev/comments/1i7fma/been_messing_around_with_praw_but_ive_been/,"The bot I""m working on posts reply's in a subreddit but it has to wait ~8 mins or it will get the ""you are doing that too much."" error. 

Is there a way to do some error checking for this or to get around it?",,False,,t5_2qizd,False,,,True,t3_1i7fma,http://www.reddit.com/r/redditdev/comments/1i7fma/been_messing_around_with_praw_but_ive_been/,
1372698598.0,8,self.redditdev,1hfptr,Use getJSON requests to retrieve lots of submissions,9,1,5,http://www.reddit.com/r/redditdev/comments/1hfptr/use_getjson_requests_to_retrieve_lots_of/,"I'm using jQuery's getJSON request to get submissions from a subreddit. The problem is the reddit API limits me to no more than 100 results per request. Is there any way I can get the 100 submissions after the previous hundred, etc?

Here's an example of what I'm doing:

    $.getJSON(""http://www.reddit.com/r/subreddit.json?jsonp=?&amp;limit=100"", function(data) {
        // do something with my data, but I want more!
    }    

edit: grammar",,False,,t5_2qizd,False,,,True,t3_1hfptr,http://www.reddit.com/r/redditdev/comments/1hfptr/use_getjson_requests_to_retrieve_lots_of/,
1372138691.0,6,self.redditdev,1h0scw,Could use some help with creating a FAQ-esque bot,9,3,133,http://www.reddit.com/r/redditdev/comments/1h0scw/could_use_some_help_with_creating_a_faqesque_bot/,"Alternatively if something similar exists and is available for modification that'd be neat, too.

I'm not a dev. I don't understand 80% of what I'm even reading here and I'm sure I could not do anything on my own *right now*. Which is why I want to learn how to!

Rather than copypasta, I'll just link to where my search began: [here](http://www.reddit.com/r/modhelp/comments/1h0lvd/bot_related_need_to_be_pointed_in_the_right/) since I had no idea where to even begin.

Essentially I want to create a bot that will scan a single subreddit for certain key words / parameters and, if appropriate, create a reply to answer certain questions. It's something I'll have to regularly update, too, until it's hopefully no longer necessary.

Think of it this way: If you've ever been a sub of a small subreddit that doesn't see much in terms of new content, old stuff tends to repeat itself very, very often. Rather than turn away people who might not even know how to search through old posts, I want to provide them with some information before they figure the subreddit is completely dead. I just don't want to have to do it manually each time, y'know?",,False,,t5_2qizd,False,,,True,t3_1h0scw,http://www.reddit.com/r/redditdev/comments/1h0scw/could_use_some_help_with_creating_a_faqesque_bot/,
1371872739.0,7,self.redditdev,1gu6y5,Did something just break with PRAW's get_top method?,8,1,5,http://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/,"I'm running a script which sucks down new and top posts to /r/pics. It works fine for new but has recently stopped working for pics. 

Compare, for example, these results via the interpreter, after connecting via PRAW

    new_submissions_generator = r.get_subreddit('pics').get_new(limit=100)
    for submission in new_submissions_generator:
        name = submission.author.name
    name
    u'anoteduser'

Works as expected (assigns and returns username). 

Similarly: 

    hot_submissions_generator = r.get_subreddit('pics').get_hot(limit=25)
    for submission in hot_submissions_generator:
        nameh = submission.author.name
    nameh
    u'preggit'

Works as expected (assigns and returns username). 

But: 

    top_submissions_generator = r.get_subreddit('pics').get_top(limit=25)
    for submission in top_submissions_generator:
        namet = submission.author.name
    Traceback (most recent call last):
    File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
    AttributeError: 'NoneType' object has no attribute 'name'

Breaks with an error that there is no attribute name for the object. 

What is going on? I am 99% sure this wasn't happening a few days ago...",,False,,t5_2qizd,1371874164.0,,,True,t3_1gu6y5,http://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/,
1370928043.0,7,self.redditdev,1g3pd5,Issues with the api's flairlist,7,0,3,http://www.reddit.com/r/redditdev/comments/1g3pd5/issues_with_the_apis_flairlist/,"**The background:**

I've been writing a minecraft mod that lets a user set for themselves on the subreddit. So far, I can log in and set the flair for a user with /api/flair 

My problem is that I want to append the new class, not replace the whole thing. For this, I *think* I need to use /api/flairlist to get their current flair, and then set the new flair based on the results.

**My problem:** 

I can't seem to get /api/flairlist to work. I get a 500 error every time. The [api documentation page](http://www.reddit.com/dev/api) is absolutely terrible at the moment, it doesn't cover the ""r=SUBREDDIT"" parameter anywhere, and many parameters have no description.

**What I need:**

The exact params I need to pass to get the flair of a user

**What I've tried:**

Any and every combination of the following (I think):

    http://www.reddit.com/api/flairlist/ (Both POST and GET)
    r=minez
    &amp;limit=1
    &amp;name=t2_5va4
    &amp;count=1
    &amp;uh=jo0m6mnanm17133909ca4fd1f1ddd29c27...
    &amp;before=t2_5aw8r
    &amp;after=t2_5aw8r
    &amp;limit=1
    &amp;count=1
    &amp;name=amoliski
    &amp;prayersToTheRedditGod=1000",,False,,t5_2qizd,False,,,True,t3_1g3pd5,http://www.reddit.com/r/redditdev/comments/1g3pd5/issues_with_the_apis_flairlist/,
1370201251.0,5,self.redditdev,1fj88q,How do I get a list of my user flair templates? Also other small questions.,8,3,4,http://www.reddit.com/r/redditdev/comments/1fj88q/how_do_i_get_a_list_of_my_user_flair_templates/,"I've been looking through the API and I can't seem to find a function for getting all my flair templates, even though this function must exist. I found get_flairs, but I get an error when I try to call it.  

I also have some other small questions.  
Is there a way to set multiple flair templates with one command, or do I have to set them one by one, with each taking 2 seconds?  
I need to edit the flair text of every user on my reddit but get_flair_list only returns the first 100 users. How do I access the rest? 
Does set_flair_csv take 2 second to update each users flair or is it done all at once?",,False,,t5_2qizd,False,,,True,t3_1fj88q,http://www.reddit.com/r/redditdev/comments/1fj88q/how_do_i_get_a_list_of_my_user_flair_templates/,
1370089007.0,8,self.redditdev,1fglw4,How does Reddit handle votes so efficiently and quickly?,9,1,3,http://www.reddit.com/r/redditdev/comments/1fglw4/how_does_reddit_handle_votes_so_efficiently_and/,"For example, how does it find out which comments I have up/down voted when loading the comments of a post?  Does it fetch all my votes and matches against all loaded comments?  Is this efficient?

Thanks :)",,False,,t5_2qizd,False,,,True,t3_1fglw4,http://www.reddit.com/r/redditdev/comments/1fglw4/how_does_reddit_handle_votes_so_efficiently_and/,
1369650574.0,5,self.redditdev,1f4qho,What happens when chaining subreddits?,10,5,6,http://www.reddit.com/r/redditdev/comments/1f4qho/what_happens_when_chaining_subreddits/,"I am just wondering, if I go to http://www.reddit.com/r/this+that.
Are the submissions just randomly picked from this and that frontpage, is it 1/2, is it a more complex algorythm?
Thanks.",,False,,t5_2qizd,False,,,True,t3_1f4qho,http://www.reddit.com/r/redditdev/comments/1f4qho/what_happens_when_chaining_subreddits/,
1368545942.0,7,self.redditdev,1ebi2b,How do you develop and unit-test your wrappers and bots without tripping the rate-limit?,7,0,4,http://www.reddit.com/r/redditdev/comments/1ebi2b/how_do_you_develop_and_unittest_your_wrappers_and/,"I'm working on some API wrappers for a bot but when I access the fancy calls a lot I keep triggering the rate-limiter. How do you guys handle that? 

Also I want to move to proper development with unit-tests but I imagine running a whole suite at once will trigger this as well, or doesn't it? What's you experience in this?",,False,,t5_2qizd,False,,,True,t3_1ebi2b,http://www.reddit.com/r/redditdev/comments/1ebi2b/how_do_you_develop_and_unittest_your_wrappers_and/,
1366492155.0,7,self.redditdev,1crgln,How can I grab a CAPTCHA image to present to a user?,9,2,4,http://www.reddit.com/r/redditdev/comments/1crgln/how_can_i_grab_a_captcha_image_to_present_to_a/,"I'd like to be able to grab the CAPTCHA image whenever it shows up, present it to the user, and submit the user's response back to reddit.  I can't seem to find any relevant entries in the official docs -- can someone please point me in the right direction?

I went to the PRAW source code for some insight and found [this](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L178) decorator function.  I'm a bit confused as to where `captcha_id` is coming from.  I assume it's an argument from the wrapped function, but which one?  And who sets it?

I guess this question now boils down to ""how do I get a CAPTCHA id""? ",,False,,t5_2qizd,1366494553.0,,,True,t3_1crgln,http://www.reddit.com/r/redditdev/comments/1crgln/how_can_i_grab_a_captcha_image_to_present_to_a/,
1366373226.0,6,self.redditdev,1co8er,How to change the default landing page?,9,3,9,http://www.reddit.com/r/redditdev/comments/1co8er/how_to_change_the_default_landing_page/,"By default on a successful login, user is landed to http://reddit.com. Is it possible if I would like the user to redirect to only new/hot (or other filter) posts?

For example
(http://domain.com) -&gt; (http://domain.com/r/all)",,False,,t5_2qizd,False,,,True,t3_1co8er,http://www.reddit.com/r/redditdev/comments/1co8er/how_to_change_the_default_landing_page/,
1366091517.0,7,self.redditdev,1cg2ek,"API request: ""saved"" property on comments",8,1,0,http://www.reddit.com/r/redditdev/comments/1cg2ek/api_request_saved_property_on_comments/,"I don't know what your policy is on exposing gold features through the API, but if you have nothing against it, it'd be great if comment data had the ""saved"": bool property on them when logged in with an account with Reddit Gold.

I want users to be able to save/unsave comments through my app, but I have no way to tell which are saved and which aren't. I have a hack when I'm currently viewing /saved/ to assume things are saved, but that's far from an ideal solution.",,False,,t5_2qizd,False,,,True,t3_1cg2ek,http://www.reddit.com/r/redditdev/comments/1cg2ek/api_request_saved_property_on_comments/,
1365571089.0,7,self.redditdev,1c1nha,API: can't get thread title of comment reply from /message/inbox?,7,0,9,http://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/,"It looks like when I pull a user's messages, for comment replies I can't get the thread title (without performing an additional request). [Here's](http://pastebin.com/BDr3FdNA) an example of the data I receive.

The website has the title I want, but there's no way to get it from the API? Is this something that could possibly be added? I'd offer to do it myself, but I'll need some babysitting...",,False,,t5_2qizd,False,,,True,t3_1c1nha,http://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/,
1365355195.0,7,self.redditdev,1bv06t,Stuck at processing each submission on a subreddit without repeating,7,0,1,http://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/,"The script is watching and fetching submissions on a subreddit's ""new"" feed every 30 minutes and I'm stuck thinking of a way to only fetch new posts.

I'm using PRAW and thinking of storing the reddit id of the first post (ex. dtg4j), everytime I fetch, in a flat file. (since the first post is always the newest)

But my program fetches 50 posts everytime, what if only 1 of that is new, then the other 49 posts I fetched are useless, right? So I'm not sure yet with that.

And even if there's only a small chance, when I fetch 50 posts, what if there were 60 new submissions. That means I will miss the other 10.

Please help me. Thanks!",,False,,t5_2qizd,False,,,True,t3_1bv06t,http://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/,
1364228338.0,8,self.redditdev,1aza9y,Multiple instances of paster,9,1,2,http://www.reddit.com/r/redditdev/comments/1aza9y/multiple_instances_of_paster/,"Hey,

I noticed that in recent installs of reddit script, the code behaves a bit differently. Now, upon installing, I see multiple instances of paster serve --reload run.ini command, 3-4 of them. Before, it was only one, I usually ran the daemon one.

So, is this maybe causing this issues, I noticed that sometimes karma is not working as it should be, maybe those multiple instances are messing up with the consumers, or this is way it should be, due to some updates to pylons or something like that?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1aza9y,http://www.reddit.com/r/redditdev/comments/1aza9y/multiple_instances_of_paster/,
1364132089.0,7,self.redditdev,1awuok,Top and Controversial broken,9,2,5,http://www.reddit.com/r/redditdev/comments/1awuok/top_and_controversial_broken/,"Hello,

I have installed a Reddit clone and everything is working except the TOP and CONTROVERSIAL tabs in the Front Page and Sub-Reddits. The time periods ""This hour"", ""This today"", ""This week"", ""This month"" are showing no posts. The only time period showing posts seems to be ""All time.""

What's strange is that TOP and CONTROVERSIAL in the All Page seems to be working fine showing all the posts in each time period.

I and others have tried installing this on different servers and this same issue appears. So is this a known bug with the clone or does it require some type of activation for these tabs to be fully functional?

Thank You!",,False,,t5_2qizd,1364139304.0,,,True,t3_1awuok,http://www.reddit.com/r/redditdev/comments/1awuok/top_and_controversial_broken/,
1364049516.0,6,self.redditdev,1auyii,Is it possible to mod reddit to allow embedding images in comments?,9,3,7,http://www.reddit.com/r/redditdev/comments/1auyii/is_it_possible_to_mod_reddit_to_allow_embedding/,"I want to enable my users to embed images and videos in their comments. I'm not trying to create a subreddit, but install the reddit software on my own domain and modify it to suit my needs. How difficult would it be to modify the source code to enable this? Are there any reddit-powered sites where this is possible?",,False,,t5_2qizd,False,,,True,t3_1auyii,http://www.reddit.com/r/redditdev/comments/1auyii/is_it_possible_to_mod_reddit_to_allow_embedding/,
1363957061.0,9,self.redditdev,1asoa5,Configuring AutoModerator conditions. Is there documentation on the subject?,10,1,5,http://www.reddit.com/r/redditdev/comments/1asoa5/configuring_automoderator_conditions_is_there/,"I've been trying to complete the setup of the AutoModerator bot on a machine running reddit. So far, the script runs periodically and is able to access the system and to make the corresponding checks. The problem is I cannot get the right configuration for the database mainly because I've found no info on the subject.

There are three tables that are responsible for determining the AutoModerator's behavior: conditions, subreddits and subreddits_conditions. However, the valid values for each field on those tables don't appear on the setup page and they are not particularly obvious.

Do you have some info on the subject? I'd really appreciate it.

Thanks in advance for your time.",,False,,t5_2qizd,False,,,True,t3_1asoa5,http://www.reddit.com/r/redditdev/comments/1asoa5/configuring_automoderator_conditions_is_there/,
1362575533.0,7,self.redditdev,19rubs,Issues with YouTube bot,7,0,14,http://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/,"A few days ago, I added a YouTube bot, [Groompbot](https://github.com/AndrewNeo/groompbot), to the /r/nerdcubed subreddit, I made it run every minute via cron job, so it would always be the first to post, and it worked great. Then one day it stops posting, I take a look to find that it throws an error 429 (Too many requests) every time, I contacted the maker of it a few days ago, but he hasn't replied yet. Here's the error:

    ERROR:root:Error logging into Reddit.
    Traceback (most recent call last):
      File ""groompbot.py"", line 27, in getReddit
        r.login(settings[""reddit_username""], settings[""reddit_password""])
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py"", line 857, in login
        self.request_json(self.config['login'], data=data)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py"", line 223, in error_checked_function
        return_value = function(cls, *args, **kwargs)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py"", line 396, in request_json
        response = self._request(url, params, data)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py"", line 283, in _request
        timeout=timeout)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py"", line 64, in __call__
        result = self.function(reddit_session, url, *args, **kwargs)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py"", line 167, in __call__
        return self.function(*args, **kwargs)
      File ""/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/helpers.py"", line 161, in _request
        response.raise_for_status()
      File ""/usr/local/lib/python2.6/dist-packages/requests/models.py"", line 638, in raise_for_status
        raise http_error
    HTTPError: 429 Client Error: Too Many Requests
    
Any ideas, Reddit?",,False,,t5_2qizd,False,,,True,t3_19rubs,http://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/,
1362189490.0,8,praw.readthedocs.org,19i0r2,New location for PRAW documentation,8,0,1,http://www.reddit.com/r/redditdev/comments/19i0r2/new_location_for_praw_documentation/,,,False,,t5_2qizd,False,,,False,t3_19i0r2,https://praw.readthedocs.org/,
1361425738.0,8,self.redditdev,18xtpb,AMQP Server Not listening on port 5672,8,0,1,http://www.reddit.com/r/redditdev/comments/18xtpb/amqp_server_not_listening_on_port_5672/,"while running reddit clone in my machine, im getting these errors on the terminal

error connecting to amqp reddit @ localhost:5672 (IOError('Socket closed',))

as ive checked on the open ports using NMAP, apparently, RabbitMQ is not listening on port 5672, or on any other port. Its not listening at all... Ive checked rabbitmqctl for status, and it says rabbitmq-server is running.

Have anyone encountered a problem as such before? Do I really need RabbitMQ to be running?",,False,,t5_2qizd,False,,,True,t3_18xtpb,http://www.reddit.com/r/redditdev/comments/18xtpb/amqp_server_not_listening_on_port_5672/,
1361176344.0,6,self.redditdev,18qq0d,is there a praw library of some sort?,7,1,8,http://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/,"it seems everyone knows how to add certain functions to there programs yet i cant find a standard place to view examples.

for instance ""submission.selftext.lower()""
how or where can i find information on this function?

from what i can tell is that this code snipbit returns a links text. but what could i change?

aka 

submission.titletext()?

submission.linkurl()?


or something of the sort. where would i be able to find information on different functions?

i guess im having a hard time understanding praw because i dont know what i can do with it. 

thanks guys, sorry for newb question.
",,False,,t5_2qizd,False,,,True,t3_18qq0d,http://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/,
1360703448.0,5,self.redditdev,18eeep,Resolved issues porting to PRAW 2,8,3,2,http://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/,"I recently ported moderatorbot to PRAW 2, and had a couple of issues.  bboe was helpful as always, and I thought I'd reproduce here what I did to get my code working, in case others can learn from this.

**Issue 1 - my_moderation**

In PRAW 1.x the way to get a list of subreddits that the logged in user moderates, you use:

    For subreddit in r.user.my_moderation():

In PRAW 2.x the correct way is now:

    For subreddit in r.get_my_moderator():

(and don't forget the () after r.get_my_moderator as I initially did)

**Issue 2 - _request**

In PRAW 1.x, I used the following to make HTTP requests

    response = r._request(url, params)

In PRAW 2.x params has been changed to data, and the following needs to be used.

    response = r._request(url, data=data)

In this particular case I was trying to make an HTTP request to remove a comment.  bboe recommended that I use 

    r.request_json(url, data=data)

&gt; *you should use r.request_json as you can't count on r._request to always be around.*

In this case that will work (even though I am not actually trying to get json data).  In other cases I believe that I still need to use _request.  I believe that some pages do not return a json properly (or the json does not have all the data I need), but the result of _request can be parsed using beatifulsoup.  I don't have any code currently in production that uses beautifulsoup on _request, so I can't remember the exact situations I used it for.

So, I'd like to ask bboe to consider putting some sort of _request functionality into supported ""user space""",,False,,t5_2qizd,False,,,True,t3_18eeep,http://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/,
1358465434.0,9,self.redditdev,16s64a,Praw + 'MORE' Comments,9,0,15,http://www.reddit.com/r/redditdev/comments/16s64a/praw_more_comments/,"I read the 'writing a bot' and 'parsing comments' and what i took away for it was that there is an upper limit of 500 comments unless you have a gold account. Is this correct? Any comments below the 500 are unreadable unless you actually go on the site into the comments and keep pressing the more comment button that only shows about 20  more comments at a time?   
If this isn't correct could someone please give me a simple example of pulling in ALL comments in a thread?",,False,,t5_2qizd,False,,,True,t3_16s64a,http://www.reddit.com/r/redditdev/comments/16s64a/praw_more_comments/,
1357773181.0,5,self.redditdev,16a0p8,Reddit Comments List API acting funny?,7,2,10,http://www.reddit.com/r/redditdev/comments/16a0p8/reddit_comments_list_api_acting_funny/,"http://pastebin.com/m2sy3E02

Don't remember this being an array and the ""{}"" string at the beginning of it seems weird. Is this intentional or is the Reddit API bugged?",,False,,t5_2qizd,False,,,True,t3_16a0p8,http://www.reddit.com/r/redditdev/comments/16a0p8/reddit_comments_list_api_acting_funny/,
1356627185.0,8,github.com,15iwjj,robbit: reddit api and bot framework for clojure,8,0,2,http://www.reddit.com/r/redditdev/comments/15iwjj/robbit_reddit_api_and_bot_framework_for_clojure/,,,False,,t5_2qizd,False,,,False,t3_15iwjj,https://github.com/one-more-minute/robbit,
1356299312.0,6,self.redditdev,15cc7k,A Couple of Praw Questions,7,1,2,http://www.reddit.com/r/redditdev/comments/15cc7k/a_couple_of_praw_questions/,I have been trying to figure this out for hours but im getting nowhere and I cant seem to find any examples with google searches. Can someone give me an example or code snippit of pulling the inbox and iterating though it. Also; what would be the command to write to the side bar in the admin area of a subreddit? ,,False,,t5_2qizd,False,,,True,t3_15cc7k,http://www.reddit.com/r/redditdev/comments/15cc7k/a_couple_of_praw_questions/,
1356021098.0,6,self.redditdev,1568t4,"Where can I find documentation on the ""full name"" for a thing, and how it gets created?",8,2,2,http://www.reddit.com/r/redditdev/comments/1568t4/where_can_i_find_documentation_on_the_full_name/,"It looks like the ""full name"" for a submitted link ""thing"" is its ""kind"" and ""id"" joined with an underscore.  It also looks like all links are kind: ""t3"".  Is this true?  If so, can I always assume that the full name for a link is ""t3_1a2b3"", assuming the ID is ""1a2b3""?

The reason I'm asking is because I want to derive the full name for a submitted link so that I can comment later, and the response I get back from a submitted link with the API is a bunch of Jquery arrays that contain mostly useless info.  The one useful thing it does contain is a url to the comments for the newly created link.  I can scrape the ID out of the comments URL, and if I can just attach ""t3_"" to the id then I'm all set.

By the way, I'm totally willing to hear a better way to get the ""full name"" of a submitted link, but I don't see it as part of the returned JSON at the moment.",,False,,t5_2qizd,False,,,True,t3_1568t4,http://www.reddit.com/r/redditdev/comments/1568t4/where_can_i_find_documentation_on_the_full_name/,
1355961458.0,8,self.redditdev,154ysn,Possible permanent OAuth tokens solution,9,1,4,http://www.reddit.com/r/redditdev/comments/154ysn/possible_permanent_oauth_tokens_solution/,"Hey all. I was thinking, when you perform an oauth authorization you have to define the scopes that you wish to use for that session. Would creating something like an ""extended"" scope solve the permanent token issue?

Currently the tokens are only valid for an hour, but what if you could specify you wanted an ""extended"" scope, which would then make the lifetime of the returned token 6-12 months?

I am not that experienced with scope, but it seems as though this could be an easy solution to the temporary token issue.

Thanks for any feedback!",,False,,t5_2qizd,False,,,True,t3_154ysn,http://www.reddit.com/r/redditdev/comments/154ysn/possible_permanent_oauth_tokens_solution/,
1355096723.0,5,self.redditdev,14kmg6,Is the '?sort=' query broken when applied to user comments? ,7,2,4,http://www.reddit.com/r/redditdev/comments/14kmg6/is_the_sort_query_broken_when_applied_to_user/,"For example here is the URL that gets generated when i try to order my comments by ""top"", and ""all time"".

http://www.reddit.com/user/Stebon24/comments/?sort=top&amp;t=all

The comments seem to remain in the order they where created however.  The same goes for any of the other ""?sort"" or ""?t"" queries when applied to user comments. Am i missing something here, or is this feature not working?",,False,,t5_2qizd,False,,,True,t3_14kmg6,http://www.reddit.com/r/redditdev/comments/14kmg6/is_the_sort_query_broken_when_applied_to_user/,
1354033522.0,8,blog.webfaction.com,13vnhe,Setting up the reddit app without root access on WebFaction,11,3,0,http://www.reddit.com/r/redditdev/comments/13vnhe/setting_up_the_reddit_app_without_root_access_on/,,,False,,t5_2qizd,False,,,False,t3_13vnhe,http://blog.webfaction.com/2012/11/setting-up-the-reddit-app-without-root-access/,
1353536701.0,6,self.redditdev,13l8tg,Is there a way to know if a user is logged in reddit and get his reddit name?,7,1,5,http://www.reddit.com/r/redditdev/comments/13l8tg/is_there_a_way_to_know_if_a_user_is_logged_in/,"I'm thinking of setting up a small website / service for a subreddit, to help organizing contests hosted there. From this site I would like to know if the user is currently logged into reddit, and if he is get his reddit name.


Is there a way to do this automatically (without having to deal with the user's password)?


I imagine that this can't be completely seamless for privacy reasons - you wouldn't want any website to be able to know your reddit name, right? - but something like the stackoverflow loggin via Google Oauth or facebook authorization thing would be great. Is this possible?",,False,,t5_2qizd,False,,,True,t3_13l8tg,http://www.reddit.com/r/redditdev/comments/13l8tg/is_there_a_way_to_know_if_a_user_is_logged_in/,
1352510217.0,7,self.redditdev,12xz7x,Useful moderation log API access,7,0,0,http://www.reddit.com/r/redditdev/comments/12xz7x/useful_moderation_log_api_access/,"At the moment the modlog is available via the api and has its own oauth scope (although it seems to be documented incorrectly), but it's pretty poor compared to the rest of the site. The only useful information contained in the response is the action and sometimes the details, depending on the action.

    {
        ""kind"":""modaction"",
        ""data"":
        {
            ""description"":null,
            ""mod_id36"":""4fer6"",
            ""details"":""confirmed ham"",
            ""action"":""approvelink"",
            ""sr_id36"":""2sz7j"",
            ""target_fullname"":""t3_12jm86""
        }
    },

Is there any intention to expand on this, adding timestamps, actual usernames, perhaps even information about the submission, etc. any time soon?



[Relevant pull request](https://github.com/reddit/reddit/pull/371) ",,False,,t5_2qizd,False,,,True,t3_12xz7x,http://www.reddit.com/r/redditdev/comments/12xz7x/useful_moderation_log_api_access/,
1351025962.0,8,self.redditdev,11yttx,Top bar: how I made it work,8,0,0,http://www.reddit.com/r/redditdev/comments/11yttx/top_bar_how_i_made_it_work/,"I have a bunch of subreddits that I added to the automatic_reddits setting. Everything worked as expected except that the top bar only seemed to list one of them, even after running

    sudo start reddit-job-update_reddits

Eventually I noticed that the one that worked was listed in lower case only in the automatic_reddits setting in run.update and the others had at least one capital letter. I switched them to lower case, then did:

    sudo make ini
    sudo start reddit-job-update_reddits

And then they all appeared. Not just a coincidence; adding a capital letter back to one of the names made it disappear again.

Something is case sensitive that should not be, but this workaround is good enough for me.",,False,,t5_2qizd,False,,,True,t3_11yttx,http://www.reddit.com/r/redditdev/comments/11yttx/top_bar_how_i_made_it_work/,
1350763240.0,7,self.redditdev,11t7l4,Optimizing comments retrieval (will tip helpful responses),8,1,5,http://www.reddit.com/r/redditdev/comments/11t7l4/optimizing_comments_retrieval_will_tip_helpful/,"I am working on a [bitcoin tipping bot](http://www.reddit.com/r/test/comments/11iby2/bitcointip_tip_redditors_with_bitcoin/) and have a couple questions about optimizing it.  Once the bot is working again, I will pay responders if they are helpful.  The more helpful, the larger the tip.

1. What is the best way to get comments from a large group of users?  My bot scans its users' comments for a command.  Right now it goes through the list of users and gets the contents of ""http://www.reddit.com/user/$username/comments.json"" for each user.  With reddit's limit of 2 requests/sec, 5,000 users might take 3 hours to go through even if none of them use the command.  

 Is there a better way?



2.  When someone does a tip like ""+bitcointip $1"", the bot doesn't know who the tip is to, so it has to look at the ""parent_id"" of that comment, then get all the comments in the entire post, and then search the array of comments for the ""author"" of the comment with the same ""name"" as the original comment's ""parent_id"".  This is fine for posts with a small number of comments, but in posts with a large number of comments ""http://www.reddit.com/comments/$link_id.json"" does not return all of the comments.

 Is there a better way to get the ""author"" of a known ""parent_id""?
",,False,,t5_2qizd,False,,,True,t3_11t7l4,http://www.reddit.com/r/redditdev/comments/11t7l4/optimizing_comments_retrieval_will_tip_helpful/,
1350713828.0,6,self.redditdev,11sej5,Scraping entire text of a subreddit?,8,2,10,http://www.reddit.com/r/redditdev/comments/11sej5/scraping_entire_text_of_a_subreddit/,"Are there any pre-existing tools/scripts for scraping an entire subreddit? By which I mean, returning all of the textual information (not retrieving images) for each post, along with the nested comments?",,False,,t5_2qizd,False,,,True,t3_11sej5,http://www.reddit.com/r/redditdev/comments/11sej5/scraping_entire_text_of_a_subreddit/,
1348859087.0,6,self.redditdev,10ms07,Is it possible to vote/ comment on a reddit post in an &lt;iframe&gt;?,8,2,10,http://www.reddit.com/r/redditdev/comments/10ms07/is_it_possible_to_vote_comment_on_a_reddit_post/,"I'm working on a radio/ music player for reddit.  I'm hoping to increase activity in my subreddit by playing the song while showing the original reddit post (and associated comments) in an &lt;iframe&gt;.

The display works fine, as you'd expect, but all comments and votes submitted from within the iframe are ignored.  (Voting appears to work since the UI is all client-side but comments don't get past the ""saving"" dialog.)  I can only assume the AJAX calls aren't being made, or reddit is ignoring them since they're coming from a different domain.

Is there a way around this or is this a limitation set by reddit to prevent just this sort of thing?  Any tips or hints are much appreciated.",,False,,t5_2qizd,False,,,True,t3_10ms07,http://www.reddit.com/r/redditdev/comments/10ms07/is_it_possible_to_vote_comment_on_a_reddit_post/,
1348800492.0,8,self.redditdev,10llwa,Why the discrepancy between comments and more comments type?,9,1,4,http://www.reddit.com/r/redditdev/comments/10llwa/why_the_discrepancy_between_comments_and_more/,"One thing I've noticed is that when I do an API call to get comments, using, the comments api (/comments/id/.json) get returned as a 'tree'. Meaning replies for a given comment are in the 'replies' object.

If I do a request for /api/morechildren, however, the result is a 'flat' array where all the 'more children' are in the first level of the array, and I must look at the parent property to figure out where it goes.

The above leads to a bit of messier code than is needed. Am I overlooking something, and there is a way to get the morecomments similar to how I get them in the initial request (meaning I get back a comment with replies in the replies property, etc)? Or is the above expected?
",,False,,t5_2qizd,False,,,True,t3_10llwa,http://www.reddit.com/r/redditdev/comments/10llwa/why_the_discrepancy_between_comments_and_more/,
1348179184.0,7,self.redditdev,107sl5,"[api notice] Use ssl.reddit.com for /api/v1/authorize and /api/v1/access_token, oauth.reddit.com for requests with bearer tokens only",7,0,0,http://www.reddit.com/r/redditdev/comments/107sl5/api_notice_use_sslredditcom_for_apiv1authorize/,"Our initial OAuth 2 examples used https://oauth.reddit.com for everything. We originally intended this domain to support the authorize and access_token API calls, in addition to all OAuth 2 calls (i.e., calls with bearer tokens). However, this doesn't make for a good user interface and complicates our haproxy configs.

From this point on, use https://oauth.reddit.com only for API calls with bearer tokens. Any other request will be rejected with a 403 error. Use https://ssl.reddit.com for the authorization URL to send users to, and for the access_token API call you make to obtain the bearer token.
",,False,,t5_2qizd,False,,,True,t3_107sl5,http://www.reddit.com/r/redditdev/comments/107sl5/api_notice_use_sslredditcom_for_apiv1authorize/,
1347897685.0,6,self.redditdev,100zp1,"Since the update on Sept 11, my bot to update the sidebar is getting a 'conflict error' on the description field when using /api/site_admin ",7,1,11,http://www.reddit.com/r/redditdev/comments/100zp1/since_the_update_on_sept_11_my_bot_to_update_the/,"[Here is the API I'm using.](http://www.reddit.com/dev/api#POST_api_site_admin)

And here is my code (Ruby)

    # Update the subreddit description
    uri = URI('http://www.reddit.com/api/site_admin')
    req = Net::HTTP::Post.new(uri.path)
    req['cookie'] = ""reddit_session= "" + session
    req.set_form_data('api_type' =&gt; 'json',            
                'allow_top' =&gt; config.get_value('allow_top'),
                'css_on_cname' =&gt; config.get_value('css_on_cname'),
                'description' =&gt; description_string,                  
                'header_title' =&gt; config.get_value('header_title'),             
                'lang' =&gt; config.get_value('language'),
                'link_type' =&gt; config.get_value('link_type'),          
                'name' =&gt; config.get_value('subbreddit_name'),
                'over_18' =&gt; config.get_value('over_18'),
                'show_cname_sidebar' =&gt; config.get_value('show_cname_sidebar'),
                'show_media' =&gt; config.get_value('show_media'),
                'sr' =&gt; config.get_value('subreddit_thing'),
                'title' =&gt; config.get_value('page_title'),
                'type' =&gt; config.get_value('subreddit_privacy'),
                'domain' =&gt; config.get_value('domain'),
                'wikimode' =&gt; config.get_value('wikimode'),
                'uh' =&gt; modhash)

    res = Net::HTTP.start(uri.hostname, uri.port) do |http|
        http.request(req)
    end


It was working perfectly fine before until the wiki features were added. I updated the wikimode field, and now I'm getting the following error:

    [[""CONFLICT"", ""conflict error while saving"", ""description""]]

I've browsed through the Reddit source for a while, couldn't find anything that explained what a conflict error was, or how to resolve it. My thoughts are that maybe there are some banned characters, or I'm not using valid markup, but I haven't figured it out yet.

The description string is usually something along these lines:


    [Please read our posting guidelines.](http://www.reddit.com/r/CalgarySocialClub/comments/t3tj6/posting_guidelines/)

    Sun|Mon|Tue|Wed|Thu|Fri|Sat
    :-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:
    ~~[26](http://goo.gl/sXbff)[^1](http://goo.gl/B7H4p)~~|~~[27](http://goo.gl/jf4DS)~~|~~[28](http://goo.gl/mt2Dc)[^1](http://goo.gl/5Wh8j)~~|~~29~~|~~30~~|~~31~~|~~1~~
    ~~[2](http://goo.gl/Sjws8)~~|~~3~~|~~[4](http://goo.gl/qv1on)[^1](http://goo.gl/BptSm)[^2](http://goo.gl/MgMT8)~~|~~5~~|~~6~~|~~7~~|~~[8](http://goo.gl/Fcdhk)~~
    ~~[9](http://goo.gl/DS1aW)~~|~~10~~|~~[11](http://goo.gl/983sn)~~|~~12~~|~~13~~|~~14~~|~~15~~
    ~~[16](http://goo.gl/0tp2p)~~|**17**|18|19|20|21|22
    23|24|25|26|27|28|29
    30|1|2|3|4|5|6

    ## Upcoming Events

    * [Oct 13 - 2012 Calgary Zombie Walk @ Olympic Plaza/City Hall @  1:00 PM](http://goo.gl/Plqxh)

    [Twitter](http://twitter.com/YYCSocialClub)
    [Google Plus](https://plus.google.com/117956267917340211251)


From that, the markdown looks valid and renders properly, so I don't know where to go from here.",,False,,t5_2qizd,False,,,True,t3_100zp1,http://www.reddit.com/r/redditdev/comments/100zp1/since_the_update_on_sept_11_my_bot_to_update_the/,
1347779870.0,7,self.redditdev,zyp17,List-O-Subreddits,7,0,4,http://www.reddit.com/r/redditdev/comments/zyp17/listosubreddits/,"Is there a large list of all 8000+ subreddits? We need to take a random weighted sample of subreddits, and it would help if we had all the subreddits and their numbers of subscribers.

I checked online, and the best I could find was a ""Wall of Subreddits"" which only had around 3000.",,False,,t5_2qizd,False,,,True,t3_zyp17,http://www.reddit.com/r/redditdev/comments/zyp17/listosubreddits/,
1347312131.0,8,self.redditdev,zo8rt,"A style is only being applied to Firefox users, and to fix it I need to know if it's intended to be displayed for everyone or removed.",8,0,2,http://www.reddit.com/r/redditdev/comments/zo8rt/a_style_is_only_being_applied_to_firefox_users/,"On Firefox, the submit button has a border-radius on all left corners. The style only exists using the -moz- prefix, so it's only visible on that browser engine.

I'm guessing browser-specific code wasn't intended, and I have 2 possible fixes ready to submit as pull requests, but I need to know if that style needs to be removed or if it should be applied to all browsers.

",,False,,t5_2qizd,False,,,True,t3_zo8rt,http://www.reddit.com/r/redditdev/comments/zo8rt/a_style_is_only_being_applied_to_firefox_users/,
1346613114.0,7,self.redditdev,z8pdp,A slightly different presentation of front pages,7,0,2,http://www.reddit.com/r/redditdev/comments/z8pdp/a_slightly_different_presentation_of_front_pages/,"[http://stuartfeldt.com/r/vis](http://stuartfeldt.com/r/vis) has been a weekend project for me...  Kind of a rip-off of what digg did ~5 years ago with their labs, and is more of a POC than intended for full scale use.  I would love to see something like 'Reddit Labs', where we try and develop crazy new ways of consuming reddit with [D3](http://d3js.org/),  [Google Charts](https://developers.google.com/chart/), etc...",,False,,t5_2qizd,False,,,True,t3_z8pdp,http://www.reddit.com/r/redditdev/comments/z8pdp/a_slightly_different_presentation_of_front_pages/,
1341539458.0,8,self.redditdev,w3vmb,Any tips for a new r/redditdev-er trying to understand the codebase?,10,2,2,http://www.reddit.com/r/redditdev/comments/w3vmb/any_tips_for_a_new_rredditdever_trying_to/,"I'm trying to get started with open source development and I figured, since I use reddit every day, it would probably be a good project to start with.

[Issue #457](https://github.com/reddit/reddit/issues/457) seems like it should be pretty easy to knock out, but I can't find the relevant source code. What tips do you have for learning the codebase and tracking down relevant code?",,False,,t5_2qizd,False,,,True,t3_w3vmb,http://www.reddit.com/r/redditdev/comments/w3vmb/any_tips_for_a_new_rredditdever_trying_to/,
1341432502.0,6,self.redditdev,w1nef,links de-duplication,8,2,4,http://www.reddit.com/r/redditdev/comments/w1nef/links_deduplication/,"Is it possible to teach reddit about some sites' URI schemes, so it knows which parts of the submitted link are not important and looks for duplicates using only the important parts?

The motivating example would be youtube. Links to youtube often contain extra stuff in the query string that changes nothing but causes reddit to miss a repost. If reddit knows that only the `v=.{11}` part and the `#t=\d+s` part are important, de-duplication (and automatic links to same video on different submissions) would be much more useful.",,False,,t5_2qizd,False,,,True,t3_w1nef,http://www.reddit.com/r/redditdev/comments/w1nef/links_deduplication/,
1340794257.0,6,self.redditdev,voh2x,CSS media queries ,7,1,8,http://www.reddit.com/r/redditdev/comments/voh2x/css_media_queries/,"I am looking to make my reddit mobile friendly while still having a cool header while browsing on a desktop. I tried to do this with CSS media query, but the CSS editor didn't recognize it. 

Did I make a typo or is this not supported in reddit? 

Here is my code: 

@media screen 
and (max-width: 480px) { 
div#header-img.default-header {
text-indent: -9999px;
background-image: url(sprite-reddit.rQ7y8qN-wzQ.png);
background-position: -0px -258px;
background-repeat: no-repeat;
height: 40px;
width: 120px;
display: inline-block;
vertical-align: bottom;
margin-bottom: 3px;
}
}

@media screen 
and (max-width: 1152px) { 
#header-img.default-header{
background-image:URL('xxxxxxxxxxxxxxxxxxxxx');
display:block;
margin-left:auto;
margin-right:auto;
}
}

@media screen 
and (min-width: 1280px) { 
#header-img.default-header{
background-image:URL('xxxxxxxxxxxxxxxxxxxxx');
display:block;
margin-left:auto;
margin-right:auto;
}
}",,False,,t5_2qizd,False,,,True,t3_voh2x,http://www.reddit.com/r/redditdev/comments/voh2x/css_media_queries/,
1336735829.0,7,self.redditdev,ti0wj,Imgur API call for subreddit galleries?,9,2,3,http://www.reddit.com/r/redditdev/comments/ti0wj/imgur_api_call_for_subreddit_galleries/,"I was wondering if there was a way to call subreddit galleries on Imgur ( http://imgur.com/r/spaceporn ), and how it would be called. I can't seem to find it.",,False,,t5_2qizd,False,,,True,t3_ti0wj,http://www.reddit.com/r/redditdev/comments/ti0wj/imgur_api_call_for_subreddit_galleries/,
1336419660.0,8,self.redditdev,tbmpn,Is there a way to find posts from a specific date?,8,0,4,http://www.reddit.com/r/redditdev/comments/tbmpn/is_there_a_way_to_find_posts_from_a_specific_date/,"Is there a way to find posts from a specific date?

I couldn't find this in the API.


*EDIT:* I was thinking about finding a post of the specific day and then continue by searching most of the rest of the posts through http://www.reddit.com/new/?sort=new&amp;after=id_of_specific_post, and updating the ""id_of_specific_post"" by the last post found.
Example:

[http://www.reddit.com/new/?sort=new&amp;after=t3_t9ks2](http://www.reddit.com/new/?sort=new&amp;after=t3_t9ks2)

[http://www.reddit.com/new/?sort=new&amp;after=t3_t9j9a](http://www.reddit.com/new/?sort=new&amp;after=t3_t9j9a)

[http://www.reddit.com/new/?sort=new&amp;after=t3_t9hxl](http://www.reddit.com/new/?sort=new&amp;after=t3_t9hxl)

etc...


But this looks a bit stupid if there is an easier way.",,False,,t5_2qizd,True,,,True,t3_tbmpn,http://www.reddit.com/r/redditdev/comments/tbmpn/is_there_a_way_to_find_posts_from_a_specific_date/,
1336230222.0,5,self.redditdev,t8dt2,"Admin delete users, reset passwords, and delete subreddits?",12,7,1,http://www.reddit.com/r/redditdev/comments/t8dt2/admin_delete_users_reset_passwords_and_delete/,"
I have my own reddit installation up and running with an admin configured (the ""turn admin on/off"" shows up).  Unfortunately I can't figure out how to use it to delete users, reset passwords, or delete subreddits.

Is the only way to ""delete"" a subreddit to ""ban"" it?

When I'm on a user's page (ie /user/username) I see an ""admin"" dropdown tab, but it isn't populated with anything.  Did one of my modifications break this, or is there just nothing there?  Is there a way to reset user passwords or delete users?",,False,,t5_2qizd,False,,,True,t3_t8dt2,http://www.reddit.com/r/redditdev/comments/t8dt2/admin_delete_users_reset_passwords_and_delete/,
1335749540.0,8,self.redditdev,sz0qb,Any other libraries to pull in thumbnail for a url?,10,2,3,http://www.reddit.com/r/redditdev/comments/sz0qb/any_other_libraries_to_pull_in_thumbnail_for_a_url/,Does anyone know of any libraries that can pull in the thumbnail for a url? Similar to how reddit does it?,,False,,t5_2qizd,False,,,True,t3_sz0qb,http://www.reddit.com/r/redditdev/comments/sz0qb/any_other_libraries_to_pull_in_thumbnail_for_a_url/,
1326996099.0,8,self.redditdev,onjvh,I always get a 500 error when trying to get data from a certain user.,9,1,4,http://www.reddit.com/r/redditdev/comments/onjvh/i_always_get_a_500_error_when_trying_to_get_data/,"Hi!

I'm trying to code a script that download data from reddit. It works quite well, however I *always* get a ""500 error"" when I try to get data from user ""AirRaven"", and I really don't know why.

This error is also reproducible on the website: Go to http://www.reddit.com/user/AirRaven/liked/?count=400&amp;limit=100&amp;after=t3_kz9zj , scroll down then click next. I always get a ""You broke reddit"" page this way.

I really don't know why. Any body else has the same issue? Does any one know a fix? 
(Note that that I handle errors, etc. so that does not crash my script or anything, it's just annoying)",,False,,t5_2qizd,False,,,True,t3_onjvh,http://www.reddit.com/r/redditdev/comments/onjvh/i_always_get_a_500_error_when_trying_to_get_data/,
1325827776.0,7,self.redditdev,o523l,Anyone want to help me figure out why my php script to change user flair isn't working?,9,2,6,http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/,"Code is [here](http://pastebin.com/PYa2vnFj). I'm following the API to a T, but when I make the second call to post_something curl gets a 403. Any help would be much appreciated, as this project is done once I get rid of the damn 403. Thanks!",,False,,t5_2qizd,False,,,True,t3_o523l,http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/,
1325802630.0,7,self.redditdev,o4jq0,Is it possible to get a complete comment when loading more comments via json?,11,4,4,http://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/,"I'm using api/morechildren to load more comments, but it will not give me the same comment attributes as the comments I already have.

Example:

When loading the comments page with GET using urls similar to http://www.reddit.com/r/pics/comments/linkid/some_link_title/.json
I get comments with the below attributes.

    ""data"": {
        ""author"": ""authorname"", 
        ""author_flair_css_class"": null, 
        ""author_flair_text"": null, 
        ""body"": ""Comment text here"", 
        ""body_html"": ""html version of comment text here"", 
        ""created"": 1325824590.0, 
        ""created_utc"": 1325799390.0, 
        ""downs"": 0, 
        ""id"": ""idstring"", 
        ""levenshtein"": null, 
        ""likes"": null, 
        ""link_id"": ""t3_linkid"", 
        ""name"": ""t1_idstring"", 
        ""parent_id"": ""t1_parentid"", 
        ""replies"": """", 
        ""subreddit"": ""pics"", 
        ""subreddit_id"": ""t5_2qh0u"", 
        ""ups"": 52
    }, 
    ""kind"": ""t1""

But when I load more comments using by POST to *api/morechildren* with the parameters *children, link_id, r and api_type* with *api_type* set to json I only get about half of the comment attributes

    ""data"": {
        ""content"": ""alot lot lot of strange content text"", 
        ""contentHTML"": ""html formatted contentText"", 
        ""contentText"": ""the actual comment"", 
        ""id"": ""t1_idstring"", 
        ""link"": ""t3_linkid"", 
        ""parent"": ""t1_parentid"", 
        ""replies"": """"
    }, 
    ""kind"": ""t1""

**TL;DR** How can I get all the comment data (ups, downs, likes, created...) when loading more comments via json?",,False,,t5_2qizd,False,,,True,t3_o4jq0,http://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/,
1325771002.0,8,self.redditdev,o3xiu,I'm noticing long periods where the API returns the same results every time. Is this due to caching?,9,1,5,http://www.reddit.com/r/redditdev/comments/o3xiu/im_noticing_long_periods_where_the_api_returns/,"I'm building a websocket stream of new posts at the moment (http://jsfiddle.net/BFzeM/) for a few ideas I have.
To do this I am grabbing new posts from /r/all/new/.json?sort=new&amp;before=t3_? every 7 seconds on the server side, but every now and then it will hit a good 2 mins of not finding any new posts then suddenly they all start coming through and has to catch up.

Is it reddits caching that causes this? Or is there something I can adjust that will get the latest data?",,False,,t5_2qizd,False,,,True,t3_o3xiu,http://www.reddit.com/r/redditdev/comments/o3xiu/im_noticing_long_periods_where_the_api_returns/,
1325370411.0,7,self.redditdev,nxyne,Reddit installation script,13,6,11,http://www.reddit.com/r/redditdev/comments/nxyne/reddit_installation_script/,"Is there anything resembling a working script for installing Reddit on any linux distribution? All the guides are out of date and the script located at;

https://raw.github.com/gist/922144/install-reddit.sh

is as well.",,False,,t5_2qizd,False,,,True,t3_nxyne,http://www.reddit.com/r/redditdev/comments/nxyne/reddit_installation_script/,
1325222986.0,7,self.redditdev,nvwp3,Help setting up,9,2,8,http://www.reddit.com/r/redditdev/comments/nvwp3/help_setting_up/,"I am following the directions on github. I just cloned the git repository and I am now trying to install the python module dependencies.

when I enter:
 make pyx

I get an error:

Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
ImportError: No module named i18n
Traceback (most recent call last):
  File ""r2/lib/js.py"", line 8, in &lt;module&gt;
    from r2.lib.translation import iter_langs
ImportError: No module named r2.lib.translation
Traceback (most recent call last):
  File ""r2/lib/js.py"", line 8, in &lt;module&gt;
    from r2.lib.translation import iter_langs
ImportError: No module named r2.lib.translation
python setup.py build_ext --inplace
Traceback (most recent call last):
  File ""setup.py"", line 30, in &lt;module&gt;
    from Cython.Distutils import build_ext
ImportError: No module named Cython.Distutils
make: *** [build/pyx-buildstamp] Error 1

I have tried uninstalling and reinstalling cython but it hasn't helped.

What do I do? Thanks!",,False,,t5_2qizd,False,,,True,t3_nvwp3,http://www.reddit.com/r/redditdev/comments/nvwp3/help_setting_up/,
1320544340.0,8,self.redditdev,m1z5r,Is there any way to get the thumbnail URL for a normally hidden thumbnail?,9,1,3,http://www.reddit.com/r/redditdev/comments/m1z5r/is_there_any_way_to_get_the_thumbnail_url_for_a/,"Before the URLs changed recently the thumbnail url was based on the post's thing ID (data-fullname), but now it looks like it's just a random string. Is there any way I can query the thumbnail URL even if the logged in user's preferences would normally hide it?",,False,,t5_2qizd,False,,,True,t3_m1z5r,http://www.reddit.com/r/redditdev/comments/m1z5r/is_there_any_way_to_get_the_thumbnail_url_for_a/,
1318379934.0,7,self.redditdev,l8ydv,Editing a subreddit's description (in the sidebar) via the API,8,1,2,http://www.reddit.com/r/redditdev/comments/l8ydv/editing_a_subreddits_description_in_the_sidebar/,"I know the API has a ""site_admin"" method that lets you to edit subreddits, but I'm not sure how the appropriate POST request should be formed. Anyone have any ideas or could you at least point me in the right direction?

None of the Python reddit API wrappers seem to have this API method implemented :/",,False,,t5_2qizd,False,,,True,t3_l8ydv,http://www.reddit.com/r/redditdev/comments/l8ydv/editing_a_subreddits_description_in_the_sidebar/,
1317750028.0,9,self.redditdev,l0pq6,Questions on the current reddit server capacity,11,2,5,http://www.reddit.com/r/redditdev/comments/l0pq6/questions_on_the_current_reddit_server_capacity/,"Hello - 

I have the reddit codebase running for a project of mine - and have also split up the separate components into different machines/roles, for future growth.  I am also using Amazon EC2, the same as reddit.  I know that it's easy to handle low traffic, but things get tough when it starts ramping up.

So can you give some insight into how your servers are split up, and how many there are?  Quick question, how many app servers (uwsgi+nginx) are you using and what Amazon machine size?  And then, how many memcached machines and what size?  And a similar question for cassandra?  How many machines do you have for rabbitmq processing - one for each queue?

Thanks - this will just give me some insight as to what I should be aiming towards.",,False,,t5_2qizd,False,,,True,t3_l0pq6,http://www.reddit.com/r/redditdev/comments/l0pq6/questions_on_the_current_reddit_server_capacity/,
1317627274.0,9,self.redditdev,kz5vd,Getting Traffic working?,10,1,2,http://www.reddit.com/r/redditdev/comments/kz5vd/getting_traffic_working/,"So I really like the reddit software, and after a couple of tweaks to finally get everything from search to embed.ly, I got it to work. But I have one question.

How would one get the traffic functionality to work? Is it just proprietary code that we can't have and we have to make a replacement?",,False,,t5_2qizd,False,,,True,t3_kz5vd,http://www.reddit.com/r/redditdev/comments/kz5vd/getting_traffic_working/,
1315759086.0,7,self.redditdev,kc5bl,How to logout of reddit using the API?,7,0,5,http://www.reddit.com/r/redditdev/comments/kc5bl/how_to_logout_of_reddit_using_the_api/,"(Background: developing on webOS)

How do I log out of reddit? A call to /api/login sets a cookie from the server via the Set-Cookie header, and that's great. However, as my app is in a different domain, I can't seemingly tamper with the cookies for another site. So, I want to get reddit to issue a ""Set-Cookie"" command to wipe my reddit_session cookie for me. Is there a simple API call that can do this?",,False,,t5_2qizd,False,,,True,t3_kc5bl,http://www.reddit.com/r/redditdev/comments/kc5bl/how_to_logout_of_reddit_using_the_api/,
1315292695.0,7,self.redditdev,k66pj,CSS Request - classes applied to &lt;body&gt; tag based on the page,10,3,13,http://www.reddit.com/r/redditdev/comments/k66pj/css_request_classes_applied_to_body_tag_based_on/,"When doing more advanced-ish subreddit styling, I've often had to resort to crude hacks to get the stylesheet to work properly on all pages inside a subreddit.

It would be *really* helpful if there was a class on the body tag depending on which page was being displayed.

For example, on r/somesubreddit you'd have something like:

    &lt;body class=""links_page whats_hot""&gt;

On r/somesubreddit/new, you'd have:

    &lt;body class=""links_page new""&gt;

And on r/somesubreddit/submit, you'd have:

    &lt;body class=""submit_page""&gt;

Thanks!",,False,,t5_2qizd,False,,,True,t3_k66pj,http://www.reddit.com/r/redditdev/comments/k66pj/css_request_classes_applied_to_body_tag_based_on/,
1303368054.0,7,self.redditdev,gv4ok,"JavaScript to load more comments when you get to 
the bottom of the page, like Twitter.",8,1,3,http://www.reddit.com/r/redditdev/comments/gv4ok/javascript_to_load_more_comments_when_you_get_to/,"This call will load more comments at the bottom of the page:

    $('.morecomments a:last').click();

Which you can run from the address bar like so:

    javascript:(function(){$('.morecomments a:last').click();})();

But using jQuery you can also set up an event watcher to do this as you scroll:

    javascript:(function(){$(document).ready(function () { $(window).scroll(function () { if ($(window).scrollTop() == $(document).height() - $(window).height()) { $('.morecomments a:last').click(); } }); });})();

I also turned this into a [user javascript](http://dl.dropbox.com/u/16366/reddit_more_comments.user.js) (tested and working on Opera) - more comments loaded automatically! :)",,False,,t5_2qizd,True,,,True,t3_gv4ok,http://www.reddit.com/r/redditdev/comments/gv4ok/javascript_to_load_more_comments_when_you_get_to/,
1302714176.0,6,self.redditdev,gp6su,api/read_message doesn't affect orangered envelope on reddit.com?,7,1,3,http://www.reddit.com/r/redditdev/comments/gp6su/apiread_message_doesnt_affect_orangered_envelope/,"I can use api/read_message to mark messages as read. However, even if I mark every unread message in the user's inbox as read, their envelope is still orangered on reddit.com. If they click the orangered envelope, it brings them to a page that says ""there doesn't seem to be anything here"".

Anything I can do besides sending a request to the unread page?",,False,,t5_2qizd,False,,,True,t3_gp6su,http://www.reddit.com/r/redditdev/comments/gp6su/apiread_message_doesnt_affect_orangered_envelope/,
1301400770.0,8,self.redditdev,gdvi0,"I'm trying to get the subreddits to display on an installation of reddit at work. However, nothing happens. Can someone explain to me the update_reddits.sh script to me?",9,1,6,http://www.reddit.com/r/redditdev/comments/gdvi0/im_trying_to_get_the_subreddits_to_display_on_an/,"Long story short, We've got everything up and running, however trying to run the above script we find that we don't have either this production.ini file or this plugin: --plugin=r2

I should mention that the subreddits aren't displaying in the top bar of reddit. 

Here's the script:

#!/bin/bash
	
	cd ~/reddit/r2
	/usr/local/bin/saferun /tmp/updatereddits.pid nice /usr/local/bin/paster --plugin=r2 run production.ini r2/lib/sr_pops.py -c ""run()""",,False,,t5_2qizd,True,,,True,t3_gdvi0,http://www.reddit.com/r/redditdev/comments/gdvi0/im_trying_to_get_the_subreddits_to_display_on_an/,
1297053148.0,6,self.redditdev,fgnef,Socialite extension for chrome?,8,2,4,http://www.reddit.com/r/redditdev/comments/fgnef/socialite_extension_for_chrome/,"I'm getting sick of the reddit toolbar and the increasing number of websites that refuse to work while framed.

I would really like a version of the firefox socialite extension for chrome.

As far as I can see, [Marvin](http://marvin.cat-v.org/) has been abandoned for over a year, and [chromakode's somthing](http://www.reddit.com/r/Marvin/comments/c2y8a/any_news/c0pv8wb) hasn't been touched in 9 months.

Is anyone working on anything else, or should I just attempt to make my own version?

Edit: And annoyingly, this got suck in the spam filter, at a time when all the admins are asleep.",,False,,t5_2qizd,True,,,True,t3_fgnef,http://www.reddit.com/r/redditdev/comments/fgnef/socialite_extension_for_chrome/,
1295307188.0,6,self.redditdev,f435o,Setting Up a New Development VM (follow-up to Updating the VM),7,1,3,http://www.reddit.com/r/redditdev/comments/f435o/setting_up_a_new_development_vm_followup_to/,"**TL;DR**  Setup new Reddit dev VM. Summary of steps: Used VMWare to create Ubuntu 10.04.1 VM, installed g++, and *BeautifulSoup-3.0.8.1*. Modified and ran [install-reddit.sh](https://gist.github.com/738525) script. Updated *run.ini*. 

-----------------------------------------------------------

This is a follow up to my [post submitted this past weekend](http://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/) on trying to setup and update the published Reddit development VM. Basically, that was a flop as the published VM is out of date and updating it was simply getting too painful. Instead, I focused on setting up a new VM image for myself based on the latest Ubuntu ISO (10.04.1). Note: this VM was setup using VMWare with the same default user (reddit) and password (password) as the publicly distributed VM.

What follows is (relatively) detailed but streamlined description of the steps taken to get the environment up to a nominal running state. There were many false steps and unnecessary side-tracks taken to get this to work. I'm fairly sure that I've captured the minimum steps necessary, but of course, let me know if anything is missing or if you would like more information about some of the missteps/issues.

1) Logged in as reddit initially then set password for root (I'm lazy and hate having to always type sudo in front of everything and then having to type the password repeatedly). Remaining steps were performed as root.

2) Installed SSH server and then logged in as root from putty instead (prefer accessing VM using putty rather than the VMware console).
	apt-get install openssh-server

3) Installed g++
    apt-get install g++

4) Downloaded BeautifulSoup  3.0.8.1 from [http://www.crummy.com/software/BeautifulSoup/download/](http://www.crummy.com/software/BeautifulSoup/download/). Then install was performed using the following commands:
    cd /home/reddit
    wget http://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.0.8.1.tar.gz
    tar -xzf BeautifulSoup.tar.gz
    cd BeautifulSoup-3.0.8.1/
    python setup.py install

5) Edited the */etc/hosts* file to add the following line:
    127.0.0.1       reddit.local reddit pay.localhost cslowe.local
	
6) Downloaded [install-reddit.sh](https://gist.github.com/738525) to */home/reddit* and modified it to retrieve the 0.6.9 version of cassandra
    cd /home/reddit
    mkdir tmp
    cd tmp
    wget https://gist.github.com/gists/738525/download
    tar -xzf download
    cd gist738525-26f2a656ecf1417376210e2d172a918e53491d97
    sed -e ""s/unstable/06x/g"" install-reddit.sh &gt; /home/reddit/install-reddit.sh
    cd /home/reddit
    rm -rfd tmp
    chmod 755 install-reddit.sh
    ./install-reddit.sh

7) Go get some coffee, go to lunch, do some real work, surf Reddit, etc.

8) Edited the *run.ini* file to change the *use_query_cache* setting to *True* (Reddit app fails to start otherwise) and other settings to get it start and run nominally.
    cd /home/reddit/reddit/r2
    rm run.ini
    sed -e ""s/use_query_cache = False/use_query_cache = True/g"" -e ""s/domain = localhost/domain = reddit.local/g"" -e ""s/media_domain = localhost/media_domain = reddit.local/g"" -e ""s/\*:hc/\*:hc:hc/g"" example.ini &gt; development.ini
    vi development.ini
(Add the following line after after [DEFAULT])
    INDEXTANK_API_URL =
(save file and exit vi)
    ln -s development.ini run.ini

9) Reset ownership of all files to reddit under the */home/reddit* home folder
    cd /home/reddit
    chown -R reddit:reddit *

10) Secured the root user (removed password) and rebooted the VM.

At this point the VM is running and is externally accessible. There are no sub-reddits defined, so I'm still working on how to post content, but I was able to register and log in, so that much is working. I'm currently researching how to load some test data to get going. If anyone has any insight on this it would be greatly appreciated.

Special thanks to [spladug](http://www.reddit.com/user/spladug) for his assistance and pointing out the [install-reddit.sh](https://gist.github.com/738525) script.

Lastly, I can't say I appreciate the Reddit servers being started by way of svscan in a development environment. Since I had many issues getting it to work, having it automatically started (and restarted) became a pain since the only way I found of killing it was to reboot or switch to single-user mode. I've therefor removed the *reddit-app01* and *reddit-app02* links from the */etc/service* folder. Perhaps once things are more stable these links will be restored.

EDIT: formatting

**EDIT:** Not sure how I missed this originally, but I noticed yesterday that the Cassandra service wasn't starting automatically. I found it necessary to modify the ""*/home/reddit/reddit/srv/cassandra/run*"" script to get it to work. In my environment the executable for Cassandra 0.6.9 was found in the ""*/usr/sbin/*"" where the *run* script was trying to locate it in the ""*/usr/local/cassandra/bin*"" folder which doesn't exist. A simple change to the script results in Cassandra starting up normally (commented out the ""cd"" line and removed the ""*bin/*"" prefix from *exec* line).",,False,,t5_2qizd,True,,,True,t3_f435o,http://www.reddit.com/r/redditdev/comments/f435o/setting_up_a_new_development_vm_followup_to/,
1289766546.0,6,self.redditdev,e61hf,"Hey redditdev, anyone interested in helping /r/anarchism with a project?",14,8,8,http://www.reddit.com/r/redditdev/comments/e61hf/hey_redditdev_anyone_interested_in_helping/,"We've got this bot, anarchystatsbot, that mirrors the mod chat on another page so everyone can see it.  The person who developed it has been inactive for over a year, last night the bot started banning people.  Someone has access to it's password, so we need to build another one to secure it.

Also, we're interested in mirroring the deleted comments so users can see what mod deleted what comments.  And we would also like to mirror an entire sub, /r/metanarchism, if that's possible.  The purpose of mirroring it is so that we can keep it private but also allow unapproved submitters to see the discussions.",,False,,t5_2qizd,False,,,True,t3_e61hf,http://www.reddit.com/r/redditdev/comments/e61hf/hey_redditdev_anyone_interested_in_helping/,
1285362233.0,8,self.redditdev,dii4s,"Wanting to use a portion of reddit's code.... questions within (license, getting started, etc).  :)",10,2,10,http://www.reddit.com/r/redditdev/comments/dii4s/wanting_to_use_a_portion_of_reddits_code/,"So, without disclosing the full details of my project, a major aspect of the site will have users voting up/down different nodes (not too much else of what Reddit does though). Is it ok (with the license) to just take that portion  (just the sorting code)?

Also, if my project takes off, is it ok to profit off of the code I use/modify?  

Any other info I should use?  And advice on how to start on this?  :)
",,False,,t5_2qizd,False,,,True,t3_dii4s,http://www.reddit.com/r/redditdev/comments/dii4s/wanting_to_use_a_portion_of_reddits_code/,
1284508191.0,8,self.redditdev,ddxi2,A good way to get stories / comments from the far past?,9,1,8,http://www.reddit.com/r/redditdev/comments/ddxi2/a_good_way_to_get_stories_comments_from_the_far/,"I'm trying to statistically compare in various ways current comments with much older comments, to try to confirm or deny the notion that reddit comments have been degrading.

Gathering comments from the current reddit frontpage is easy enough, but what is the best way to get historical data? Right now my only ideas are to go to the subreddits all time top scoring links, which will include some old links, [reredd.com](http://reredd.com/date/2007/12/23), and [archive.org](http://web.archive.org/*/http://www.reddit.com/). Those two sites are very poor options though because I can't really get the content directly. The first options might not provide enough data.

Is there any better way to get historic comment data? It would probably be too much to ask for some kind of comment dump, but I'm not sure how to get enough data to be accurate.",,False,,t5_2qizd,False,,,True,t3_ddxi2,http://www.reddit.com/r/redditdev/comments/ddxi2/a_good_way_to_get_stories_comments_from_the_far/,
1271695593.0,8,self.redditdev,bt606,Trying (and failing) to get Reddit installed,10,2,27,http://www.reddit.com/r/redditdev/comments/bt606/trying_and_failing_to_get_reddit_installed/,"Let's preface this with ""I haven't had to upgrade or install linux in 5 years because of my cushy job"".

I'm striving for reproducibility, so I'm using VMWare Player as my platform.  I'm starting with [this image](http://www.visoracle.com/vm/ubuntu810/); it's a full Ubuntu 8.10 install, but with Python 2.5 installed.  I installed [these python 2.6 debs](http://jaredforsyth.com/blog/2009/jan/20/install-python-26-ubuntu/).  However, this does not upgrade python 2.5, but instead installs a parallel python 2.6.  I suspect this is where my problem lies.

I had to make the [BeautifulSoup patch](http://www.reddit.com/r/redditdev/comments/b21ri/reddit_start_to_finish_ubuntu_updates_needed/) from this thread.

Finally, I relinked /usr/bin/python from python2.5 to python2.6.

I'll crank the VM back up and update with the current error, but I'm sure there's some obvious things wrong with what I've done so far.

Thanks.",,False,,t5_2qizd,False,,,True,t3_bt606,http://www.reddit.com/r/redditdev/comments/bt606/trying_and_failing_to_get_reddit_installed/,
1262099786.0,8,self.redditdev,ajk0j,Problem with BeautifulSoup when setting Reddit up.,8,0,19,http://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/,"I've been setting up a local copy of Reddit for the first time today.
I'm fairly new to Linux (Ubuntu 9.10), Python and Pylons, so I'm not sure what caused this problem. But I found a work around so thought I'd post it.

I am following the Ubuntu instructions [http://code.reddit.com/wiki/RedditStartToFinishIntrepid](found here.)

The following line keeps crashing out because it can't find BeautifulSoup

    sudo python setup.py develop 

I solved the problem by adding the correct download path.

    sudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/

Not sure where the out of date download path is stored.

Edit: 

I then encountered another problem

    Building lxml version 2.2.4.
    NOTE: Trying to build without Cython, pre-generated 'src/lxml/lxml.etree.c' needs to be available.
    ERROR: /bin/sh: xslt-config: not found

    ** make sure the development packages of libxml2 and libxslt are installed **

I solved this by installing libxslt1-dev (no idea why this was needed. Perhaps this was installed by default on the version of Ubuntu the instructions where tested on.

    sudo apt-get install libxslt1-dev

Edit 2:
Now have a problem with awards , which probably did not exist when the instructions where written. I've tried solving it with 

    postgres$ createdb -E utf8 awards

But now I am having another problem

    File ""/usr/local/lib/python2.6/dist-packages/SQLAlchemy-0.5.3-py2.6.egg/sqlalchemy/engine/base.py"", line 931, in _handle_dbapi_exception
    raise exc.DBAPIError.instance(statement, parameters, e, connection_invalidated=is_disconnect)
    sqlalchemy.exc.ProgrammingError: (ProgrammingError) function ip_network(character varying) does not exist
    LINE 1: ...p_network_reddit_data_award on reddit_data_award (ip_network...
                                                             ^
    HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
    ""create index idx_ip_network_reddit_data_award on reddit_data_award (ip_network(value)) where key = 'ip'"" {}

Not sure what to do about this one.

Edit 3:

I was also missing the authorize table

    postgres$ createdb -E utf8 authorize

and the sql functions had not run correctly due to these missing tables (I think). so I re ran

    postgres$ psql newreddit &lt; ../sql/functions.sql

Now having problems with time-outs on pre-populating the data.

continued below",,False,,t5_2qizd,True,,,True,t3_ajk0j,http://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/,
1376296932.0,6,self.redditdev,1k738n,Show User Karma and Age on Listings. Take II.,7,1,3,http://www.reddit.com/r/redditdev/comments/1k738n/show_user_karma_and_age_on_listings_take_ii/,"I have written a Greasemonkey script that will add the [karma and account age to user names](http://imgur.com/bQ0PgyU). I have tried to [get permission to release this script before](http://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/) but the response was (probably?) negative, although I can only see that on the upvotes and reddit gold that the only commenter got who told me it would be too heavy on the servers. Well, I was aware of that and that is why I had asked for permission in the first place : )

I have now taken measures to make this script much less resource hungry. The script was aimed to help my moderation tasks, and as such I have now improved this script in two ways:

- The script is now limited to only work in subreddits that the logged in user moderates. I.e. it is now also only useful to someone who *is* a moderator.
- It now uses the jQuery inView plugin to only load about.json for such usernames that are currently in the viewport. The delay of 2100ms between requests to about.json remains.

So what do you say now, redditdevs?

EDIT: I have now incorporated a cache in localStorage for this script and removed the ability to show karma. The script only shows user age now. The script was now [released the script to userscripts.org](http://userscripts.org/scripts/show/175700). Please let me know if there are issues.",,False,,t5_2qizd,1376448931.0,,,True,t3_1k738n,http://www.reddit.com/r/redditdev/comments/1k738n/show_user_karma_and_age_on_listings_take_ii/,
1376012291.0,5,self.redditdev,1jzxqn,Looking for permission to release a certain Greasemonkey script or improve it.,9,4,2,http://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/,"I have written a Greasemonkey script that, similar to [Reddit Uppers and Downers](http://userscripts.org/scripts/show/56641), will add the karma and account age to user names. [Here is a screenshot](http://imgur.com/bQ0PgyU). It does this by making calls to /user/*username*/about.json for (most) usernames visible on a page. In order to stay within the guidelines the API calls have a delay of 2100ms from each other.

The script identifies itself as User-Agent *""GM/show_user_karma_and_age/1.0 by dub4u""*.

1. Can I release this script as is?
2. Is there a way to improve the script such that it does not have so many calls to the API? In other words, can I call about.json for multiple usernames in one single HTTP request?",,False,,t5_2qizd,False,,,True,t3_1jzxqn,http://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/,
1375175913.0,6,self.redditdev,1jc7gy,A script to turn an AMA into an interview,6,0,15,http://www.reddit.com/r/redditdev/comments/1jc7gy/a_script_to_turn_an_ama_into_an_interview/,"Before I go back to my code editor, I just wanted to make sure that this didn't already exist somewhere: I find AMAs very hard to read, even with RES. When I come too late to ask a question myself, all I want to read is the answers of the OP and the related question. It would be easy to do it but if it's already out there, let's not do it again.  
  
Do you know of somethnig similar?  
Would you find it useful?  
  
Edit: maybe not the best subreddit for that, but if this becomes a project it will rely on the API so... relevant-ish?",,False,,t5_2qizd,False,,,True,t3_1jc7gy,http://www.reddit.com/r/redditdev/comments/1jc7gy/a_script_to_turn_an_ama_into_an_interview/,
1374294110.0,7,github.com,1iofh2,I've made a short script to unhide hidden posts. Any suggestions/critiques?,8,1,2,http://www.reddit.com/r/redditdev/comments/1iofh2/ive_made_a_short_script_to_unhide_hidden_posts/,,,False,,t5_2qizd,False,,,False,t3_1iofh2,https://github.com/DanielGibbsNZ/reddit-unhider,
1373612799.0,5,self.redditdev,1i51by,My PRAW code ran for 3 days then died.,10,5,9,http://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/,"A picture of the traceback can be found [here](http://imgur.com/lY8vSLd). My code simply pulls data regarding submissions from a large number of subreddits and places them in a database. I'm not entirely sure what's happened here, but it seems like an internal error from praw. Maybe one of you guys can help me figure it out? Thanks!",,False,,t5_2qizd,False,,,True,t3_1i51by,http://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/,
1373548301.0,7,self.redditdev,1i2svy,Batch request data from Reddit API,7,0,0,http://www.reddit.com/r/redditdev/comments/1i2svy/batch_request_data_from_reddit_api/,"I am looking to write a API scraper for the Reddit API which will pull data for all subreddits.

I am aware of the API rules and will keep the the 30 calls per minute.

What I want to know is which endpoints would be best to call?

The process will in some way be prioritized so that more requested endpoints will be updated more frequently.

I will also want to retrieve comments for each of the 'Things'.

You guys got any advice or design ideas for this. ",,False,,t5_2qizd,False,,,True,t3_1i2svy,http://www.reddit.com/r/redditdev/comments/1i2svy/batch_request_data_from_reddit_api/,
1372452823.0,5,github.com,1h9t80,[PRAW] A little script I wrote to transfer one account's subscriptions to another. Anyone got any feedback?,6,1,3,http://www.reddit.com/r/redditdev/comments/1h9t80/praw_a_little_script_i_wrote_to_transfer_one/,,,False,,t5_2qizd,False,,,False,t3_1h9t80,https://github.com/jamesturner/reddit-subscription-sync,
1372249326.0,6,self.redditdev,1h3qir,How does reddit know who I am?,7,1,10,http://www.reddit.com/r/redditdev/comments/1h3qir/how_does_reddit_know_who_i_am/,"Hi all...

I know this may be a silly question. I know reddit cache almost everything. There is a div containing user information on top right of reddit's page. It has a id of ""header-bottom-right"". My question is, if the page I get from reddit is cached, say, when I log in, how does reddit know what is my username? There are so MANY MANY reddit users, how does reddit cache
pages among which only user information are different but main content of these pages are the same? Or does reddit only cache content data and render page on every request? Or is that rendered by javascript? I know I can read the source code of reddit to find out how it works, but reddit's source code is way too complicated to me to read....

Sorry if this is a silly question, but I'm new to web developing and learning it by myself... I really want to know how to solve this issue because I'm trying to implement a basic caching mechanism.... Any tips will be appreciated. Thanks!",,False,,t5_2qizd,1372249966.0,,,True,t3_1h3qir,http://www.reddit.com/r/redditdev/comments/1h3qir/how_does_reddit_know_who_i_am/,
1371867822.0,7,self.redditdev,1gu272,Suddenly receiving http 403 response codes from API urls.,7,0,10,http://www.reddit.com/r/redditdev/comments/1gu272/suddenly_receiving_http_403_response_codes_from/,"I've been using the API urls for a while now and have not seen this. After I launched my bot this morning /u/BlackjackBot, I am suddenly now noticing these 403s.  I believe I have been careful to respect all of the API usage rules.  Am I being locked out? Is there someone I can ask about this? My user agent includes my reddit username (as per the API usage rules suggestions) but no one has contacted me. What gives?
",,False,,t5_2qizd,False,,,True,t3_1gu272,http://www.reddit.com/r/redditdev/comments/1gu272/suddenly_receiving_http_403_response_codes_from/,
1371400569.0,6,self.redditdev,1gglok,How can I log in?,9,3,3,http://www.reddit.com/r/redditdev/comments/1gglok/how_can_i_log_in/,"I'm trying to write a Reddit framework and the API is giving me a hard time trying to log in.

Apparently, I need to POST *api_type=json&amp;user=USERNAME&amp;passwd=PASSWORD* to https://ssl.reddit.com/api/login/USERNAME (the /USERNAME part is not in the API docs but several tutorials on the net do it that way), with the User-Agent, Content-Length and Content-Type header fields set to the appropriate values.

Is that correct? If yes, there's another problem. I've apparently used up my login attempts (that's what happens when you misspell your password), and now I'm just getting a response saying I'm a bad robot. Can I change my user-agent and try again, or will the admins punch me in the balls when they see me doing that?

If no: what do I need to POST where with what parameters? GET works fine, but POST is what bothers me.",,False,,t5_2qizd,False,,,True,t3_1gglok,http://www.reddit.com/r/redditdev/comments/1gglok/how_can_i_log_in/,
1371338737.0,7,mikenon.github.io,1gfbec,RedditAnalytics.com's comment stream -&gt; PubSub,7,0,2,http://www.reddit.com/r/redditdev/comments/1gfbec/redditanalyticscoms_comment_stream_pubsub/,,,False,,t5_2qizd,False,,,False,t3_1gfbec,http://mikenon.github.io/CommentsPubSub/,
1370909392.0,5,self.redditdev,1g308t,"In PRAW, getting the object that is the parent of this comment",7,2,3,http://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/,"It is difficult to find an example of this code anywhere, but what I came up with is:

    def ParentObj(self, obj):
        assert type(obj) == praw.objects.Comment
        submission = self.rh.get_submission(url = obj.permalink)
        if obj.is_root:
            return submission
        return self.rh.get_submission(url = submission.permalink + obj.parent_id[3:])._comments[0]

This scares me for three reasons:

1. I am using _comments, which has an underscore in front of it, which implies to me that it's dubious or marginal or internal.

2. I am converting from an ID of the form ""t1_cag5h2j"" to one of the form ""cag5h2j"" by flushing the first three characters down
the toilet, which seems a scary way to convert from type A to type B.

3. I would think there would be something explicit somewhere that allows you to move up and down the tree, but I can't find it.

Did I implement this in a sane fashion? Should I scrape the comment tree or something instead?

I promise that I'm not an idiot, really.
",,False,,t5_2qizd,False,,,True,t3_1g308t,http://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/,
1370869998.0,6,self.redditdev,1g1jcl,I just wrote this Reddit / Imgur Image Downloader feedback would be welcome,9,3,0,http://www.reddit.com/r/redditdev/comments/1g1jcl/i_just_wrote_this_reddit_imgur_image_downloader/,"I just wrote an image downloader using C# / WPF and part of someones redditAPI wrapper. Unlike most imgur downloaders I've seen this one grabs images per subreddit so you can easily get all your favorite wallpapers or images from any subreddit.

If anyone would like to try it out and make any suggestions for future development I'd be really grateful as I'm wanting to add some good features in upcoming versions:

Here's the link to the post I made in learnprogramming: http://www.reddit.com/r/learnprogramming/comments/1fzonv/reddit_image_downloader/",,False,,t5_2qizd,False,,,True,t3_1g1jcl,http://www.reddit.com/r/redditdev/comments/1g1jcl/i_just_wrote_this_reddit_imgur_image_downloader/,
1370754802.0,4,self.redditdev,1fyusj,What is clicked in Reddit's JSON?,7,3,1,http://www.reddit.com/r/redditdev/comments/1fyusj/what_is_clicked_in_reddits_json/,"What does ""clicked"" represent? All of mine are false and I can't figure out why. My initial thought was that is turns true if the user has at any point selected a link but I guess not.",,False,,t5_2qizd,False,,,True,t3_1fyusj,http://www.reddit.com/r/redditdev/comments/1fyusj/what_is_clicked_in_reddits_json/,
1370460408.0,6,self.redditdev,1fqo32,Forked reddit/reddit some time ago - how to keep it up to date?,10,4,6,http://www.reddit.com/r/redditdev/comments/1fqo32/forked_redditreddit_some_time_ago_how_to_keep_it/,"I used github to fork reddit/reddit a year or so ago. I don't have any local git installation; I've used github.com only.

I'd like to get back into helping fix bugs and contributing code.

Do I need to take any steps to refresh or update my fork?

I'm worried my fork contains year-old code, and of course I'd want to start with the latest files.

If I'm misunderstanding how Github works, please let me know :)",,False,,t5_2qizd,False,,,True,t3_1fqo32,http://www.reddit.com/r/redditdev/comments/1fqo32/forked_redditreddit_some_time_ago_how_to_keep_it/,
1369543198.0,5,self.redditdev,1f2fuy,Logic for when posts and comments are archived and can no longer be commented on?,7,2,1,http://www.reddit.com/r/redditdev/comments/1f2fuy/logic_for_when_posts_and_comments_are_archived/,"Old submissions prevent you from leaving new comments. Does anyone know where that logic is for that cutoff occurs? Is it a hard archival date or something more complex?

Additionally, when a submission gets archived, do all comments become un-reply-able or is the can_reply toggle based on the timestamp of the comment (not the submission)?",,False,,t5_2qizd,False,,,True,t3_1f2fuy,http://www.reddit.com/r/redditdev/comments/1f2fuy/logic_for_when_posts_and_comments_are_archived/,
1369532471.0,6,self.redditdev,1f266q,Team Fortress April Fools Code?,7,1,2,http://www.reddit.com/r/redditdev/comments/1f266q/team_fortress_april_fools_code/,"I tried looking, but couldn't seem to find it... is this available somewhere? ",,False,,t5_2qizd,False,,,True,t3_1f266q,http://www.reddit.com/r/redditdev/comments/1f266q/team_fortress_april_fools_code/,
1368181792.0,5,reddit.com,1e288g,A subreddit recommender with Machine Learning [x-post from /r/MachineLearning],7,2,0,http://www.reddit.com/r/redditdev/comments/1e288g/a_subreddit_recommender_with_machine_learning/,,,False,,t5_2qizd,False,,,False,t3_1e288g,http://www.reddit.com/r/MachineLearning/comments/1dzoim/a_subreddit_recommender_with_machine_learning/,
1368119517.0,6,self.redditdev,1e0ffk,Did something in the Reddit API change last night? I can no longer log in insecurely with PRAW 2.0.15 hosted on PythonAnywhere,7,1,2,http://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/,"I've been running a script on PythonAnywhere which logs in using PRAW. I've run it every day, and it hasn't failed until just now, which is strange because I didn't change anything in my code or my PRAW installation. I therefore can only conclude that the Reddit API must have changed, or PythonAnywhere must have changed something. Does anyone know how I might fix this?

    Traceback (most recent call last):
      File ""bravery20.py"", line 1314, in &lt;module&gt;
        r.login(username=username, password=password)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/__init__.py"", line 906, in login
        self.request_json(self.config['login'], data=data)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/decorators.py"", line 223, in error_checked_function
        return_value = function(cls, *args, **kwargs)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/__init__.py"", line 407, in request_json
        response = self._request(url, params, data)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/__init__.py"", line 294, in _request
        timeout=timeout)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/decorators.py"", line 64, in __call__
        result = self.function(reddit_session, url, *args, **kwargs)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/decorators.py"", line 167, in __call__
        return self.function(*args, **kwargs)
      File ""/home/person/.local/lib/python2.7/site-packages/praw/helpers.py"", line 137, in _request
        allow_redirects=False, auth=auth)
      File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 399, in post
        return self.request('POST', url, data=data, **kwargs)
      File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 354, in request
        resp = self.send(prep, **send_kwargs)
      File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 460, in send
        r = adapter.send(request, **kwargs)
      File ""/usr/local/lib/python2.7/site-packages/requests/adapters.py"", line 246, in send
        raise ConnectionError(e)
    requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retries exceeded with url: http://www.reddit.com/api/login/.json (Caused by 
    &lt;class 'socket.error'&gt;: [Errno 111] Connection refused)

I'm using PRAW 2.0.15 because the latest version doesn't work for some reason; also, I've removed `""login""` from the `SSL_PATHS` list in `__init__.py` in order to prevent PRAW from attempting to connect securely (which PythonAnywhere doesn't support).

Thanks in advance.",,False,,t5_2qizd,False,,,True,t3_1e0ffk,http://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/,
1367293180.0,6,self.redditdev,1de0a6,How was Alien Blue created?,10,4,6,http://www.reddit.com/r/redditdev/comments/1de0a6/how_was_alien_blue_created/,"I'm very interested in any information pertaining to how someone would build a reddit app for iOS.  I was under the impression that you had to code in objective-c to work iOS, but I see that the reddit API is written in python.  

I'm somewhat of a newb, I appreciate any assistance or relavant articles.  Thanks!

",,False,,t5_2qizd,False,,,True,t3_1de0a6,http://www.reddit.com/r/redditdev/comments/1de0a6/how_was_alien_blue_created/,
1367237115.0,6,self.redditdev,1dbzvd,Tags on Reddit,8,2,6,http://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/,"Hey!

One thing I have not yet completely figured out is how tags work on Reddit. Are they assigned to submissions by users? Can they ""invent"" new tags or do they have to use existing ones?

Is there a way to crawl the tags? E.g., via PRAW?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1dbzvd,http://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/,
1366973235.0,5,self.redditdev,1d5adk,How can you tell if Reddit is down using praw?,9,4,12,http://www.reddit.com/r/redditdev/comments/1d5adk/how_can_you_tell_if_reddit_is_down_using_praw/,"Im a little embarrassed asking this, but I cant find it anywhere.  Surely its an exception of some kind, but looking over the exceptions listed in the documentation I dont see anything specific to this.  Should I just wrap my API calls in a try, except and assume that if they fail its because Reddit is down and go to sleep for a while?",,False,,t5_2qizd,False,,,True,t3_1d5adk,http://www.reddit.com/r/redditdev/comments/1d5adk/how_can_you_tell_if_reddit_is_down_using_praw/,
1366647583.0,5,self.redditdev,1cvckr,"In the context of OAuth, is the 30 requests / minute limit applied to each access token or to the client?",6,1,4,http://www.reddit.com/r/redditdev/comments/1cvckr/in_the_context_of_oauth_is_the_30_requests_minute/,"When requesting on behalf of a user utilizing their token, is the request limit considered against that individual user (access token) or are all requests on behalf of all users by a single client rate limited?",,False,,t5_2qizd,False,,,True,t3_1cvckr,http://www.reddit.com/r/redditdev/comments/1cvckr/in_the_context_of_oauth_is_the_30_requests_minute/,
1366556582.0,5,self.redditdev,1csvar,Persistent store for praw?,7,2,9,http://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/,"I know praw already cache's results from reddit for 30 seconds but I'm looking for a way to add a more persistent data store so that I only hit reddit once for any ""thing"" in its API, ever.  

For now, I don't care about seeing any changes due to edits or votes but if one did, I guess that would complicate things.

I can come up with something explicit that is layered on top of praw but having this store insinuated into praw so it's transparent to any existing praw-using code seems like a good design.

Any thoughts on this?  Any work in this direction yet?

Edit: I'm looking for something that persists between executions.",,False,,t5_2qizd,1366565763.0,,,True,t3_1csvar,http://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/,
1366532274.0,5,self.redditdev,1csi2o,Admin tools with email verification,7,2,1,http://www.reddit.com/r/redditdev/comments/1csi2o/admin_tools_with_email_verification/,"Hi guys,

I read /u/foolblog's [fantastic guide](http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/) on how to gain admin privileges, and I noticed that he mentioned that admins have very few powers until their email address is verified. I couldn't get email to work either, so I just ran the [hack that was in the comments section](http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/c8r2hj1). Now the admin account has a verified email address. Great!

The thing is, I don't see any new tools that I couldn't access before. Notably, when I visit a user's page, the admin drop-down menu is completely empty. Is it supposed to be empty, or do you guys see a few options in there? Are there any other admin tools you've noticed once ""verifying"" an email address?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1csi2o,http://www.reddit.com/r/redditdev/comments/1csi2o/admin_tools_with_email_verification/,
1365933617.0,5,self.redditdev,1cbicr,/api/friend issue,6,1,2,http://www.reddit.com/r/redditdev/comments/1cbicr/apifriend_issue/,"Hello,

When I try to invite a new moderator to a subreddit on my clone, I get the 500 error occured message. The weird this is that the user gets the actual invite message. So, I checked the logs, and found this: http://pastebin.com/YiVvWJ6W

I followed the trace quite a bit, and found that the issue is probably when the /api/friends request is made and during the validation. 

This happens only when I want to add a moderator to a subreddit which has unicode characters in name, because I did that modification to enable UTF8 chars in name. For regular subreddit names, this error doesn't pop up.

So, my question is can anyone provide a piece of advice how to fix this, and avoid showing of that error?

Thanks in advance!

EDIT: WOW IT'S MY CAKEDAY!",,False,,t5_2qizd,False,,,True,t3_1cbicr,http://www.reddit.com/r/redditdev/comments/1cbicr/apifriend_issue/,
1365590965.0,7,self.redditdev,1c209w,Reddit has now made it impossible to get old submissions except one at a time?,11,4,7,http://www.reddit.com/r/redditdev/comments/1c209w/reddit_has_now_made_it_impossible_to_get_old/,"I've noticed that www.reddit.com/new/ no longer returns anything after the few most recent thousands of submissions or so.  Is the only way to get old submissions with the API just one id at a time?

At the rate of getting one every two seconds, it will take eons to get any real data. 

Why has this undocumented change been made?  You will now force developers to make 100x more hits to your server for the same information they used to be able to get for one hit.

Worse still, there is no longer spam filtering in place. 

Can a developer pay a fee to get more access?  As it stands now, our archival / search engine project is impossible now.",,False,,t5_2qizd,False,,,True,t3_1c209w,http://www.reddit.com/r/redditdev/comments/1c209w/reddit_has_now_made_it_impossible_to_get_old/,
1364391983.0,6,self.redditdev,1b3xp1,Instructions for configuring AWS Cloudsearch,6,0,3,http://www.reddit.com/r/redditdev/comments/1b3xp1/instructions_for_configuring_aws_cloudsearch/,"Hi Guys,  Does anyone have instructions for how to configure cloudsearch?  Specifically, I am not sure how to create the indexes.

Thanks,",,False,,t5_2qizd,False,,,True,t3_1b3xp1,http://www.reddit.com/r/redditdev/comments/1b3xp1/instructions_for_configuring_aws_cloudsearch/,
1362455838.0,6,self.redditdev,19op8t,3rd-party app authentication,7,1,8,http://www.reddit.com/r/redditdev/comments/19op8t/3rdparty_app_authentication/,"I'm building an app, purely for learning purposes, but I'm building it with the idea that it'd be public-facing/others would use it.  As such, I imagine a user providing Reddit credentials and it gathering their saved posts.  

What I'm stuck on (as a n00b) is how do I capture their credentials, securely, NOT storing them, but allowing a scheduled sync (or even a user-forced sync (while still abiding by Reddit's API rules))?

I fear I'd have to store them for future syncing and I could do so with some level(s) of encryption, but then how do I unpack that later? :-/ Perhaps that question is too broad or outside the scope of /r/redditdev

None the less...I thought I'd ask. 

Thanks! ",,False,,t5_2qizd,False,,,True,t3_19op8t,http://www.reddit.com/r/redditdev/comments/19op8t/3rdparty_app_authentication/,
1362042483.0,5,self.redditdev,19dzi0,"reddit install problem:  is it possible to install cassandra as needed, in ubuntu 11.10?",7,2,6,http://www.reddit.com/r/redditdev/comments/19dzi0/reddit_install_problem_is_it_possible_to_install/,"Hi,
I don't know if it's possible to install reddit in Ubuntu 11.10 (oneiric).  If it isn't, then please ignore what follows.

I would like to install Cassandra, one of the dependencies for the reddit.  However, after adding the reddit ppa with:

    ~$ sudo add-apt-repository ppa:reddit/ppa
    [sudo] password for jack: 
    You are about to add the following PPA to your system:
     reddit ppa
 
     More info: https://launchpad.net/~reddit/+archive/ppa
    Press [ENTER] to continue or ctrl-c to cancel adding it

    gpg: keyring `/tmp/tmp6aCndX/secring.gpg' created
    gpg: keyring `/tmp/tmp6aCndX/pubring.gpg' created
    gpg: requesting key 65506D27 from hkp server      keyserver.ubuntu.com
    gpg: /tmp/tmp6aCndX/trustdb.gpg: trustdb created
    gpg: key 65506D27: public key ""Launchpad PPA for reddit""   imported
    gpg: Total number processed: 1
    gpg:               imported: 1  (RSA: 1)
    OK

, it seems that the ppa is not available for oneiric
    
    W: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/source/Sources  404  Not Found
    
    W: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/binary-i386/Packages  404  Not Found

Indeed, there is no package called *cassandra*:
    ~$ sudo apt-get install cassandra
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    Package cassandra is not available, but is referred to by another package.
    This may mean that the package is missing, has been obsoleted, or
is only available from another source
    
    E: Package 'cassandra' has no installation candidate

What can I do in this case?  Use the ppa for other ubuntu releases?

Thanks in advance for any help.


",,False,,t5_2qizd,False,,,True,t3_19dzi0,http://www.reddit.com/r/redditdev/comments/19dzi0/reddit_install_problem_is_it_possible_to_install/,
1361830594.0,4,self.redditdev,197uz8,PRAW: url_data removed from get_content(),6,2,3,http://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/,"Hello, redditdev!

I'm using PRAW to pull some basic information from Reddit, namely the top 500 posts from /r/pics. My code is very straightforward:

    import praw

    r = praw.Reddit(user_agent='MrFanzyPanz Datascraper :D')

    for post in r.get_subreddit('pics').get_hot(limit=500, url_data={'limit': 100}):
      
        *post.name
        *post.score
        *post.author
        *post.created_utc

I'm writing it out to a csv to play around with.

My problem is that, while this worked a couple weeks ago, I've revisited the scraper, and I'm now getting this error when I run it:
	
    TypeError: get_content() got an unexpected keyword argument 'url_data'

Was url_data removed in an update? I've update PRAW to the newest version.

Thanks!		

",,False,,t5_2qizd,False,,,True,t3_197uz8,http://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/,
1361780994.0,5,self.redditdev,196m6v,How to enable thumbnails?,7,2,0,http://www.reddit.com/r/redditdev/comments/196m6v/how_to_enable_thumbnails/,"Ive already created an S3 bucket for the thumbnails, and fixed the scrapping part. When posting links to youtube, video player is already working good. But when placing other links, no thumbnail is show, I am sure that the scrapping part is good, and that the thumbnail was stored on S3.

**EDIT:** Okay, ive got some answers to my own question. The default preference for users is to show media(thumbnails) on subreddit. Ive edited the default value in the models/account.py, pref_media variable, to ""on"". Now this sets media to be shown by default once a user is newly created. But on the Front page, or where the users is not logged in, no media thumbnails is still shown.  Does anyone know what file needs to be edited for this to be done?",,False,,t5_2qizd,1361811389.0,,,True,t3_196m6v,http://www.reddit.com/r/redditdev/comments/196m6v/how_to_enable_thumbnails/,
1361242625.0,7,self.redditdev,18sl6k,Firefox can't find the server at reddit.local.,8,1,6,http://www.reddit.com/r/redditdev/comments/18sl6k/firefox_cant_find_the_server_at_redditlocal/,"**update: justoman diagnosed and solved the problem**

&gt; **When it does the redirect it's redirecting without the port number. Try hitting just http://reddit.local:8081**

I think that I did the installation correctly, but trying to open http://127.0.0.1:8081/ redirects to http://www.reddit.local/ and then complains: ""Firefox can't find the server at reddit.local.""

I'm running Linux Mint ( Release 13 Maya, 32-bit; Kernel Linux 3.2.0-23-generic)

Here's the terminal in which I start paster:

     indeed@indeedbox ~ $ cd ~/reddit/r2     
     indeed@indeedbox ~/reddit/r2 $ paster serve --reload example.ini http_port=8081
     Starting subprocess with file monitor
     Overriding g.http_port to 8081
     indeedbox:19824 started e301b34 at 20:36:55 (took 0.59s)
     Starting server in PID 19824.
     serving on 0.0.0.0:8081 view at http://127.0.0.1:8081


And here's the last bits of syslog:

     Feb 18 18:55:37 indeedbox dbus[716]: [system] Successfully activated service 'com.ubuntu.SoftwareProperties'
     Feb 18 19:17:01 indeedbox CRON[17150]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
     Feb 18 19:50:21 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted
     Feb 18 20:17:01 indeedbox CRON[19030]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
     Feb 18 20:29:09 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted
     Feb 18 20:52:53 indeedbox sudo: pam_ecryptfs:       pam_sm_authenticate: /home/indeed is already mounted
     
What should I be looking at to find the problem, please?
",,False,,t5_2qizd,1361326908.0,,,True,t3_18sl6k,http://www.reddit.com/r/redditdev/comments/18sl6k/firefox_cant_find_the_server_at_redditlocal/,
1360042419.0,4,self.redditdev,17x2g0,How do you find the liked status of comments on http://www.reddit.com/message/inbox.json ?,8,4,12,http://www.reddit.com/r/redditdev/comments/17x2g0/how_do_you_find_the_liked_status_of_comments_on/,I don't see any likes field.,,False,,t5_2qizd,False,,,True,t3_17x2g0,http://www.reddit.com/r/redditdev/comments/17x2g0/how_do_you_find_the_liked_status_of_comments_on/,
1359923103.0,7,self.redditdev,17tn7r,Replacement for all_comments_flat,7,0,5,http://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/,"Hey,

I have a bot that uses all_comments_flat to parse a submission's comments. I just set up a new development station and noticed that the newest version of praw doesn't have that method on the Submission object anymore, and the comment attribute is a tree. What's the easiest way to just get all the comments on a submission? I'm suspecting that I'll need to use some kind of ""load more comments"" function to get all the available comments but I'm not sure where it is. 

Thanks,",,False,,t5_2qizd,False,,,True,t3_17tn7r,http://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/,
1358352043.0,8,self.redditdev,16otwf,Client side auth without disclosure of private key.,8,0,2,http://www.reddit.com/r/redditdev/comments/16otwf/client_side_auth_without_disclosure_of_private_key/,"I'd like this to be possible. It's one of my biggest issues with oauth.

Basically what I'd like is a JavaScript client that never touches my server but that has full API access.",,False,,t5_2qizd,False,,,True,t3_16otwf,http://www.reddit.com/r/redditdev/comments/16otwf/client_side_auth_without_disclosure_of_private_key/,
1357183583.0,6,self.redditdev,15v7l4,Retrieving a user's comments seems to be limited to 1000... is there a way around this?,8,2,3,http://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/,"The title says it all. I have some hope that a workaround is possible, particularly since I can still *see* older comments (from old saved links or old submissions), they just don't seem to be tied to the user page. My instinct says that the comments still exist in the database some where, but is there any way to get to them?

N.B. I'm using PRAW.",,False,,t5_2qizd,False,,,True,t3_15v7l4,http://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/,
1355858609.0,6,self.redditdev,152d2w,Praw URL parameters,6,0,2,http://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/,"Hello!

Silly question here, sorry for the noobacity.

I'm trying to use praw's get_content() generator in order to pull data from subreddits, however I don't know how to iterate over this object. I keep receiving an error that pics.json is an unknown url type:

    import praw
    import pprint

    r = praw.Reddit('Test Code - MrFanzyPanz')

    content_scrape = r.get_content(""pics"", limit = 10)

    for submission in content_scrape:
        print submission

**or**

        print submission.title

If there is an alternative method which will allow me to simply parse www.reddit.com/.json as text, that would be also fine, although I need to be able to do it through python requests (preferably praw), rather than copy-pasting the text from the page manually.",,False,,t5_2qizd,False,,,True,t3_152d2w,http://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/,
1355073940.0,6,self.redditdev,14jy6b,How to get a user's saved links,6,0,6,http://www.reddit.com/r/redditdev/comments/14jy6b/how_to_get_a_users_saved_links/,"It doesn't seem that the API has an option for getting a user's saved links. I've tried to use the [reddit PHP SDK](https://github.com/jcleblanc/reddit-php-sdk) to authenticate and then grab the json contents, but all I get is a 403 error.

Code:

    new reddit(""username"", ""password"");
    print_r(json_decode(file_get_contents('http://www.reddit.com/user/username/saved.json'),true));

If I go to `http://www.reddit.com/user/username/saved.json` in my browser while logged in, I can see the data fine.

Any ideas?



**Edit: Solved.**

I added a function to the PHP SDK:

    /**
    * Get saved posts
    *
    * Get the listing of a user's saved posts 
    * @param string $username The username.
    */
    public function getSaved($username){
        return $this-&gt;runCurl(""http://www.reddit.com/user/"".$username.""/saved.json"");
    }

Usage:

    require_once('reddit-sdk.php');
    $reddit = new reddit(username, password);
    $saved = $reddit-&gt;getSaved(username);
    print_r($saved);

I've submitted a pull request to jcleblanc to have this function incorporated into the PHP SDK.",,False,,t5_2qizd,1355076004.0,,,True,t3_14jy6b,http://www.reddit.com/r/redditdev/comments/14jy6b/how_to_get_a_users_saved_links/,
1354909114.0,7,self.redditdev,14gjdj,Enable thumbnails on individual requests,7,0,0,http://www.reddit.com/r/redditdev/comments/14gjdj/enable_thumbnails_on_individual_requests/,"Hello /r/redditdev,

I was wondering if there was a way to enable thumbnails for a listing request through a GET variable (or any other method). For example, if you have thumbnails disabled in preferences, is there a way to override that setting for individual requests?

Thanks in advance

",,False,,t5_2qizd,False,,,True,t3_14gjdj,http://www.reddit.com/r/redditdev/comments/14gjdj/enable_thumbnails_on_individual_requests/,
1353858383.0,7,self.redditdev,13rfz0,How do I change the client UserAgent in a Phonegap App to comply with the reddit rules?,8,1,2,http://www.reddit.com/r/redditdev/comments/13rfz0/how_do_i_change_the_client_useragent_in_a/,"The reddit API rules state *Change your client's User-Agent string to something unique and descriptive, preferably referencing your reddit username.*

Any idea how I would do this in html and/or JavaScript?",,False,,t5_2qizd,False,,,True,t3_13rfz0,http://www.reddit.com/r/redditdev/comments/13rfz0/how_do_i_change_the_client_useragent_in_a/,
1353834522.0,7,self.redditdev,13r65c,"""Likes"" status of comment replies from /message/inbox?",8,1,1,http://www.reddit.com/r/redditdev/comments/13r65c/likes_status_of_comment_replies_from_messageinbox/,"When you fetch the JSON for your messages, you can get comment replies / post replies that look like this:

    {
        ""kind"": ""t1"",
        ""data"": {
            ""body"": ""Excellent.  :) Thanks!"",
            ""was_comment"": true,
            ""first_message"": null,
            ""name"": ""t1_c76bdd6"",
            ""created"": 1353844643.0,
            ""dest"": ""ross456"",
            ""author"": ""GoodGuyAlex"",
            ""created_utc"": 1353815843.0,
            ""body_html"": ""&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\""md\""&amp;gt;&amp;lt;p&amp;gt;Excellent.  :) Thanks!&amp;lt;/p&amp;gt;\n&amp;lt;/div&amp;gt;&amp;lt;!-- SC_ON --&amp;gt;"",
            ""subreddit"": ""reddit_to_go"",
            ""parent_id"": ""t4_c76b9t4"",
            ""context"": ""/r/reddit_to_go/comments/13qfoo/feature_request_post_comment_by_hitting_ctrlenter/c76bdd6?context=3"",
            ""replies"": null,
            ""new"": false,
            ""id"": ""c76bdd6"",
            ""subject"": ""comment reply""
        }
    }

Note that there's no ""likes"" field set to true/false/null. If viewing your messages on reddit.com, you'll get voting arrows next to messages like these, but an app won't be able to initialize similar arrows to the correct state.

Is it possible to get the returned data changed to properly set the vote status?",,False,,t5_2qizd,False,,,True,t3_13r65c,http://www.reddit.com/r/redditdev/comments/13r65c/likes_status_of_comment_replies_from_messageinbox/,
1350545173.0,6,self.redditdev,11ogy1,How can I set up Trac to make wiki work on my reddit clone?,7,1,0,http://www.reddit.com/r/redditdev/comments/11ogy1/how_can_i_set_up_trac_to_make_wiki_work_on_my/,"I've tried to find a way to make trac work, so far no light.
I thought just installing is end of work, however authentication stuff, subdomain etc,. are confusing.
Can anyone help me? I'm using lighttpd.",,False,,t5_2qizd,False,,,True,t3_11ogy1,http://www.reddit.com/r/redditdev/comments/11ogy1/how_can_i_set_up_trac_to_make_wiki_work_on_my/,
1348884152.0,8,self.redditdev,10nfai,request: OAuth last longer than 10 minutes,9,1,2,http://www.reddit.com/r/redditdev/comments/10nfai/request_oauth_last_longer_than_10_minutes/,"Hi,

I'm interested in validating user logins via OAuth instead of making a POST request and depending on the cookie.

Everything works great except users have to re-hit accept every ten minutes due to the expiration time on OAuth keys which Reddit returns being so short a time period.

I was wondering if Reddit had plans to increase the expiration time on these keys, or if I should stay clear of OAuth?

Thanks,
David",,False,,t5_2qizd,False,,,True,t3_10nfai,http://www.reddit.com/r/redditdev/comments/10nfai/request_oauth_last_longer_than_10_minutes/,
1347363954.0,6,self.redditdev,zpcoi,All IP address is saved as 127.0.0.1,6,0,5,http://www.reddit.com/r/redditdev/comments/zpcoi/all_ip_address_is_saved_as_127001/,"Finally I can change my reddit clone to production stage. Thank you all of you. 
However I got a problem. I backup postgresql db, then I can see details of DB. All IP related data like registration_ip are saved as 127.0.0.1.
Did I miss some option? ",,False,,t5_2qizd,False,,,True,t3_zpcoi,http://www.reddit.com/r/redditdev/comments/zpcoi/all_ip_address_is_saved_as_127001/,
1344334012.0,6,self.redditdev,xtf08, connect to localhost port 11211 (tcp) failed !!!,6,0,7,http://www.reddit.com/r/redditdev/comments/xtf08/connect_to_localhost_port_11211_tcp_failed/,"+ service cassandra start
+ echo 'Waiting for services to be available, see source for port meanings...'
Waiting for services to be available, see source for port meanings...
+ for port in 11211 5432 5672 9160
+ nc -vz localhost 11211
nc: connect to localhost port 11211 (tcp) failed: Connection refused
+ sleep 1
",,False,,t5_2qizd,False,,,True,t3_xtf08,http://www.reddit.com/r/redditdev/comments/xtf08/connect_to_localhost_port_11211_tcp_failed/,
1342645072.0,4,self.redditdev,ws3j7,5 characternumerics thing_Id in reddit URL - how is it generated?,7,3,12,http://www.reddit.com/r/redditdev/comments/ws3j7/5_characternumerics_thing_id_in_reddit_url_how_is/,"In reddit URL, there is ""5 characternumerics"" thing_id part (for example, ""wplf7"" from ""http://redd.it/wplf7"") which is generated by base36. 

wplf7 is generated from number 54941875 =&gt; to36(54941875) =wplf7 : this is what I found so far...I'm wondering how this number ""54941875"" is generated in the first place? Is it by category? or just random?

Anyone who can explain this in the simple manner? Unfortunately Python is not my domain and 2000 lines of python code listed on Reddit wiki didn't help me much. 
",,False,,t5_2qizd,False,,,True,t3_ws3j7,http://www.reddit.com/r/redditdev/comments/ws3j7/5_characternumerics_thing_id_in_reddit_url_how_is/,
1341615504.0,5,self.redditdev,w5iec,"lib/cssfilter.py out of date; library code duplicated in 
application",8,3,1,http://www.reddit.com/r/redditdev/comments/w5iec/libcssfilterpy_out_of_date_library_code/,"lib/cssfilter.py fails to validate some recent CSS3 features, including RGBA color values. There is a pending pull request [1] regarding this issue, but I looked a little deeper.

The Python library cssutils, which is used by lib/cssfilter.py for the bulk of the validation work, seems to support RGBA colors, as well as numerous other features that are duplicated in the reddit code. For example, see this same list of color names in the Reddit code at [2] and in the library at [3].

I note that lib/cssfilter.py has had only a handful of substantive changes in the last year, while cssutils acquired these features in the last few months. I believe the current version of the library has made most of lib/cssfilter.py obsolete.

Reddit's setup.py does specify an older version of the library that does not contain the features I mentioned, but I'm not sure it's necessary. The version was pinned three years ago [4] due to ""API breakage"". In my judgment, using a newer version of the library is worth it for the gigantic code deduplication and more-correct CSS validation.

Is this a project worth pursuing, or am I missing something?


[1] https://github.com/reddit/reddit/pull/461

[2] https://github.com/reddit/reddit/blob/master/r2/r2/lib/cssfilter.py#L65

[3] https://bitbucket.org/cthedot/cssutils/src/024c6fff92d7/src/cssutils/css/colors.py#cl-17

[4] https://github.com/reddit/reddit/commit/498c3b01fce22e1761f1dd6a93038a9dfdeda13f#r2/setup.py",,False,,t5_2qizd,False,,,True,t3_w5iec,http://www.reddit.com/r/redditdev/comments/w5iec/libcssfilterpy_out_of_date_library_code/,
1335651351.0,7,self.redditdev,sxcig,Extending Reddit: Adding more fields to text link submission?,7,0,8,http://www.reddit.com/r/redditdev/comments/sxcig/extending_reddit_adding_more_fields_to_text_link/,"To skip background/motivation go to the next section ;)

------------------------------------------------

I am working on a website for an annual graduate research conference that would like to make some of their internal paper reviews public, so that the review process is more transparent to the research community.  Hopefully this will help even the playing field for researchers who are not intimate with the review process, lead to better quality paper submissions, and foster productive discussion, perhaps even innovation, regarding the process itself.

I initially thought of the reddit source engine for this purpose, but decided to go another route because of its excessive features (both user interface -- ads, gold, search, etc., and back-end enterprise design -- memcached, cassandra, etc.).  It simply had too many features and was too complicated for our needs.  I ended up prototyping a quick site from scratch using pyramid, which worked pretty well.  Unfortunately I have now been asked to add a lot more features such as comment sorting, moderation, voting, etc.  This has prompted me take a second look at the reddit source to see how hard it would be to adapt to our needs.

So I spent this morning getting the latest version of the source up and running on a Xen Ubuntu 11.04 VM, then proceeded to dig in to how to modify it for our needs.  The [ubuntu install script](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) worked great (after I finally found it, heh), other than some problems with Cassandra crashing unexpectedly (mostly due to VM issues, but it took forever to debug :/).  I managed to figure out (mostly) how to hide / disable stuff we don't need through css / templates, but I'm having a bit harder timing adding additional features we need (since that involves, you know, real coding...).  I as a bit disappointed that I couldn't find any high level descriptions of or tutorials for hacking the actual code.  I guess I should go look at the pylons layout specifically?  Is it  fairly close to a normal pylons project?

Notably I am just a lowly graduate student who was notified this had to be done ""urgently"" with less than a weeks notice for the intended rollout :/.  Obviously that's not going to happen at this point, but any expedited input would be greatly appreciated =)

------------------------------------------------

Basically what I would like to do is force all submissions to be ""text"" and enable extra fields (such as authors). Then I would like to add an additional ""review"" section with multiple fields (such as reviewer and text) to the top of that ""link""'s page.  This review should be addable only by moderators, but should have some sort of form submission.  So for starters, **does anyone have any tips on what I would have to do to enable additional submission fields in the "".../r/blah/submit"" form, then have them display on the "".../r/blah/comments/xxxx/xxxxx"" page for that submission?**  Perhaps just a high level description of the process, including which files would have to be modified (and how), would be a *huge* help.

Presumaby I would have to modify:

   1) r2/templates/newlink.html to add the additional form fields.  (Very novice question... is the templating language Mako?  I started getting used to Chameleon with my prototype, but I'm not very familiar with Python web dev.)
    
   2) Whatever model this form gets submitted to.  (Presumably it will still be routed correctly, right?  Maybe for the purposes of understanding, someone could mention how it gets routed there?)
    
   3) The persistent storage.  How is the data actually mapped back to SQL / Cassandra?

The next step will be to make the review submission form for moderators, but I think if I understand how to do this I will have a much better idea of how to tackle that.  Perhaps this post was a bit hasty, since I am still very much exploring on my own, but I figured the sooner I ask the sooner I will get feedback ;).  Regardless, I think others could benefit from a description of how to do this, as I searched for a bit and couldn't really find much (let me know if I just missed it).  Thanks!",,False,,t5_2qizd,True,,,True,t3_sxcig,http://www.reddit.com/r/redditdev/comments/sxcig/extending_reddit_adding_more_fields_to_text_link/,
1332868078.0,6,self.redditdev,rg2we,"Request: unread counts for mail, mod mail, mod mail by subreddit (details inside)",13,7,4,http://www.reddit.com/r/redditdev/comments/rg2we/request_unread_counts_for_mail_mod_mail_mod_mail/,"Just a suggestion:

Currently, to get unread counts, I have to download the JSON that corresponds with the ""listing"" of mail - meaning, for example, /message/unread.json ... then I count the results... that's a decent bit of unnecessary data for Reddit to send, and for me to parse through.

For mod mail, there isn't (that I'm aware of) even an unread page -- so I'd need to download 100 messages (since that's what my prefs are set at) and parse through them individually counting up the ""new"":""true"" flags to determine the unread count...

It would be great if I could just make a simple query that returned only the data I need... something like:
    {
      ""unread"":""6"",
      ""last_unread_timestamp"": [UTC timestamp]
    }

I'm not even sure what other data would be needed / useful beyond that, but I'm guessing it'd be more efficient than downloading the JSON data that includes the text of every message, etc...

Ideally, I'd love to have this for:

- messages/mail

- mod mail  [general]

- mod mail [separated by subreddit, i.e. unread messages to r/whatever]

While I'm at it:

- mod queue [# of reported posts that need to be tended to] might also be good!",,False,,t5_2qizd,False,,,True,t3_rg2we,http://www.reddit.com/r/redditdev/comments/rg2we/request_unread_counts_for_mail_mod_mail_mod_mail/,
1332396904.0,4,self.redditdev,r81dl,API Submit - cannot seem to get it working,8,4,11,http://www.reddit.com/r/redditdev/comments/r81dl/api_submit_cannot_seem_to_get_it_working/,"I'm using the apigee Reddit console attempting to get a simple test submit working. All required fields are filled in. I used the api/login to obtain a uh and cookie. I copied the uh and added it to the query. I've added the cookie returned to the header as both cookie and reddit_session. I've set api_type to json and left that off. As a last ditch I tried adding user and passwd to the query, again failure. 

Based on the API wiki in github(including talklittle) I thought this sufficient, but I'm obviously missing something.  No matter what I try I always get a response that user is required and to please login. Here's an example request, I'm at a loss:

    POST /api/submit?uh=blah&amp;text=Test of API Submit on apigee&amp;kind=self&amp;sr=redditdev&amp;title=API Submit Test&amp;r=redditdev&amp;api_type=json HTTP/1.1
     cookie: blah",,False,,t5_2qizd,True,,,True,t3_r81dl,http://www.reddit.com/r/redditdev/comments/r81dl/api_submit_cannot_seem_to_get_it_working/,
1331306894.0,7,self.redditdev,qoyf3,Rate limit exceeded when trying to login,9,2,8,http://www.reddit.com/r/redditdev/comments/qoyf3/rate_limit_exceeded_when_trying_to_login/,"Hi everyone.
I'm trying to login but it gives an error saying rate limit exceeded try again in 5 hours.
I wasnt flooding the api. i just did it once!",,False,,t5_2qizd,False,,,True,t3_qoyf3,http://www.reddit.com/r/redditdev/comments/qoyf3/rate_limit_exceeded_when_trying_to_login/,
1330805334.0,5,self.redditdev,qg8mf,Reddit Python API question,7,2,10,http://www.reddit.com/r/redditdev/comments/qg8mf/reddit_python_api_question/,"how do I get the top post of the week in a given subreddit?

Also is there a way to do the something with comments?",,False,,t5_2qizd,False,,,True,t3_qg8mf,http://www.reddit.com/r/redditdev/comments/qg8mf/reddit_python_api_question/,
1329836870.0,5,self.redditdev,pzd8l,How can I delete a subreddit and remove it from the top bar?,7,2,0,http://www.reddit.com/r/redditdev/comments/pzd8l/how_can_i_delete_a_subreddit_and_remove_it_from/,"I am setting up a reddit clone and I am trying to figure out how I can completely delete a subreddit. I have banned the subreddit, removed all the posts from it, and even tried going into the database and deleting its entries but the link still appears in the top bar when you go to the site, regardless of whether or not you are logged in.

I thought it might have just been cached but it has been more than 24 hours now and it still appears. I also ran ""update_reddits.sh"" and that did not change anything. 

I have coding experience, but I'm new to web apps and databases, so any help is greatly appreciated.",,False,,t5_2qizd,False,,,True,t3_pzd8l,http://www.reddit.com/r/redditdev/comments/pzd8l/how_can_i_delete_a_subreddit_and_remove_it_from/,
1329816208.0,6,self.redditdev,pz4xj,How can I get a list of all posts in a subreddit?,8,2,8,http://www.reddit.com/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/,"I've been looking through the reddit API on reddit, but I haven't found a way to get a list of *all* of the posts (just title, date, etc) from a subreddit. How can I do that? Thanks!",,False,,t5_2qizd,False,,,True,t3_pz4xj,http://www.reddit.com/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/,
1327611443.0,6,self.redditdev,oy3ie,Captcha error on api/submit,9,3,2,http://www.reddit.com/r/redditdev/comments/oy3ie/captcha_error_on_apisubmit/,"I am trying to submit a story via the api (at /api/submit) - currently i have the reddit_session cookie, uh, title, text, sr, and kind in the request. However, when I get the response back, it is giving me a captcha error:

{""jquery"": [[0, 1, ""call"", [""body""]], [1, 2, ""attr"", ""find""], [2, 3, ""call"", ["".status""]], [3, 4, ""attr"", ""hide""], [4, 5, ""call"", []], [5, 6, ""attr"", ""html""], [6, 7, ""call"", [""""]], [7, 8, ""attr"", ""end""], [8, 9, ""call"", []], [1, 10, ""attr"", ""captcha""], [10, 11, ""call"", [""vXuRjliIHD1LsA8IexwjuYjuxT1ZZ0rv""]], [1, 12, ""attr"", ""find""], [12, 13, ""call"", ["".error.BAD_CAPTCHA.field-captcha""]], [13, 14, ""attr"", ""show""], [14, 15, ""call"", []], [15, 16, ""attr"", ""text""], [16, 17, ""call"", [""care to try these again?""]], [17, 18, ""attr"", ""end""], [18, 19, ""call"", []]]}

The api docs don't mention anything about needing a captcha for api/submit, so I went to look at the API code (https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py) - I could not find any place under Post_submit that has a captcha.

Trying to figure out what I'm doing wrong, and if I do need a captcha, how would I go about implementing this.

Thanks!",,False,,t5_2qizd,False,,,True,t3_oy3ie,http://www.reddit.com/r/redditdev/comments/oy3ie/captcha_error_on_apisubmit/,
1325809170.0,7,self.redditdev,o4op9,Accessing moderator-restricted data via API,10,3,0,http://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/,"I'm working on a bot to help with some of the common moderation tasks, but it seems like the API data doesn't include most or all of the moderator-restricted data that can be viewed through the site itself, like:

* number of reports on a comment/submission
* the moderator that approved a submission (if anyone)
* the moderator that removed a submission (if anyone)
* whether a submission/comment on the spam page was posted by a shadow-banned user

Is there any way to access these?",,False,,t5_2qizd,False,,,True,t3_o4op9,http://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/,
1323866369.0,5,self.redditdev,ncbok,Problem calling api/me.json,7,2,6,http://www.reddit.com/r/redditdev/comments/ncbok/problem_calling_apimejson/,"Lately I've encountered a bug in a previously reliable python script I was using to update the flair for /r/sports

I find that if I log in and then make a call to /api/me.json no data is returned.  If I log in via the browser and access http://www.reddit.com/api/me.json in the browser it loads all of the data fine.  But my python client fails.  Without getting the modhash value from me.json I'm unable to make subsequent calls to the api.

Is anyone else having similar problems?  Is it possible my client has been banned or something?  I'm essentially using this client: https://github.com/logan/reddit-hacks/blob/master/redditclient.py",,False,,t5_2qizd,False,,,True,t3_ncbok,http://www.reddit.com/r/redditdev/comments/ncbok/problem_calling_apimejson/,
1321479504.0,8,groups.google.com,mev1j,Reddit Recommendor: Google Group to coordinate progress,12,4,4,http://www.reddit.com/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/,,,False,,t5_2qizd,False,,,False,t3_mev1j,http://groups.google.com/group/rrecommender,
1318969868.0,7,self.redditdev,lgo4r,Why is there a difference between the posts shown on /r/all/new/.json and /r/all/new/.json?jsonp=callback,8,1,10,http://www.reddit.com/r/redditdev/comments/lgo4r/why_is_there_a_difference_between_the_posts_shown/,"Example 1: http://www.reddit.com/r/all/new/.json
Example 2: http://www.reddit.com/r/all/new/.json?jsonp=callback

Is it a bug? How to fetch the latest posts in a jsonp format?
",,False,,t5_2qizd,False,,,True,t3_lgo4r,http://www.reddit.com/r/redditdev/comments/lgo4r/why_is_there_a_difference_between_the_posts_shown/,
1318457730.0,5,self.redditdev,la2vf,"Never worked with reddit/Python apps before, could use some guidance from anyone who's willing",9,4,0,http://www.reddit.com/r/redditdev/comments/la2vf/never_worked_with_redditpython_apps_before_could/,"Hey guys. I've installed reddit for a client of mine and he needs a few changes done. I've never worked with any Python apps before (I'm mostly a Rails guy), and I'm just so lost in this framework. 
The first thing I'm having a problem with is that I can't get the app to serve my modified stylesheets (reddit.css). Even after I restart the entire server (no idea how to stop/start a Python app), it still serves the old file. There must be an application cache somewhere?
    
That's not really the main problem though, as I'm sure I can figure that one out. He needs the app to scrub any incoming posts/comments, etc for certain URLs that we will then need to append information onto. Can anyone help me find where I should be getting this done? In Rails you would generally do this in the data model, with a callback that runs the method before the object gets saved to the database. I have no idea where to even start with this, and any help would be greatly greatly appreciated.",,False,,t5_2qizd,False,,,True,t3_la2vf,http://www.reddit.com/r/redditdev/comments/la2vf/never_worked_with_redditpython_apps_before_could/,
1317978600.0,6,self.redditdev,l3z1o,I have a problem watching youtube links via reddit.,8,2,10,http://www.reddit.com/r/redditdev/comments/l3z1o/i_have_a_problem_watching_youtube_links_via_reddit/,"Below is my stunning summary of how I access youtube links, since just clicking on them won't work:

[Part 1](http://i.imgur.com/m02ra.png)

[Part 2](http://imgur.com/cTIfV.png)

Sorry for the horrible visual representation. Anyone have a clue what's wrong? Is this the correct subreddit for this by the way?",,False,,t5_2qizd,False,,,True,t3_l3z1o,http://www.reddit.com/r/redditdev/comments/l3z1o/i_have_a_problem_watching_youtube_links_via_reddit/,
1316666049.0,7,self.redditdev,knkpl,Getting started with contributing - Good first problem to tackle.,10,3,6,http://www.reddit.com/r/redditdev/comments/knkpl/getting_started_with_contributing_good_first/,"I have been programming in python for a while now and I am familiar with some web frameworks (django specifically).  I want to get involved in an open source project and Reddit seemed like an amazing fit (I wanted to give back to it too!) I have the system up and running and I am starting to look into the code, but I was wondering what might be a good first problem to tackle and get my toes wet.  I didn't see anything under the issue tracker that stood out in particular.  Any suggests on a good bug/feature to work on?

I also have some experience with Markdown.  If there's anything that would be good to document on the wiki I would be down for doing that too.  Cheers!",,False,,t5_2qizd,False,,,True,t3_knkpl,http://www.reddit.com/r/redditdev/comments/knkpl/getting_started_with_contributing_good_first/,
1315602677.0,6,self.redditdev,kagvo,New install - how do I get an admin account?,7,1,6,http://www.reddit.com/r/redditdev/comments/kagvo/new_install_how_do_i_get_an_admin_account/,I just setup a new local install of reddit but I have no idea how to create an admin account. How do I do this?,,False,,t5_2qizd,False,,,True,t3_kagvo,http://www.reddit.com/r/redditdev/comments/kagvo/new_install_how_do_i_get_an_admin_account/,
1312930977.0,6,self.redditdev,jdztk,Weird issues when trying to install reddit,6,0,7,http://www.reddit.com/r/redditdev/comments/jdztk/weird_issues_when_trying_to_install_reddit/,"Hi!

I have a weird problem when trying to install reddit. It all began with the ""keyspace reddit not found"" error message (more details [here](https://github.com/reddit/reddit/issues/182#issuecomment-1755430)). I was advised to create a new keyspace using cassandra-cli. However when I've tried to connect using the ""connect localhost/9160;"" I was greeted by this:

*Login failure. Did you specify 'keyspace', 'username' and 'password'?*

Could anyone lend me a hand with this? I'm not really knowledgeable about Cassandra so I have no clue about the problems I might encounter and their solution (I'm a PHP type of guy :P).

Also every time I start the ""whole shebang"" (i.e. Cassandra and everything that goes with it) I'm unable to shut it down. The program doesn't seem to react to commands such as ""service cassandra stop"" or ""/etc/init.d/cassandra stop"" at all. Therefore first I had to rename /usr/bin/svscanboot, shut it down, then proceed with shutting down the rest of the stuff required by reddit that doesn't shut down by shutting down svscanboot. It goes witthout saying that PostgreSQL isn't affected by this at all. Any ideas about this and/or suggestions for a possible fix?",,False,,t5_2qizd,False,,,True,t3_jdztk,http://www.reddit.com/r/redditdev/comments/jdztk/weird_issues_when_trying_to_install_reddit/,
1310998946.0,7,self.redditdev,istty,Sending large numbers of PMs through API,9,2,0,http://www.reddit.com/r/redditdev/comments/istty/sending_large_numbers_of_pms_through_api/,"A little over a month ago, [I set up a bot in /r/gaming that uses the API and processes PMs that people send to it](http://www.reddit.com/r/gaming/comments/hzp98/official_whats_rgaming_playing_statistics/).

Now I want to start ""expiring"" the data from particular users if they haven't sent in an update for a while, but I'd like to send the user a PM when their submission expires. Sending a PM from the bot's account forces a captcha, so that doesn't seem workable.

Would it be acceptable to send the PMs from my own account, or will a certain volume of them get me flagged somehow, or start requiring a captcha after some point? The bot currently checks for new messages once every 5 minutes, so I'll probably just process a maximum of one expiration on each run, which means the fastest I'll ever be sending these PMs out is one every 5 minutes.

Is that a reasonable approach, or is there some other way I could make it work through the bot account?",,False,,t5_2qizd,False,,,True,t3_istty,http://www.reddit.com/r/redditdev/comments/istty/sending_large_numbers_of_pms_through_api/,
1308624834.0,6,self.redditdev,i4wkk,Reddit's custom domain name thing: it's just an iframe?,9,3,15,http://www.reddit.com/r/redditdev/comments/i4wkk/reddits_custom_domain_name_thing_its_just_an/,"Not 100% sure where to post this..  
When you create a subreddit you are allowed to create a CNAME and link to it from your own domain name. I did this and it seems that the only thing that happens is that reddit's servers serve an iframe linking to a trivially modified version of the subreddit itself. One click and you are off the domain entirely. Am I missing something or is that working as intended?  
http://reddit.sixcorners.info/  

Edit: I'm stupid.. I should have said frameset and frame instead of iframe..",,False,,t5_2qizd,True,,,True,t3_i4wkk,http://www.reddit.com/r/redditdev/comments/i4wkk/reddits_custom_domain_name_thing_its_just_an/,
1306598576.0,5,self.redditdev,hmad8,"I tried installing Reddit on my laptop...but after installing Cassandra, I couldn't use linux at all.  Is this normal?",9,4,13,http://www.reddit.com/r/redditdev/comments/hmad8/i_tried_installing_reddit_on_my_laptopbut_after/,"I was installing the prerequisites, and after I installed cassandra, my mouse started slowing down, and then my computer froze.  So I hard-restarted it.  The same thing happened again, but this time after I logged in from the login screen for Ubuntu.  I kept trying, and sometimes it happened during the login screen, sometimes after I login but before things loaded, but at no point was my system usable.  I couldn't even get to a virtual terminal.

I had to reboot with my Ubuntu CD and go to /media/whatever/etc/init.d/cassandra and delete it.  After that, it boots just fine.  And yeah, I installed Ubuntu *yesterday*.  This was a fresh system.

Is this normal?

EDIT: I guess it was sorta like a forkbomb...like all my memory was used up.  I couldn't do anything.  Also, I should note that I'm kinda a linux noob-intermediate, and not at all a programmer.  I just wanted to see how reddit worked, really.",,False,,t5_2qizd,False,,,True,t3_hmad8,http://www.reddit.com/r/redditdev/comments/hmad8/i_tried_installing_reddit_on_my_laptopbut_after/,
1297464551.0,7,self.redditdev,fjr2p,User's Total upvote/downvote API,8,1,2,http://www.reddit.com/r/redditdev/comments/fjr2p/users_total_upvotedownvote_api/,"Is there any way to get a user's total upvote/downvote count through the api? http://www.reddit.com/user/adotout.api just seems to return my posts, which isn't what I'm looking for. ",,False,,t5_2qizd,False,,,True,t3_fjr2p,http://www.reddit.com/r/redditdev/comments/fjr2p/users_total_upvotedownvote_api/,
1297436947.0,7,self.redditdev,fjgtj,Trending on Reddit?,9,2,5,http://www.reddit.com/r/redditdev/comments/fjgtj/trending_on_reddit/,"So I thought it would be neat to determine some trends on Reddit through the use of concept extraction from titles and comments, comment volume, and up/down votes.

Of course, in order to do that, I would need access to data. I could crawl say, the politics or worldnews subreddits, but perhaps there is a sample database I could use instead?

If there isn't a sample database available, are there specific rules (other than respecting robots.txt) to crawling reddit?",,False,,t5_2qizd,False,,,True,t3_fjgtj,http://www.reddit.com/r/redditdev/comments/fjgtj/trending_on_reddit/,
1295163673.0,5,self.redditdev,f36q5,Updating the VM,6,1,11,http://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/,"**TL;DR**  Getting the following error when attempting to run the application after rebuilding the application from latest source in the VM:
    ...
      File ""/usr/local/lib/python2.6/dist-packages/pycassa-0.3.0-py2.6.egg/pycassa/columnfamily.py"", line 6, in &lt;module&gt;
        from cassandra.ttypes import Column, ColumnOrSuperColumn, ColumnParent, \
    ImportError: cannot import name Mutation

Appears to be related to the pycassa and/or possibly the thrift python packages, but I don't know where to go from here.

-----------------------------------------

Before I get into the details I feel it necessary to point out that while I'm an experienced programmer, I've done very little in the way of Python development (basically, I'm pretty much a newb in that regard). Also, I'm only slightly more experienced with regard to Linux. I'm reasonably capable on the command-line but it's not my primary OS and I'll be the first to admit that I'm no guru. Please forgive me if I erred in some way in attempting to get this to work. 

I'm trying to setup a local Reddit dev environment partly as an exercise and partly to see if I might in some small way contribute to the community. To this end I reasoned that the best place to start would be to download the VM.  Despite having some minor issues with getting the image start VM (VT wasn't enabled in the BIOS and the HP BIOS had dumbed down setting labels which didn't allow me to quickly identify which settings were in need of toggling) eventually I got it running and everything came up fine. Once the VM booted up I happily found that all the services started up and I was able to access the server from a browser in the host.

Of course, anytime you want to begin coding, or even just looking at the code, you will want to work off the latest version. So I attempted to update the sources under the ~/reddit folder. This is where things went awry.

At first I tried updating it by pulling in the latest source and did manage to get it to build, but I ran into the error described above and then tried to completely replace that folder by removing it and then following the instructions in the 
[Start to Finish](http://code.reddit.com/wiki/RedditStartToFinish#CheckingoutGIT) guide to pull down the latest code.

After many false starts, I finally managed to get it to build, but every attempt to get it to run has failed with the above error. I'm guessing that there is some basic incompatibility with the pycassa and thrift packages installed, but I've not determined how to resolve this and Google isn't providing many clues on this one. I did try installing updated versions of pycassa (modified the ~/reddit/r2/setup.py to reference a slightly newer version) but I determining that was a dead end and restored to 0.3.0 version. I then followed [these instructions](http://pycassa.github.com/pycassa/installation.html) for updating the Thrift python package. Updating thrift python packages also had no effect.

If anyone has any ideas as to what has gone wrong or if you require more information please let me know.

FYI: I did run into issues in trying to build latest sources whereby it was necessary to install the Cython package and update the BeautifulSoup package. I'm currently trying to verify these and any other steps which were taken prior to getting the error above and I will post those once the refreshed VM decides to cooperate (PostgreSQL didn't start properly for some reason and it's taking forever to shutdown). I had every intention of posting a step-by-step guide of updating the VM when I could not find one, but until I can get it to work I can't guarantee the steps I've taken are valid. Of course if a guide to getting the VM up-to-date exists, please feel free to point it out!

EDIT: formatting

**EDIT:** Given up on using the existing VM. Please read my comment below regarding getting a fresh install to work.",,False,,t5_2qizd,True,,,True,t3_f36q5,http://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/,
1291942699.0,6,self.redditdev,ej9az,Reddit Timing Out,9,3,1,http://www.reddit.com/r/redditdev/comments/ej9az/reddit_timing_out/,"I have been playing around with the reddit VMWare image and have been trying to replace the [reddit header in the upper left hand corner](http://static.reddit.com/reddit.com.header.png) with my own image. By default it is set to [http://static.reddit.com/reddit.com.header.png](http://static.reddit.com/reddit.com.header.png). 
 
I've searched for help and found instructions telling me to change the string referencing this location that is found in [/r2/r2/models/subreddit.py](http://code.reddit.com/browser/r2/r2/models/subreddit.py) on line 721 to be the new location. I can no longer remember where I found the documentation telling me how to do this and I can't seem to find it in any searches. 
 
I made this change, and restarted my reddit. Afterward, my reddit would no longer load. It times out every time. Even after undoing the changes I made, the reddit still times out. I have repeated this process once already, so I am fairly certain that this change is what is causing my problem. 
 
* Firstly, I believe that there has got to be a better way to apply changes to .py files than rebooting reddit. I would be highly appreciative of anyone who can let me know how to do this. 

* Second, does anyone have any idea how I can recover my reddit from this, ideally without having to start it from scratch? 

* Third, does anyone know of a working way to change the reddit header image?  

Thanks everyone!

    EDIT: I managed to solve this problem. It turns out that the documentation I had read before was correct 
    and modifying the subreddit.py file will change your header image. It would have helped if I had gotten the 
    URL I was using for the replacement image correct. (I had been copying from a file that had http;//)

    I would still appreciate anybody would could instruct me as to how to apply the changes I make without 
    having to reboot. Thanks again!",,False,,t5_2qizd,True,,,True,t3_ej9az,http://www.reddit.com/r/redditdev/comments/ej9az/reddit_timing_out/,
1291062045.0,5,self.redditdev,edi0g,Aggregate Submission Data,7,2,6,http://www.reddit.com/r/redditdev/comments/edi0g/aggregate_submission_data/,"I basically want to do the following query:

    select count(*), avg(votes), avg(comments) from submitted_links
    where user = &lt;me&gt;
    group by subreddit

Then I can display stuff using fancy graphs etc.

I started to implement this by downloading my data to a local db (props on the awesome API, btw) and working on it there, but I realized that this was kind of stupid if Reddit would allow me to build this functionality into the site. 

If I made this, would it be something that I could donate to reddit? Otherwise, it's probably easier for me to just do it locally.

EDIT: I realize reddit doesn't have that db layout, I was using it as pseudocode. A more realistic query might be something like:

    select avg(Ups - Downs) from Thing
    where Thing.Type=""Link"" AND Thing.ID in (
        select distinct Thing_ID from Data
        where Data.Key = ""User"" AND Data.Value = &lt;me&gt;
    )

I'm still downloading the VM, so that might not be exactly right, but you get the gist.
    ",,False,,t5_2qizd,True,,,True,t3_edi0g,http://www.reddit.com/r/redditdev/comments/edi0g/aggregate_submission_data/,
1289785522.0,7,self.redditdev,e6592,Reddit API: Set all unread messages as read ,9,2,12,http://www.reddit.com/r/redditdev/comments/e6592/reddit_api_set_all_unread_messages_as_read/,"I'm going to be interacting with the Reddit API for a little personal project of my own. To help facilitate this project I'm looking to add some functionality to the [Ruby Reddit API](https://github.com/jamescook/RubyRedditAPI).  

Adding the ability to get my list of unread messages was simple enough, and marking them as read and unread works just fine. That said, I'd love to be able to mark the messages as read when I fetch the unread list to avoid calling the API for each message I've read.  

I can't identify any way to do this in the API. Is there any way that any of you know to set all unread messages as read at the same time as the GET unread messages? Either that or some way to mass mark messages as read. I'd like to cut back on HTTP traffic.",,False,,t5_2qizd,False,,,True,t3_e6592,http://www.reddit.com/r/redditdev/comments/e6592/reddit_api_set_all_unread_messages_as_read/,
1284287226.0,5,github.com,dcrws,iReddit with better voting feedback,7,2,8,http://www.reddit.com/r/redditdev/comments/dcrws/ireddit_with_better_voting_feedback/,,,False,,t5_2qizd,False,,,False,t3_dcrws,http://github.com/rmccue/iReddit,
1280162682.0,6,self.redditdev,ctv5p,"I've got reddit set up on virtualbox, but need to make it available to my network. How can I get it to redirect to my servers IP?",8,2,5,http://www.reddit.com/r/redditdev/comments/ctv5p/ive_got_reddit_set_up_on_virtualbox_but_need_to/,"At the moment I can get onto the page 192.168.xx.xx as can anyone else on the network. However, the page then redirects to reddit.local. Which isn't on our dns. How can I change the redirect to continue to be 192.168.xx.xx/url?",,False,,t5_2qizd,False,,,True,t3_ctv5p,http://www.reddit.com/r/redditdev/comments/ctv5p/ive_got_reddit_set_up_on_virtualbox_but_need_to/,
1280101064.0,6,self.redditdev,ctlzs,"When you click reply, load new comments to the comment you are replying to",9,3,1,http://www.reddit.com/r/redditdev/comments/ctlzs/when_you_click_reply_load_new_comments_to_the/,"Can this be done, please? This came from ideasforadmins. raldi told me to ask here.",,False,,t5_2qizd,False,,,True,t3_ctlzs,http://www.reddit.com/r/redditdev/comments/ctlzs/when_you_click_reply_load_new_comments_to_the/,
1275517465.0,8,self.redditdev,caspu,A tool to export saved/liked stories to a database,9,1,3,http://www.reddit.com/r/redditdev/comments/caspu/a_tool_to_export_savedliked_stories_to_a_database/,"This has been requested more than a few times... e.g.

http://www.reddit.com/comments/9uicm/could_we_possibly_get_a_way_to_export_all_of_our/
http://www.reddit.com/comments/9z4e1/can_we_make_the_saved_page_searchable/
http://www.reddit.com/comments/a93wu/can_we_please_have_a_way_of_sorting_through_our/
http://www.reddit.com/comments/ackbz/saving_all_ones_reddit_posts_to_a_html_file/
http://www.reddit.com/comments/ak3lp/you_should_be_able_to_arrange_the_order_of_your/
http://www.reddit.com/comments/amxb6/a_way_to_categorize_saved_links/
http://www.reddit.com/comments/ca1l0/option_to_organize_saved_links_by_category/

Source is [here](http://github.com/achea/linkhive).  Currently, you can export your stories to a SQLite or MySQL database, and then 'view' them by typing in SQL statements into a desktop app.  [Screenshot](http://imgur.com/XIr7k).  I know, it's a little 'clunky' at the moment, but it works. :)  It does not have any categorizing/tagging features, which is next on the TODO list, after I'm satisfied with a design.  Does anyone have any suggestions?  Jimmyr has a [neat idea](http://www.reddit.com/r/programming/comments/b541x/i_made_an_organizer_for_my_bookmarks_after_3/c0l0oak?context=1), but I'm still looking for more.

Also, both the fetch and search programs are untested in Windows, but I've tried to code them to be as cross-platform as possible, so they should work.

Enjoy!",,False,,t5_2qizd,True,,,True,t3_caspu,http://www.reddit.com/r/redditdev/comments/caspu/a_tool_to_export_savedliked_stories_to_a_database/,
1274811949.0,7,self.redditdev,c806m,redditdev help: domain pages are broken... I think,8,1,4,http://www.reddit.com/r/redditdev/comments/c806m/redditdev_help_domain_pages_are_broken_i_think/,"Searching via domain leaves me in a strange predicament. I'm doing this from both my account and via a server authenticated (via api). I'll use imgur.com as the example: 

http://www.reddit.com/domain/imgur.com

Notice how everything is 2 months old? This is the same with all domains; it seems the cache is stuck for that (?) **however** it's working fine for new pages **when** set to rising:

http://www.reddit.com/domain/i.imgur.com/new/?sort=rising

but if you sort by new:

http://www.reddit.com/domain/i.imgur.com/new/?sort=new

It has the same problem, stuck!

Any ideas about this? If this can't be fixed it leaves me in a predicament but it's workable I guess, this is the functionality I'd prefer though. Any ideas? 
",,False,,t5_2qizd,False,,,True,t3_c806m,http://www.reddit.com/r/redditdev/comments/c806m/redditdev_help_domain_pages_are_broken_i_think/,
1269193443.0,6,self.redditdev,bg8wv,"Does reddit block IPs and if so, possible to get whitelisted?",7,1,7,http://www.reddit.com/r/redditdev/comments/bg8wv/does_reddit_block_ips_and_if_so_possible_to_get/,"Playing with the API and logging in via CURL (for legit things, happy to explain why if needed) however I get ""incorrect password"", it had worked before so I switched to another of my servers and bingo, worked fine. 

From this I can only deduct that my IP is blacklisted from logging in, I assume from too many attempts? (About 10 or so, testing things). Is it possible to be whitelisted? ",,False,,t5_2qizd,False,,,True,t3_bg8wv,http://www.reddit.com/r/redditdev/comments/bg8wv/does_reddit_block_ips_and_if_so_possible_to_get/,
1263981868.0,6,self.redditdev,aruiw,Problem with the reddit api,6,0,7,http://www.reddit.com/r/redditdev/comments/aruiw/problem_with_the_reddit_api/,"Hey reddit devs. I am trying to build a little app using the reddit api, accessing the latest comments by everyone on every subreddit.
The thing is that when i use the url http://www.reddit.com/comments.json?count=100 i always get the last 25 comments, it doesn't seem to be using the ""count"" parameters. Is this an intentional limitation or just a bug?
thanks for your help!",,False,,t5_2qizd,False,,,True,t3_aruiw,http://www.reddit.com/r/redditdev/comments/aruiw/problem_with_the_reddit_api/,
1262642054.0,5,self.redditdev,alj66,Reddit search improvement?,7,2,5,http://www.reddit.com/r/redditdev/comments/alj66/reddit_search_improvement/,"I have couple of points where you can make the search better. Note: I am not a troll, i have quite long experience with solr in making search and matching engines. 

 
1: Use lucene parser for user input. It is more natural, can use boosts on field:keyword pairs, can use wildcards and Boolean operators. In this case you can only use Dismax for ""must not have"" which can be made with lucene using AND NOT.

 
2: Don't use the LowercaseFilterFactory. It doesnt work as it should. Makes the wildcards unusable.

 


3: Default query operator should be AND. Internal query if i search for ""reddit alien"" should look something like this: 

    (title:(reddit+alien)^150+OR+content:(reddit+alien)^80)+AND+NOT+spam:(true)

The fields are OR'ed but user input is AND'ed. That way the search will be more accurate.



4: Add more boost to title, less to content and no boost to the other fields. Add the subreddit and other filters as facets with quotes. (more accurate).

That's it for now. If there is any interest in this i will continue with suggestions, i'll even write some examples. I am the developer of the search and matching engine for one big German recruiting platform. I have developed 3 additional projects utilizing solr/lucene.",,False,,t5_2qizd,False,,,True,t3_alj66,http://www.reddit.com/r/redditdev/comments/alj66/reddit_search_improvement/,
1259756271.0,7,self.redditdev,aa8k9,Small Javascript bug in user preferences section,7,0,2,http://www.reddit.com/r/redditdev/comments/aa8k9/small_javascript_bug_in_user_preferences_section/,"I already reported this bug through the feedback section last week, but as I got no response I'm sending it here.

The problem is really simple: when you're messing with your preferences, if you switch the **content language** radio button from **some languages** to **all languages**, you'll end up deactivating *all* checkboxes in the page (not only languages').

Where is the problem found: [reddit.js - line 1079](http://code.reddit.com/browser/r2/r2/public/static/js/reddit.js)

Proposed solution:

    $(elem).parents(""form"").find(""input[type=checkbox]"").attr(""checked"", false);

should be

    $(elem).parents(""td"").find(""input[type=checkbox]"").attr(""checked"", false);",,False,,t5_2qizd,True,,,True,t3_aa8k9,http://www.reddit.com/r/redditdev/comments/aa8k9/small_javascript_bug_in_user_preferences_section/,
1252793679.0,6,self.redditdev,9jyf7,Question re reddit algorithm,6,0,8,http://www.reddit.com/r/redditdev/comments/9jyf7/question_re_reddit_algorithm/,"I want to create a reddit site using their open source code, and I'm considering posting a project on Scriptlance/Elance in order to get someone to do this for me. (I have no tech skills, but a good idea for a site)

My question is, once the site is up and running, will it use the same algorithm as reddit.com does with regards to the popularity of the content posted?
",,False,,t5_2qizd,False,,,True,t3_9jyf7,http://www.reddit.com/r/redditdev/comments/9jyf7/question_re_reddit_algorithm/,
1251727084.0,5,self.redditdev,9fus2,Usability problem with submit page,8,3,3,http://www.reddit.com/r/redditdev/comments/9fus2/usability_problem_with_submit_page/,"There's a recurring problem with submitters who don't realize that text-posts are always self-posts.  People will occasionally start a submission, adding a title and a link, then click on the 'text' tab and type what they think is going to become the 'first post' in the submission, then submit; the result is that the text of the submission contains references to the link that has now been discarded.

This might be made more smooth if the title of the submission disappeared when switching between the 'link' and 'text' tabs (which might be enough of a ""what?"" moment to make the submitter understand that 'text' and 'link' posts are two different things).

A better fix, and I'm not sure that the DB schema supports it as-is, might be to just always allow a block of text to be submitted with every post, so that you can both link to an external site and post a block of text with the submission.",,False,,t5_2qizd,False,,,True,t3_9fus2,http://www.reddit.com/r/redditdev/comments/9fus2/usability_problem_with_submit_page/,
1246425852.0,7,self.redditdev,8x6x7,Do I need to do anything to optimize my installation?,7,0,6,http://www.reddit.com/r/redditdev/comments/8x6x7/do_i_need_to_do_anything_to_optimize_my/,"Hi! I've been testing my own reddit for a few days, and I notice that it's considerably slower than the real reddit on reddit.com, even though I have probably 4 subscribed uers and only a handful of submissions. My server is a VPS and I have plenty of resources available.

Is there anything I need to do to enable some caching or something that will speed things up? I searched Trac but I didn't find anything.
Thanks!",,False,,t5_2qizd,False,,,True,t3_8x6x7,http://www.reddit.com/r/redditdev/comments/8x6x7/do_i_need_to_do_anything_to_optimize_my/,
1237920604.0,5,self.redditdev,875lm,Notification on reply to someone else's comment,6,1,2,http://www.reddit.com/r/redditdev/comments/875lm/notification_on_reply_to_someone_elses_comment/,,,False,,t5_2qizd,False,,,True,t3_875lm,http://www.reddit.com/r/redditdev/comments/875lm/notification_on_reply_to_someone_elses_comment/,
1222191539.0,4,self.redditdev,732jw,/r/redditdev as deserted as it looks like?,7,3,3,http://www.reddit.com/r/redditdev/comments/732jw/rredditdev_as_deserted_as_it_looks_like/,,,False,,t5_2qizd,False,,,True,t3_732jw,http://www.reddit.com/r/redditdev/comments/732jw/rredditdev_as_deserted_as_it_looks_like/,
1214363041.0,6,self.redditdev,6oue4,Update on stats server?,7,1,7,http://www.reddit.com/r/redditdev/comments/6oue4/update_on_stats_server/,,,False,,t5_2qizd,False,,,True,t3_6oue4,http://www.reddit.com/r/redditdev/comments/6oue4/update_on_stats_server/,
1376609016.0,5,self.redditdev,1kgauh,"In PRAW, is there a way to request an entire comment tree without using more_comments?",5,0,2,http://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/,"I wanted to rebuild a comment tree to make some pretty graphs. I've been using comment.replies to pull child posts, and most of these can requests can be funneled through one call. However, while the code works, after 5 levels down the comment tree it stops pulling comments unless a ""more_comments"" call is made. This call has greatly increased the number of queries my code makes, and following PRAW's 2 second delay, larger comment trees take forever to pull.

Is there a more efficient way to go through the whole comment tree in PRAW?

Thanks, /r/redditdev!",,False,,t5_2qizd,False,,,True,t3_1kgauh,http://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/,
1375995653.0,4,self.redditdev,1jzcvv,Using Reddit's OAuth 2 with a bot,5,1,7,http://www.reddit.com/r/redditdev/comments/1jzcvv/using_reddits_oauth_2_with_a_bot/,"Hey everyone,

I'm misunderstanding something here about OAuth2 as it pertains to non-browser based applications.


So I'm at the point where I can generate proper URLs for protected, scoped resources but I'm not quite sure how a 'headless bot application' would go about retrieving the access token (barring some custom code to 'click' the allow access button - which seems totally wrong)


For example, say my bot has determined it should post a comment:
http://www.reddit.com/dev/api#POST_api_comment


First, it would need to get an access token using an URL like this one:
https://ssl.reddit.com/api/v1/authorize?state=[generated-state]&amp;redirect_uri=[my-apps-redirect-uri]&amp;response_type=code&amp;client_id=[my-key]&amp;duration=temporary&amp;scope=submit


Then, it would need to use that access token in it's next request to make a new comment. My disconnect right now is how to avoid needing to 'manually click' the allow access button - since my app is just a bot.


I've seen bots on reddit before, so I know this can be done - my google fu is just not very good at finding examples of this.


Thanks for the info in advance.",,False,,t5_2qizd,False,,,True,t3_1jzcvv,http://www.reddit.com/r/redditdev/comments/1jzcvv/using_reddits_oauth_2_with_a_bot/,
1375820225.0,5,self.redditdev,1ju1ut,Accessing a user's likes with node.js,6,1,4,http://www.reddit.com/r/redditdev/comments/1ju1ut/accessing_a_users_likes_with_nodejs/,"I'm using node.js as my server side language. I'm trying to get a json output that has all of a users likes. Here's what I have currently.
    
    const DEFAULT_PATH = ""http://reddit.com/"";
    var modhash;
    
    function userLogin(username, password, remember) {
        var options = {
            headers:{""Content-Type"":""application/json"",""X-Accept"":""application/json"" },
            data:{'api_type' : 'json', 'passwd' : password , 'rem' : remember, 'user' : username},
        }
        restler.post('https://ssl.reddit.com/api/login', options);
        restler.get(DEFAULT_PATH + '/api/me.json').on('success', function(data, response){
            if(data.json.data) {
                modhash = data.json.data.modhash;
                console.log(""got modhash"");
            } 
        });
    }
    
    function getLiked(username, password, remember) {
        userLogin(username, password, remember);
        
        var options = {
            headers:{""Content-Type"":""application/json"",""X-Accept"":""application/json""},
            data:{""username"":username}
        }
        
        restler.get(DEFAULT_PATH + 'user/' + username + '/overview.json', options).on('success', function(data, response){
            console.log(data);
        });
    }
When I run getLiked(USERNAME, PASSWORD, false); I get this output:
&gt;got modhash

&gt;{ json:

&gt;{ errors: [],

&gt;data:

&gt;{ modhash: 'Long string',

&gt;cookie: 'Number, Date, String' } } }

As far as I can tell i should be getting a json output that contains tons of different things, not just the modhash and cookie, like what appears when you enter http://reddit.com/user/USERNAME/liked.json

Also I'm using the [node.js restler module](https://github.com/danwrong/restler).",,False,,t5_2qizd,False,,,True,t3_1ju1ut,http://www.reddit.com/r/redditdev/comments/1ju1ut/accessing_a_users_likes_with_nodejs/,
1375507230.0,5,self.redditdev,1jm2ad,Is there a rate limit on voting?,6,1,4,http://www.reddit.com/r/redditdev/comments/1jm2ad/is_there_a_rate_limit_on_voting/,"I've tried to research this but to no avail. I know rate-limiting applies to submissions, commenting, and login, but I haven't seen anything about it applying to voting.",,False,,t5_2qizd,False,,,True,t3_1jm2ad,http://www.reddit.com/r/redditdev/comments/1jm2ad/is_there_a_rate_limit_on_voting/,
1374872052.0,5,self.redditdev,1j4dg5,What does the license let me do for using Reddit with commercial products?,5,0,6,http://www.reddit.com/r/redditdev/comments/1j4dg5/what_does_the_license_let_me_do_for_using_reddit/,"I'm working on a video game, and I would really love to use reddit's API for filtering through content if I ever have user created maps or levels. At the same time, I would like to sell my game so that I can make a living off of it. So what does the license for the reddit source code allow me to do with it? It looks like patents and trademarks are out, but can I copyright and sell a product that uses the reddit source code in a modified form?",,False,,t5_2qizd,False,,,True,t3_1j4dg5,http://www.reddit.com/r/redditdev/comments/1j4dg5/what_does_the_license_let_me_do_for_using_reddit/,
1374349293.0,5,self.redditdev,1ipl24,Problem logging in using reddit api,6,1,5,http://www.reddit.com/r/redditdev/comments/1ipl24/problem_logging_in_using_reddit_api/,"Hi,

I'm using python and just started fiddling around with reddit api. But I don't manage to get a account logged in. I'm doing this:

    h = httplib2.Http()
    h.request('https://ssl.reddit.com/api/login', 'POST', body='api_type=json&amp;user=myuser&amp;passwd=mypass')

And I always get a response with 'invalid password'. What's wrong with this code snippet?",,False,,t5_2qizd,False,,,True,t3_1ipl24,http://www.reddit.com/r/redditdev/comments/1ipl24/problem_logging_in_using_reddit_api/,
1374175036.0,7,self.redditdev,1iku9z,[Reddit Code] Can't see language changes after updating r2.po,7,0,1,http://www.reddit.com/r/redditdev/comments/1iku9z/reddit_code_cant_see_language_changes_after/,"I updated the language file of the arabic language. I restarted the app, flushed the cache and executed the ""make"" command in /reddit/reddit-i18n/ but I still can't see the changes I made. It still shows the English.

EDIT: I figured it out thanks to the guys in the IRC channel. You need to run these commands after editing.

&gt;cd $REDDIT_HOME/reddit-i18n/
&gt;
&gt;python setup.py build
&gt;
&gt;python setup.py develop
&gt;
&gt;make
&gt; 
&gt;cd /home/reddit/reddit/r2
&gt;
&gt;make",,False,,t5_2qizd,1374256448.0,,,True,t3_1iku9z,http://www.reddit.com/r/redditdev/comments/1iku9z/reddit_code_cant_see_language_changes_after/,
1373472716.0,4,self.redditdev,1i0jtf,is the reddit api restricting me from grabbing pictures?,5,1,10,http://www.reddit.com/r/redditdev/comments/1i0jtf/is_the_reddit_api_restricting_me_from_grabbing/,"I have a web app that gets a picture from any subreddit with pics, and I just click next and cycle through pictures. Sometimes any where from 1 to 5 images will fail to load in a row, I know there is some limit to the amount of requests I can make, but even if I go slowly it fails. I was wondering if its because the image size? I have it scaling, im not sure why else the images can fail?",,False,,t5_2qizd,False,,,True,t3_1i0jtf,http://www.reddit.com/r/redditdev/comments/1i0jtf/is_the_reddit_api_restricting_me_from_grabbing/,
1372774274.0,5,self.redditdev,1hhwse,Is there any way I can use PRAW to search for submissions that meet a certain criteria that were posted within a given timeframe?,6,1,0,http://www.reddit.com/r/redditdev/comments/1hhwse/is_there_any_way_i_can_use_praw_to_search_for/,"Here is my code:

    with open('test.csv', 'w') as fp:
        writer = csv.writer(fp)
        for submission in r.search('Obama OR obamacare'):
            id = submission.id
            title = submission.title
            url = submission.short_link
            score = submission.score
            created = submission.created
            created_date = datetime.fromtimestamp(submission.created)
            domain = submission.domain
            num_comments = submission.num_comments
            ups = submission.ups
            downs = submission.downs
            edited = submission.edited
            edited_date = datetime.fromtimestamp(submission.edited)
            author = submission.author
            a=(id, title, url, author, ups, downs, score, created, created_date, edited, edited_date, domain, num_comments)
            writer.writerow(a)


However, I would like to paginate through results so I get *everything* within a given timeframe, what is the best way to do that?",,False,,t5_2qizd,False,,,True,t3_1hhwse,http://www.reddit.com/r/redditdev/comments/1hhwse/is_there_any_way_i_can_use_praw_to_search_for/,
1372253985.0,6,self.redditdev,1h3uix,Any affordable hosting to host reddit code?,10,4,15,http://www.reddit.com/r/redditdev/comments/1h3uix/any_affordable_hosting_to_host_reddit_code/,"I noticed that reddit requires a specific set of apps and tools to work. I tried running it on my Linux Mint locally but that did not end well.

So I'm wondering if there's a hosting company that would support reddit code and is kind of affordable. I don't wanna end up paying thousands of dollars.",,False,,t5_2qizd,False,,,True,t3_1h3uix,http://www.reddit.com/r/redditdev/comments/1h3uix/any_affordable_hosting_to_host_reddit_code/,
1371518748.0,5,self.redditdev,1gk1f0,What is the best project I can do to learn python as a beginner?,9,4,3,http://www.reddit.com/r/redditdev/comments/1gk1f0/what_is_the_best_project_i_can_do_to_learn_python/,"I have some programming experience (some C, java, python). What is a cool project/program I can make in Python to learn more about the language? ",,False,,t5_2qizd,False,,,True,t3_1gk1f0,http://www.reddit.com/r/redditdev/comments/1gk1f0/what_is_the_best_project_i_can_do_to_learn_python/,
1371482974.0,5,self.redditdev,1giq7r,[Reddit API] Is there an OData $select equivalent?,7,2,1,http://www.reddit.com/r/redditdev/comments/1giq7r/reddit_api_is_there_an_odata_select_equivalent/,"Hello RedditDevs!

I was looking at the API itself and its parameters and have not been able to find any parameter which is similar to the $select function provided as a standard in odata.

For those that aren't aware of what $select does, its a parameter which allows you to pass through properties of the resource, and ONLY get returned the mentioned properties. Saves a LOT of bandwidth, makes responses faster.

Is anyone aware of a similar capability with the Reddit API?",,False,,t5_2qizd,False,,,True,t3_1giq7r,http://www.reddit.com/r/redditdev/comments/1giq7r/reddit_api_is_there_an_odata_select_equivalent/,
1371109938.0,6,self.redditdev,1g98a4,PRAW: Looking for the proper way to get a submission or comment object from a URL,7,1,4,http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/,"[This ancient thread is related.](http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/) That guy knew he had a comment, so he just did `submission = rh.get_submission(url = &lt;whatever&gt;)`, and then looked at `submission._comments[0]`, and that was his comment.

Assume I don't know whether my thing is a submission or a comment.

If I don't want to parse the URL, presumably I can do this:

    submission = rh.get_submission(url = my_url)
    if submission.permalink == my_url:
        object = submission
    elif submission._comments[0].permalink == my_url:
        object = submission._comments[0]
    else
        &lt;burst into flames&gt;

So, is this the proper way to do this? I'm concerned because I don't have total control over the URL, and the URL might not be normalized.

edit:

I'm concerned this would crash if the URL is not normalized and there are no comments, but that would presumably be an easy fix. It may also make sense to test the comment first and see if its ID is present in the URL, then test the submission ID against the URL:

    submission = rh.get_submission(url = my_url)
    if len(submission._comments) &gt; 0 and my_url.find(submission._comments[0].id) &gt;= 0:
        object = submission._comments[0]
    elif my_url.find(submission.id) &gt;= 0:
        object = submission
    else
        &lt;burst into flames&gt;

But this doesn't sound like the right way to do something that I would think should be easy to do: ""Here's a URL -- get the object.""
",,False,,t5_2qizd,1371146692.0,,,True,t3_1g98a4,http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/,
1370582442.0,6,self.redditdev,1fufte,API Function: If upvoted/downvoted?,8,2,1,http://www.reddit.com/r/redditdev/comments/1fufte/api_function_if_upvoteddownvoted/,"I know votes are calculated in a direction of 1,-1 or 0 and that to submit a vote you use api/vote but how can I figure out if the loged-in user has already voted on a post? I feel like it would be obvious but I can find a solution.  ",,False,,t5_2qizd,False,,,True,t3_1fufte,http://www.reddit.com/r/redditdev/comments/1fufte/api_function_if_upvoteddownvoted/,
1370453450.0,5,self.redditdev,1fqelf,Specific subreddit posts in spotlight bar,7,2,1,http://www.reddit.com/r/redditdev/comments/1fqelf/specific_subreddit_posts_in_spotlight_bar/,"Hello guys,

On my clone, I am trying to modify how the spotlight bar behaves. Instead of promoted links and links from user-subscribed reddits, I want to show posts from a **specific subreddit**, which will be used for announcements and such.

I tried modifying the organic.py in r2/lib, this line:

    def organic_links(user):
        sr_ids = Subreddit._by_name(""my_announcement_reddit"")

But this now shows the empty box (I had to get lose of the .sort() and similar things because I have only one, not a list of subreddits). I also tried posting something there, to rule out the ""freshness"" issue.

How can I achieve this?

Thanks!
",,False,,t5_2qizd,False,,,True,t3_1fqelf,http://www.reddit.com/r/redditdev/comments/1fqelf/specific_subreddit_posts_in_spotlight_bar/,
1370287051.0,5,self.redditdev,1fll01,How far back in time does reddit.com/random go?,6,1,3,http://www.reddit.com/r/redditdev/comments/1fll01/how_far_back_in_time_does_redditcomrandom_go/,"http://mail.reddit.com/ticket/632

Reddit.com/random goes to a recent randomly chosen thread. Does anyone know about how it determines recency? For example, is it picking a random thread within the past 24 hours?",,False,,t5_2qizd,False,,,True,t3_1fll01,http://www.reddit.com/r/redditdev/comments/1fll01/how_far_back_in_time_does_redditcomrandom_go/,
1370212144.0,7,self.redditdev,1fjl11,PRAW Exception: Connection refused,8,1,8,http://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/,"Solved
--

It was indeed the proxy. The following praw.ini configuration is working for me on PythonAnywhere:

    [reddit_nossl]
    domain: www.reddit.com
    oauth_domain: oauth.reddit.com
    short_domain: redd.it
    check_for_updates: false
    http_proxy: http://proxy.server:3128

Hi,

I'm trying to get PRAW working over at PythonAnywhere.com and have run up against a wall.

**log**:

    retrieving: http://www.reddit.com/r/opensource/.json                                                                                                                            
    params: {'limit': 5}                                                                                                                                                            
    data: None                                                                                                                                                                      
    Traceback (most recent call last):                                                                                                                                              
      File ""test.py"", line 4, in &lt;module&gt;                                                                                                                                           
        print [str(x) for x in submissions]                                                                                                                                         
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py"", line 438, in get_content                                                                            
        page_data = self.request_json(url, params=params)                                                                                                                           
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/decorators.py"", line 95, in wrapped                                                                               
        return_value = function(reddit_session, *args, **kwargs)                                                                                                                    
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py"", line 473, in request_json                                                                           
        response = self._request(url, params, data)                                                                                                                                 
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py"", line 346, in _request                                                                               
        response = handle_redirect()                                                                                                                                                
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py"", line 317, in handle_redirect                                                                        
        timeout=timeout, **kwargs)                                                                                                                                                  
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py"", line 135, in wrapped                                                                                
        result = function(cls, **kwargs)                                                                                                                                            
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py"", line 54, in wrapped                                                                                 
        return function(cls, **kwargs)                                                                                                                                              
      File ""/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py"", line 90, in request                                                                                 
        return self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False)                                                                                     
      File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 460, in send                                                                                         
        r = adapter.send(request, **kwargs)                                                                                                                                         
      File ""/usr/local/lib/python2.7/site-packages/requests/adapters.py"", line 246, in send                                                                                         
        raise ConnectionError(e)                                                                                                                                                    
    requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /r/opensource/.json?limit=5 (Caused by &lt;class 'socket.error'&gt;: [Errno 111] Connection refused)

**test.py**:

    import praw
    r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl')
    submissions = r.get_subreddit('opensource').get_hot(limit=5)
    print [str(x) for x in submissions]

The reddit_nossl settings are as the name implies, and log_requests: 2, check_for_updates: false

Using wget to access the same url works, [pastie link](http://pastie.org/pastes/7998257/text?key=9gpwt88jid8zww3ykt0yg)

I also tried using just requests.get, which also works. [script](http://pastie.org/pastes/7998286/text?key=49wrr8rknnn4gkignsqflq), [output](http://pastie.org/pastes/7998283/text?key=t1qfmdty0jkkxtq5uj2svg)

An admin at PythonAnywhere has said it's not a proxy issue, which is showing other users accessing reddit. I'm stumped, but that's not saying a whole lot. Is something staring me in the face?

**Edit**:

I added some debugging, and modified the request pool as suggested. The new script looks like:

    #!/usr/bin/python
    import praw
    import requests
    import logging
    import httplib
    httplib.HTTPConnection.debuglevel = 1
    logging.basicConfig()
    logging.getLogger().setLevel(logging.DEBUG)
    requests_log = logging.getLogger(""requests.packages.urllib3"")
    requests_log.setLevel(logging.DEBUG)
    requests_log.propagate = True

    # Works
    print 'Making basic Requests get'
    requests.get('http://www.reddit.com/r/opensource/.json?limit=5')

    # Doesn't work
    print '\n\nImporting PRAW and requesting submissions'
    r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl')
    submissions = r.get_subreddit('opensource').get_hot(limit=5)
    print [str(x) for x in submissions]

I manually edited PRAW's handlers.py to modify the pool, the first 2 lines of the PRAW output shows the settings.

The output is rather long, so I'll [link to it here](http://pastie.org/pastes/7998858/text?key=pzb01ahx79yzmbsp9k7na)

Interestingly, with just requests.get(url), the debugging shows it connecting to proxy.server, but with PRAW it says it is connecting to www.reddit.com. How would I go about setting PRAW to use the proxy?",,False,,t5_2qizd,1370229346.0,,,True,t3_1fjl11,http://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/,
1370017522.0,3,self.redditdev,1fetks,Where is the list of all possible errors you can receive on post submission?,6,3,2,http://www.reddit.com/r/redditdev/comments/1fetks/where_is_the_list_of_all_possible_errors_you_can/,"Basically I am looking for a list that shows the error value (ie. BAD_CAPTCHA, RATELIMIT, QUOTA_FILLED) along with the short description about them that reddit returns, ie. ""you are doing that too much. try again in 3 minutes."" for BAD_CAPTCHA. Looking at the source for api/submit I can only find the capitalized (presumably) enum values. I want the enum values + the description. I poked around the source for a little bit, but couldn't find them. I figured maybe somebody could save me time. Thanks!",,False,,t5_2qizd,False,,,True,t3_1fetks,http://www.reddit.com/r/redditdev/comments/1fetks/where_is_the_list_of_all_possible_errors_you_can/,
1369840675.0,4,self.redditdev,1f9t5s,When is praw.errors.RateLimitExceeded thrown?,6,2,4,http://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/,"When is praw.errors.RateLimitExceeded actually thrown? The API docs say it's 'An exception for when something has happened too frequently', but doesn't PRAW just delay API calls, rather than throwing an exception if the 2 second rule isn't obeyed? Or do I have to write a 2 second delay into my bot?",,False,,t5_2qizd,1369899620.0,,,True,t3_1f9t5s,http://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/,
1369771164.0,5,self.redditdev,1f7x3g,Getting exception after running install script on ubuntu 12.04,5,0,8,http://www.reddit.com/r/redditdev/comments/1f7x3g/getting_exception_after_running_install_script_on/,"After running install script on ubuntu 12.04. The script completed succesfully wihtout errors. But then i`m getting this error when trying to access :

InvariantException: Redefining type 'CommentTreeStorageV2'?

File '/usr/lib/python2.7/dist-packages/weberror/evalexception.py', line 431 in respond
app_iter = self.application(environ, detect_start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 168 in call
return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 265 in call
return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 223 in call
return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 238 in call
return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 343 in call
return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 369 in call
return self.app(environ, custom_start_response)
File '/usr/lib/pymodules/python2.7/routes/middleware.py', line 130 in call
response = self.app(environ, start_response)
File '/usr/lib/pymodules/python2.7/pylons/wsgiapp.py', line 116 in call
self.setup_app_env(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 380 in setup_app_env
self.load_controllers()
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 392 in load_controllers
controllers.load_controllers()
File '/home/reddit/reddit/r2/r2/controllers/__init__.py', line 42 in load_controllers
from listingcontroller import ListingController
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 23 in 
from oauth2 import OAuth2ResourceController, require_oauth2_scope
File '/home/reddit/reddit/r2/r2/controllers/oauth2.py', line 30 in 
from r2.lib.base import abort
File '/home/reddit/reddit/r2/r2/lib/base.py', line 28 in 
from r2.lib.template_helpers import get_domain
File '/home/reddit/reddit/r2/r2/lib/template_helpers.py', line 23 in 
from r2.models import *
File '/home/reddit/reddit/r2/r2/models/__init__.py', line 26 in 
from builder import *
File '/home/reddit/reddit/r2/r2/models/builder.py', line 34 in 
from r2.lib.comment_tree import moderator_messages, sr_conversation, conversation
File '/home/reddit/reddit/r2/r2/lib/comment_tree.py', line 28 in 
from r2.models.comment_tree import CommentTree
File '/home/reddit/reddit/r2/r2/models/comment_tree.py', line 133 in 
class CommentTreeStorageV2(CommentTreeStorageBase):
File '/home/reddit/reddit/r2/r2/lib/db/tdb_cassandra.py', line 133 in init
raise InvariantException(""Redefining type %r?"" % (cls._type_prefix))
InvariantException: Redefining type 'CommentTreeStorageV2'?

I have tried to populate database with test data using populatedb.populate(). And it also completed succesfully, but same error when trying to access it.",,False,,t5_2qizd,False,,,True,t3_1f7x3g,http://www.reddit.com/r/redditdev/comments/1f7x3g/getting_exception_after_running_install_script_on/,
1369699430.0,5,self.redditdev,1f61mp,Is it possible to roll random threads with given constraints?,5,0,3,http://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/,"For some research I'm working on, I was contemplating looking for random threads posted within the previous 7 days. After some tooling around with PRAW, I've hit a bit of a wall. Has anyone managed to pull off grabbing random threads within a series of given constraints? (e.g. someone else may be interested in looking at a random thread within the previous year, or above a given vote threshold.)",,False,,t5_2qizd,False,,,True,t3_1f61mp,http://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/,
1369206109.0,6,self.redditdev,1etkr0,Enabling CORS for non-auth'd POST API endpoints?,6,0,0,http://www.reddit.com/r/redditdev/comments/1etkr0/enabling_cors_for_nonauthd_post_api_endpoints/,"Hey all, I'm getting my feet wet with D3 and would love to make some in-browser visualizations based around real reddit data -- visualizing posting patterns, comment threads, etc.  Here's my problem though -- some of the data is only available behind POST endpoints (for example, [api/morechildren](http://www.reddit.com/dev/api#POST_api_morechildren), and so Same Origin makes this rather difficult to grab in a general shared-blogpost type form factor (ie a browser executing a normal script).  JSONP only works for GET requests. 

[CORS](http://en.wikipedia.org/wiki/Cross-origin_resource_sharing) is designed to address this, but [a request to add it seems to have been ignored](https://github.com/reddit/reddit/issues/432).  In response to spladug's comment:

&gt; If you have a specific example of a portion of the API that you would want to support CORS and that can be done so without allowing third party websites to arbitrarily execute actions as yourself, then please let us know.

My response would be: any API endpoint that is POST and does not require oauth (if it doesn't require authentication, the action probably isn't tied to yourself).  Specifically, api/morechildren.  

Or just make api/morechildren a GET endpoint since it's reeeaaally just a fancy read-only endpoint.  Or am I overcomplicating this and is there an easy way to get my morechildren in a regular browser script? ",,False,,t5_2qizd,False,,,True,t3_1etkr0,http://www.reddit.com/r/redditdev/comments/1etkr0/enabling_cors_for_nonauthd_post_api_endpoints/,
1368402850.0,4,self.redditdev,1e7nim,What is the difference between /hot and /listing?,5,1,0,http://www.reddit.com/r/redditdev/comments/1e7nim/what_is_the_difference_between_hot_and_listing/,"I'm pulling data from reddit and it seems like /hot and /listing are giving me the same post data - http://www.reddit.com/dev/api

Anyone know what the difference is between these two api requests?",,False,,t5_2qizd,1368403165.0,,,True,t3_1e7nim,http://www.reddit.com/r/redditdev/comments/1e7nim/what_is_the_difference_between_hot_and_listing/,
1368126965.0,4,self.redditdev,1e0p94,displaying comment content on the command line with python,6,2,1,http://www.reddit.com/r/redditdev/comments/1e0p94/displaying_comment_content_on_the_command_line/,"Hi redditdev,

i've been toying around with python development and have taken up to writing a command line client that will display output of the title, but the comments always display an Object ID.... is there a way to display comment content (body) in plain text?",,False,,t5_2qizd,False,,,True,t3_1e0p94,http://www.reddit.com/r/redditdev/comments/1e0p94/displaying_comment_content_on_the_command_line/,
1367915187.0,6,self.redditdev,1duo7a,Accessing user likes/dislikes with JSONP?,6,0,11,http://www.reddit.com/r/redditdev/comments/1duo7a/accessing_user_likesdislikes_with_jsonp/,"I'm working on a doohickey for an AI project that scrapes a user's liked/disliked posts and uses the data to select ""preferred"" posts from /r/all.  I'm not super familiar with web scripting and I'm kind of at the ""know enough to break everything"" phase of using jQuery, but I got the scraper mostly working... on publicly accessible pages.

I'm trying to use JSONP to grab likes and dislikes and it's throwing a 403 error, even on the account I'm logged into (I can grab the standard JSON version just fine from my browser, but obviously not from the script due to origin control).  Is there a way to log into reddit from a remote script, or at least to convince reddit that I'm already logged in?  The API documentation doesn't seem to indicate that oauth is usable for this.

For reference, here's the toy implementation I'm working with, which works on public pages:

    &lt;html&gt;
    
    &lt;head&gt;
    &lt;title&gt;Reddit Personalizer v0.01&lt;/title&gt;
    &lt;/head&gt;
    
    &lt;body&gt;
    
    &lt;div id = ""placeholder""&gt;empty&lt;/div&gt;
    
    &lt;script src = ""StringFunctions.js""&gt;&lt;/script&gt;
    &lt;script src = ""RedditFunctions.js""&gt;&lt;/script&gt;
    &lt;script src = ""http://code.jquery.com/jquery-latest.js""&gt;&lt;/script&gt;
    &lt;script&gt;
    	$.getJSON('http://www.reddit.com/user/jbradfield/submitted.json?jsonp=?', function(data) {
    		var output = ""&lt;ul&gt;"";
    		for (var i in data.data.children) {
    			console.log(data.data.children[i].data);
    			var post = data.data.children[i].data;
    			output += ""&lt;li&gt;"" + post.score + "" - "" + ""&lt;a href=\""http://www.reddit.com"" + post.permalink + ""\""&gt;"" + post.title + ""&lt;/a&gt;&lt;/li&gt;"";
    		}
    		output += ""&lt;/ul&gt;"";
    		document.getElementById(""placeholder"").innerHTML = output;
    	});
    &lt;/script&gt;

    &lt;/body&gt;
    &lt;/html&gt;

If I try to access http://www.reddit.com/user/jbradfield/liked.json?jsonp=? the whole thing breaks.

Sorry if this a really daft question, I really have next to no clue what I'm doing when it comes to web services and whatnot.",,False,,t5_2qizd,False,,,True,t3_1duo7a,http://www.reddit.com/r/redditdev/comments/1duo7a/accessing_user_likesdislikes_with_jsonp/,
1367807310.0,4,self.redditdev,1dro2b,Problems with the PRAW get_content() method,5,1,2,http://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/,"I'm pretty sure this is a newbie question; I've just started using the API today. I want to be able to retrieve a comment given a url for the comment without searching through the submission for a matching ID. I tried using the get_content method with the full URL of the target comment, but when I try to use the generator it creates, the API throws an error:

    &gt;&gt;&gt; gen = r.get_content('http://www.reddit.com/r/pics/comments/1dplco/took_a_ride_and_suddenly_found_myself_in_a/c9sqtmx')
    &gt;&gt;&gt; data = gen.next()
    
    Traceback (most recent call last):
      File ""&lt;pyshell#84&gt;"", line 1, in &lt;module&gt;
        data = gen.next()
      File ""C:\Python27\lib\site-packages\praw-2.1.1-py2.7.egg\praw\__init__.py"", line 438, in get_content
        root = page_data[root_field]
    TypeError: list indices must be integers, not str

As I have my program set up now, I take the submission ID from the url, and then get the submission using praw. I then proceed to search through all if the comments if the submission, checking each to see if their ID matches the one in the url.

This usually doesn't take too long, but if the comment I'm looking for is in one of the 'more comments' areas, then it takes forever to open all of the sections to look for it.

I know there must be some way to access a comment in that way, but I must not be doing it right.",,False,,t5_2qizd,False,,,True,t3_1dro2b,http://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/,
1367766233.0,6,self.redditdev,1dqeri,Problems with /api/submit.json and Captcha,6,0,2,http://www.reddit.com/r/redditdev/comments/1dqeri/problems_with_apisubmitjson_and_captcha/,"So I have been trying to learn the Reddit API with python. The final goal of the project is to post encrypted messages to a subreddit. I got the idea from http://www.reddit.com/r/A858DE45F56D9BC9. I know it isn't very practical but I wanted to play with the Reddit API and the crypto libraries in python. The problem is that I don't have enough karma, apparently, to make posts without captchas so I decided to just write a way to solve the captchas into the program. My first submit seems to go fine. I get the captcha iden, get the captcha, and respond with the iden and the captcha letters in the parameters. I always get an error 500 from Reddit when I post with the captcha data.

There is a copy of my code below. I can't for the life of me figure out what I am doing wrong. If anybody could point me in the right direction it would be amazing. I haven't been using a wrapper for the API because I want to actually learn the API not just use it. I have also googled as much as I can and looked at various wrapper source code and even the Reddit source code and can't see what I am doing wrong.

https://gist.github.com/lospheris/5520957

Also, some of the stuff in the class is for using it with a GUI. For the most part I play with it from the python shell but I have a quickly thrown together GUI that I played with too so that is why there are a few things that aren't actually required for talking to reddit.",,False,,t5_2qizd,False,,,True,t3_1dqeri,http://www.reddit.com/r/redditdev/comments/1dqeri/problems_with_apisubmitjson_and_captcha/,
1367691633.0,7,self.redditdev,1doto3,Reddit bot reading inbox,7,0,8,http://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/,"What would be the best method for allowing a bot/program to keep track of read and unread private messages?

I was planning on having my program mark unread messages in the inbox as 'read' after it finishes processing the message, but it seems like there are flaws in this plan. 
For example, if I accidentally log on to the reddit account associated with the bot, I might accidentally open up a message and it wouldn't be processed by the bot.

The PRAW tutorials have a method in which processed submissions are kept track of using the submission ID.

Would this be the best method for handling messages?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1doto3,http://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/,
1367538533.0,5,self.redditdev,1dl3l9,What is flair_template_id and how do I properly get to it?,7,2,2,http://www.reddit.com/r/redditdev/comments/1dl3l9/what_is_flair_template_id_and_how_do_i_properly/,"The API has [`api/selectflair`](http://www.reddit.com/dev/api#POST_api_selectflair) for setting a user's flair. It has a required parameter `flair_template_id` but I can't find any useful information about it. I see it's part of the flair selection form, but this is the only place it's provided, embedded in this HTML form. A form that is served by an unlisted API point,`/api/flairselector`. This API doesn't serve anything when the flair isn't editable by the requesting user, making these values effectively private.

Is there any reasonable way to access this id? Why can't I refer to flair by its CSS name, like the one flairlist serves? It seems the flair API is partly broken right now because of this strange parameter.
",,False,,t5_2qizd,False,,,True,t3_1dl3l9,http://www.reddit.com/r/redditdev/comments/1dl3l9/what_is_flair_template_id_and_how_do_i_properly/,
1366925817.0,6,self.redditdev,1d4018,HTTP 429 With GAE,7,1,9,http://www.reddit.com/r/redditdev/comments/1d4018/http_429_with_gae/,"Hey guys, I know this comes up a bit but I can't find the info I'm looking for. Basically I'm running the local development server of Google App Engine and I'm testing a cron job to pull down /top posts of a subreddit.


I've created a unique User-Agent string, including my username, I'm sleeping for 3 seconds between each request to make sure, and yet Reddit is still replying with a 429. Sometimes a 429 with no content, sometimes a 429 with the top submissions! But only once every 20 or so tries, and only 1 out of the 3 subreddits I'm trying to pull.


Using Chrome still works with the same URL's, ditto with urllib with custom User-Agent from the interpreter. What's going wrong? Is it the local dev server using a proxy or something that's being limited by Reddit? Thanks for any help.",,False,,t5_2qizd,False,,,True,t3_1d4018,http://www.reddit.com/r/redditdev/comments/1d4018/http_429_with_gae/,
1366774828.0,5,self.redditdev,1czjti,How to grab parent of message/comment,5,0,3,http://www.reddit.com/r/redditdev/comments/1czjti/how_to_grab_parent_of_messagecomment/,"I have a list of the user's messages, much of which are comment replies. When the user clicks one of these messages, I would like to show the message itself, as well as its parent. So the user can see what the message was a response to. 
In the json data for the message, there is a 'first_message' and 'first_message_name' properties, however they always seem to be undefined.  And is the 'parent_id' property referring to the post on which a comment was made or the comment's parent? 
And finally, how can I retrieve the data for a comment's parent, what api call? 
Thanks!",,False,,t5_2qizd,False,,,True,t3_1czjti,http://www.reddit.com/r/redditdev/comments/1czjti/how_to_grab_parent_of_messagecomment/,
1366613200.0,7,self.redditdev,1cumng,PRAW + Voting &amp; Removing,7,0,12,http://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/," Every time i try to upvote or downvote i get this 
  
  
      user='name'
      passwd='mypassword'
      reddit.login(user,passwd) 
       m=reddit.get_submission(submission_id='1crjy2')
       m.upvote()  
  
   
##OUTPUT  
  
  
            Traceback (most recent call last):
             File ""C:\Python27\Scripts\testB0T2.py"", line 61, in &lt;module&gt;
             reddit.get_submission(submission_id='19zce4').upvote()
             File ""C:\Python27\lib\site-packages\praw\objects.py"", line 447, in upvote
             return self.vote(direction=1)
              File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 341, in wrapped
              return function(cls, *args, **kwargs)
              File ""C:\Python27\lib\site-packages\praw\objects.py"", line 464, in vote
             return self.reddit_session.request_json(url, data=data)
             File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 239, in          error_checked_function
              raise error_list[0]
               NotLoggedIn: `please login to do that` on field `None` 
   
  
 And when I try to remove something that I am the author of with this code  
  
        user='name'
      passwd='mypassword'
      reddit.login(user,passwd) 
       m=reddit.get_submission(submission_id='1crjy2')
       m.remove()  
  
   
##OUTPUT
        Traceback (most recent call last):
         File ""C:\Python27\Scripts\sheboon\testB0T2.py"", line 73, in &lt;module&gt;
        m.remove()
       File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 331, in wrapped
       if mod_req and not is_mod_of_all(obj.user, subreddit):
      File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 288, in is_mod_of_all
      mod_subs = user.get_cached_moderated_reddits()
      File ""C:\Python27\lib\site-packages\praw\objects.py"", line 688, in        get_cached_moderated_reddits
    for sub in self.reddit_session.get_my_moderation(limit=None):
    File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 372, in get_content
    page_data = self.request_json(url, params=params)
    File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 223, in      error_checked_function
       return_value = function(cls, *args, **kwargs)
      File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 407, in request_json
      response = self._request(url, params, data)
     File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 294, in _request
      timeout=timeout)
     File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 64, in __call__
    result = self.function(reddit_session, url, *args, **kwargs)
     File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 167, in __call__
    return self.function(*args, **kwargs)
     File ""C:\Python27\lib\site-packages\praw\helpers.py"", line 154, in _request
    .format(prev_url, url))
     ClientException: Unexpected redirect from http://www.reddit.com/reddits  /mine/moderator/.json to http://www.reddit.com/subreddits/login.json?dest=%2Freddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024  
  
    ",,False,,t5_2qizd,1366613711.0,,,True,t3_1cumng,http://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/,
1365995742.0,4,self.redditdev,1cd807,How can we see if user has voted on a post?,8,4,2,http://www.reddit.com/r/redditdev/comments/1cd807/how_can_we_see_if_user_has_voted_on_a_post/,"It appears that we can somehow check if user has voted on a post (up or down or neither). 
When I populate my list of items from a subreddit, I would like to indicate to the user whether they have voted on each post. I see nothing in the json data returned with the list of posts that would indicate user votes. 
How can I check this?",,False,,t5_2qizd,False,,,True,t3_1cd807,http://www.reddit.com/r/redditdev/comments/1cd807/how_can_we_see_if_user_has_voted_on_a_post/,
1365695338.0,4,self.redditdev,1c52gt,Submit reply to users most recent comment.,6,2,15,http://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/,"hey guys, I'm playing about with praw and am a total python nubbin. 

I've got it to login, make posts and read page titles and self post text but for what I'm planing I'm going to need to find a way of replying to a users last comment and reading the bot's inbox. 


any help you guys would could offer would be muchly appreciated.
",,False,,t5_2qizd,False,,,True,t3_1c52gt,http://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/,
1364290222.0,5,self.redditdev,1b172z,gen_time_listings.sh explanation,7,2,2,http://www.reddit.com/r/redditdev/comments/1b172z/gen_time_listingssh_explanation/,"Hey guys, 
I noticed that in order to have the top listings, we have to run the mr_top thing. I found a script in scripts, which appears to deals with this.However, there are quite a few parameters in there which I am not able to understand.

    LINKDBHOST=""$1""
    # e.g. 'year'
    INTERVAL=""$2""
    # e.g. '(""hour"",""day"",""week"",""month"",""year"")'
    LISTINGS=""$3""

I am using a stack install script from github, what to put instead of the linkhost? I tried ""localhost"" for linkdbhost, and ""day"" or ""week"" for inverval and listings, like:
    ./gen_time_listings.sh ""localhost"" ""day"" ""day""

And I get this kind of output: http://pastebin.com/zQTwxGZ0

So, what to put for those variables?

Thanks!",,False,,t5_2qizd,1364374650.0,,,True,t3_1b172z,http://www.reddit.com/r/redditdev/comments/1b172z/gen_time_listingssh_explanation/,
1363520211.0,5,self.redditdev,1agkmm,PRAW and Oauth,5,0,2,http://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/,"Hi, 

Im trying to make an app just for learning purposes. I want to get the list of subreddits that a user is subscribed to. 

Of course i can use the `.login` method within PRAW but i felt that using oauth would give the user better confidence.

I managed to get this going and you can find it over [here](https://github.com/shrayas/slashsub) on my Github. But my question is this: 

Why does one have to authorize the app every time he tries to login with reddit? isn't that a one time thing? If he's already logged in to Reddit it should automatically redirect back to the callback URL is what i feel. 

Maybe i'm doing something wrong.. In a gist here's what im doing

    #get object
    r = praw.Reddit(...)

    # set oauth info
    r.set_oauth_app_info(...)

    # get URL
    r.get_authorize_url(...)

    # Open a web browser and go to URL
    # Here is where the authorization page is shown everytime
    
    #on callback
    accessInfo = r.get_access_details(...)
    r.set_access_credentials(...)

    #do whatever i want
    r.get_me() #for instance

Any help would be appreciate. Please excuse the n00bness in the question (if any) ",,False,,t5_2qizd,False,,,True,t3_1agkmm,http://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/,
1363069000.0,6,self.redditdev,1a4ut5,make fails with latest reddit source,7,1,2,http://www.reddit.com/r/redditdev/comments/1a4ut5/make_fails_with_latest_reddit_source/,"[solved] Change lessc's shebang to nodejs
------------------------------------------------------------------------------------------------------------------------------------
I believe this was posted before, but I still couldn't get it to work. Using the latest code and following the exact instructions, I get these error while trying to build

    [+] including definitions from Makefile.py
    # see above
    rm -f build/public/static/sprite-reddit.png build/public/static/reddit.css
    python r2/lib/nymph.py build/public/static/css/reddit.less
    build/public/static/sprite-reddit.png &gt; build/public/static/reddit.less.tmp    
    r2/lib/contrib/less.js/bin/lessc build/public/static/reddit.less.tmp &gt; build/public/static/reddit.css.tmp
    make: *** [build/public/static/reddit.css] Error 1

In [this](http://www.reddit.com/r/redditdev/comments/19ntpk/make_fails_buildpublicstaticredditcss_error_127/) post, the user didn't have nodejs installed, but I do. I have tried everything I could think of, but nothing works :( 

please help!",,False,,t5_2qizd,1363139748.0,,,True,t3_1a4ut5,http://www.reddit.com/r/redditdev/comments/1a4ut5/make_fails_with_latest_reddit_source/,
1362685367.0,6,self.redditdev,19v2pn,"Complete noob question: how do I see the individual posts from the generator that I get back from r.get_front_page? It must return the posts as some kind of object with a bunch of information inside it, but I don't know how to get to that information!",6,0,2,http://www.reddit.com/r/redditdev/comments/19v2pn/complete_noob_question_how_do_i_see_the/,"I want to see the individual posts that I get back from it (I see that I can do that with [str(x) for x in submissions], but I also want the URL from all of the submissions in the collection, and I want to take all the information from those and put it in a database.

Sorry for the noob question. I just found out this existed today, I have little to no Python experience, and I'm just doing a total crash course on it to learn.",,False,,t5_2qizd,False,,,True,t3_19v2pn,http://www.reddit.com/r/redditdev/comments/19v2pn/complete_noob_question_how_do_i_see_the/,
1362116589.0,4,self.redditdev,19g4yq,Is it possible to run PRAW on the Google App Engine?,5,1,10,http://www.reddit.com/r/redditdev/comments/19g4yq/is_it_possible_to_run_praw_on_the_google_app/,"I tried setting it up and I had some issues, so I'm just wondering if it is even possible. Also, if you know of any tutorials that'd be wonderful (I couldn't find anything).",,False,,t5_2qizd,False,,,True,t3_19g4yq,http://www.reddit.com/r/redditdev/comments/19g4yq/is_it_possible_to_run_praw_on_the_google_app/,
1361818048.0,5,self.redditdev,197erw,Praw Moderator Bot &amp; Remove Method,5,0,5,http://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/,"I cannot seem to get a bot to remove or delete a submission even when both the submission and the subreddit being posted to belongs to the account im using. Consider  
  
      reddit = praw.Reddit(user_agent='BotScript 1.0, by u/LamerX')
      print ""Logging in...""
    
      user='LamerX'
      passwd='password'
      reddit.login(user,passwd)
      print ""Logged In""    
      reddit.get_submission(submission_id=id of submission i own)  
      submission.remove()   
  
This come ALWAYS draws the following error

     NotLoggedIn: `please login to do that` on field `None` 
  

      ",,False,,t5_2qizd,False,,,True,t3_197erw,http://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/,
1361757012.0,4,self.redditdev,195yz7,How exactly does the page limit work on subreddits?,5,1,6,http://www.reddit.com/r/redditdev/comments/195yz7/how_exactly_does_the_page_limit_work_on_subreddits/,"I'm working on visualizing post topics over time in a subreddit, and I'm using ReddiWrap to do the heavy API lifting.  Unfortunately, I can only seem to get to about 940 posts per subreddit, and some of the old data looks wierd.  [This is a graph](http://imgur.com/vT2KxRD) of /r/uiuc for post activity as percent of daily activity.

As you can see, most of the data before last month isn't very useful.  Is there a way to get all of those old posts?  How exactly does the ""post phase-out"" algorithm work?",,False,,t5_2qizd,False,,,True,t3_195yz7,http://www.reddit.com/r/redditdev/comments/195yz7/how_exactly_does_the_page_limit_work_on_subreddits/,
1361508544.0,6,self.redditdev,1905ed,How to reuse OAuth Tokens with PRAW,7,1,8,http://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/,"I followed the example for how to use OAuth with PRAW to gain access to a user's account that's [here](https://github.com/praw-dev/praw/wiki/OAuth).   However, I'm wondering what is the right way to reuse that access once I've gotten the tokens from Reddit.   

I created a 'get_user_data()' function to reuse the token, and within that I perform an r.refresh_access_information(refresh_token).   Unfortunately, this doesn't seem to immediately update the user information for the user that the refresh token is for.   Instead, it seems to take 2 calls to get_user_data() in order to get the current information for the user I'm interested in.

If r.refresh_access_information() is the correct function to use, how to I force PRAW to block long enough to get the information for the user that the refresh token is for, rather than returning the information for the last user that get_user_data() was called for?

[My code is available here.](http://pastebin.com/tib9Hm5z)

As you can probably tell from looking at it, the idea is to be able to have multiple widgets use the same server to request data from Reddit.   However, I'm pretty new to all of this, so I may be going about it all wrong...",,False,,t5_2qizd,1361514543.0,,,True,t3_1905ed,http://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/,
1361423940.0,6,self.redditdev,18xrsm,"Enabling wiki, and some problems",6,0,6,http://www.reddit.com/r/redditdev/comments/18xrsm/enabling_wiki_and_some_problems/,"In the .ini file, the disable_wiki = false, but whenever I go to reddit.local/wiki it says that the wiki is disabled and Forbidden to enter

Also ive set the disable_require_admin_otp = false. When i turn admin on, it only asks for password, but whenever i click submit, it redirects me to reddit.local/prefs/otp and asks me to setup a two factor auth, which i dont want.

appologies for the many questions, the documentation is just so lacking for these kinds of problems. Thanks!",,False,,t5_2qizd,False,,,True,t3_18xrsm,http://www.reddit.com/r/redditdev/comments/18xrsm/enabling_wiki_and_some_problems/,
1360818959.0,7,self.redditdev,18hwx4,Banning With Praw,7,0,2,http://www.reddit.com/r/redditdev/comments/18hwx4/banning_with_praw/,"Anyone have any idea how this is accomplished?   
Im pretty sure its an update_setting() but no idea about the keywords used. Anyone know?",,False,,t5_2qizd,False,,,True,t3_18hwx4,http://www.reddit.com/r/redditdev/comments/18hwx4/banning_with_praw/,
1360175549.0,5,self.redditdev,180g0b,"Reddit install now prepending https to all my links, how do I turn if off?",6,1,0,http://www.reddit.com/r/redditdev/comments/180g0b/reddit_install_now_prepending_https_to_all_my/,"-subj! spent the last 30 mins looking at the code, still can't figure it out!",,False,,t5_2qizd,False,,,True,t3_180g0b,http://www.reddit.com/r/redditdev/comments/180g0b/reddit_install_now_prepending_https_to_all_my/,
1359933896.0,3,self.redditdev,17u052,Is there a way to ban users from a subreddit via the API?,7,4,3,http://www.reddit.com/r/redditdev/comments/17u052/is_there_a_way_to_ban_users_from_a_subreddit_via/,"Sorry if this a basic question, I couldn't find the answer anywhere.",,False,,t5_2qizd,False,,,True,t3_17u052,http://www.reddit.com/r/redditdev/comments/17u052/is_there_a_way_to_ban_users_from_a_subreddit_via/,
1359765151.0,5,self.redditdev,17q7b3,404 Error during account scraping,5,0,13,http://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/,"Hello!

I'm using PRAW to get some of the links/comments from a list of users' history and their data, but after only two are three accounts I'll be returned a 404 error. I check if a user account still exists before pulling the data, so I know it's not that the account was deleted.

The 404 error doesn't appear to be connected to a problem in my code: it fails at random times while running the script. Is it possible that I'm getting kicked from my connection to the server?

I'm using get_comments() and get_submitted() to pull the information. ",,False,,t5_2qizd,False,,,True,t3_17q7b3,http://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/,
1359672734.0,6,self.redditdev,17npd0,Give Gold Using Reddit Api?,6,0,6,http://www.reddit.com/r/redditdev/comments/17npd0/give_gold_using_reddit_api/,"I couldn't find anything to do with Giving Gold in the api documentation, is this feature going to be implemented?",,False,,t5_2qizd,False,,,True,t3_17npd0,http://www.reddit.com/r/redditdev/comments/17npd0/give_gold_using_reddit_api/,
1359130013.0,4,self.redditdev,179di4,Trying to get my comments and likes,5,1,6,http://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/,"    import praw
    
    r = praw.Reddit(user_agent='just to dl my comments and likes')
    r.login('jokoon', 'xxxxxx')
    f = open(""comments.txt"",'w')
    
    r.set_oauth_app_info(client_id='xxxxxxxx-xx', client_secret='xxxxxxxxxxxxxxxxxx',
                         redirect_url='http://127.0.0.1:65010/authorize_callback')
    
    url = r.get_authorize_url('xxxxxxxx', 'xxxxxx-xx', True)
    import webbrowser
    webbrowser.open(url) 
    
    # user = r.get_redditor('jokoon')
    # f.write(r.get_me().get_liked())

I get the error

    set_oauth_app_info() got an expected keyword argument 'redirect_url'",,False,,t5_2qizd,False,,,True,t3_179di4,http://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/,
1359012332.0,5,self.redditdev,176gsq,Is there a way to automatically assign link flair using a URL argument?,6,1,5,http://www.reddit.com/r/redditdev/comments/176gsq/is_there_a_way_to_automatically_assign_link_flair/,"I was wanting to know if there is any way I can automatically assign link flair to a post using a URL argument?

(e.g. r/subreddit_name/submit?sidebar&amp;title=Post+Title**&amp;linkflair=link-flair-css-class**)

Where linkflair is the URL variable, and link-flair-css-class is the argument for it - which is the css class for the link flair.",,False,,t5_2qizd,False,,,True,t3_176gsq,http://www.reddit.com/r/redditdev/comments/176gsq/is_there_a_way_to_automatically_assign_link_flair/,
1358864034.0,6,self.redditdev,171zm9,Grab next submissions [PRAW],8,2,5,http://www.reddit.com/r/redditdev/comments/171zm9/grab_next_submissions_praw/,"started playing around with praw/reddit api. I'm able to login, grab submissions and comments. But was wondering how to move onto the 'next' 100 submissions. 

Currently what I have (going from memory/psudo) is:

    r = praw.Reddit(useragent=""my user agent"")
    r.login()
    already_done[]
    while 1:
        submissions = r.get_subreddit('python').get_new_by_date(limit=100)
        for submission in submissions:
            if submission.id not in already_done:
                print submission.(title/body/author/etc)
                for comments in submission.comments:
                    (get comment info I want)
    


I've tried throwing in 'after' in the get new by date. But, it doesn't like that.  What would be the proper syntax to accomplish this?  I am able to grab the full title (T#_) value, but not sure what to do with it.

Thanks!",,False,,t5_2qizd,False,,,True,t3_171zm9,http://www.reddit.com/r/redditdev/comments/171zm9/grab_next_submissions_praw/,
1358782481.0,5,self.redditdev,16zplu,Get Access Token 401,6,1,4,http://www.reddit.com/r/redditdev/comments/16zplu/get_access_token_401/,"Hi,

I have trouble in getting access token. I use HTTP basic authentication on the request. I set the client_id as the username and the client_secret as the password. The code likes follows:

    $.ajax({ 
        url: 'https://ssl.reddit.com/api/v1/access_token', 
        type: 'POST', 
        dataType: 'json', 
        data: { 
            code: _this.authorization_code, 
            client_id: _clientID, 
            client_secret: _clientSecret, 
            redirect_uri: _redirectURI, grant_type: 'authorization_code' }, 
            error: function(xhr) { 
                console.log(""access token request failed""); 
            }, 
            success: function(resp) { 
                console.log(""success""); 
            }, 
            beforeSend: function(xhr) { 
                console.log(""CALLED BEFORE SEND""); 
                xhr.setRequestHeader(""Authorization"", ""Basic "" + btoa(""username:password"")); // I used my client_id and client_secret 
            } 
        });

By the way, I tried to open https://ssl.reddit.com/api/v1/access_token in the browser. It poped up a window which needed to filled username and password with it. I typed and after clicking the log in button, I got 404 error. 

Would someone please help me? Could that be my client_id or client_secret is wrong? 

Thanks!",,False,,t5_2qizd,1358783025.0,,,True,t3_16zplu,http://www.reddit.com/r/redditdev/comments/16zplu/get_access_token_401/,
1358287287.0,5,self.redditdev,16n6i9,Reddit login comment system for Wordpress?,8,3,4,http://www.reddit.com/r/redditdev/comments/16n6i9/reddit_login_comment_system_for_wordpress/,"I would like a plugin that allows reddit.com users to log in comment and add content to a wordpress build by automatically making them a contributing member.
Also I would of course want comment voting and it would be great if whatever comments they posted wound up on reddit as well as posts..
Anything even close to my wishlist?",,False,,t5_2qizd,False,,,True,t3_16n6i9,http://www.reddit.com/r/redditdev/comments/16n6i9/reddit_login_comment_system_for_wordpress/,
1357512160.0,5,self.redditdev,1630jj,PRAW is returning post.author.name errors,8,3,7,http://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/,"Hi, redditdev!

I'm having some trouble with my code. I'm collecting a large amount of data for a survey experiment and I'm new to Python as well as Stata, so I'm just writing all the data out to .txt files in CSV. Usually my code works fine, but I've tried running it recently and keep encountering an error related to post author's names. The error is: ""'None-type' object has no attribute 'name'"". I've looked at the posts which it corresponds to and they seem like normal posts.

I'm at a loss for what is wrong, can anybody help? I've attached my code for posterity, although I don't think it's needed for this fix.


    from datetime import datetime
    import praw
    import time

    r = praw.Reddit(user_agent='your unique user agent')



    def datascraper(counter, subreddit):
        #Open up the files to which the data will be written

        filename = 'dataset_' + subreddit + '.txt'
        filename_titles = 'dataset_' + subreddit + 'titles.txt'
        namelist = []
        txt = open(str(filename), 'a')
        title = open(str(filename_titles), 'a')





        # If this is the first time that this document is being
        # opened, we need to insert the headings first (for CSV format)

        if counter == 0:
            txt.write(""rank , subreddit , num_comments , net_score , is_self, over_18 , downs , url , is_imgur , author , time_created , time_current , ups \n"")
            title.write('rank, title \n')
            counter = 1




        for post in r.get_subreddit(subreddit).get_hot(limit=500, url_data={'limit': 100}):

        # write to CSV/titles_subreddit files
        # data listed as needed is in order:

            txt.write(str(counter) + ' , ')
            txt.write(post.subreddit.display_name + ' , ')
            txt.write(str(post.num_comments) + ' , ')
            txt.write(str(post.score) + ' , ')
            txt.write(str(post.is_self) + ' , ')
            txt.write(str(post.over_18) + ' , ')
            txt.write(str(post.downs) + ' , ')
            txt.write(post.url + ' , ')
            txt.write(str(post.domain.endswith('imgur.com')) + ' , ')


            name = post.author.name    **This is where the problem is**
            namelist.append(name)


            txt.write(name + ' , ')
            txt.write(str(post.created_utc) + ' , ')
            txt.write(str(int(time.time())) + ' , ')
            txt.write(str(post.ups) + '\n')

            post_title = post.title.encode('ascii', 'ignore')
            title.write(post_title + '\n')

            counter += 1



        txt.close()
        title.close()

        return counter, namelist



Thanks for your input!",,False,,t5_2qizd,False,,,True,t3_1630jj,http://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/,
1357225935.0,5,self.redditdev,15w2cs,Help with submitting with Praw,5,0,3,http://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/,"I'm completely new to `praw`, so please excuse my ignorance.

I want to write a simple bot that reads from an RSS feed and submits posts based on that feed.  I already have permission from the RSS owner to post to their subreddit this way.

Is there a quick example for this somewhere?  I just need to submit a link, no self posts.

Thanks!",,False,,t5_2qizd,False,,,True,t3_15w2cs,http://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/,
1356734188.0,7,self.redditdev,15lfje,How to set a link's flair.,8,1,8,http://www.reddit.com/r/redditdev/comments/15lfje/how_to_set_a_links_flair/,"Hey! I am tearing my hair out at the lack of documentation for doing this, and the part where I believe does it is not clear.

I have attempted to use /api/flair to set a link's flair but it's not descriptive on what inputs it needs for changing a link's flair and not a users.

I presume link is what link you want to add the flair to, but I don't know if it is a thing or a URL, if it is a thing is it prefixed with t3?

Trying this out on my own I have only been getting 403 errors while trying this and I really can't work it out by myself so any help would be appreciated!",,False,,t5_2qizd,False,,,True,t3_15lfje,http://www.reddit.com/r/redditdev/comments/15lfje/how_to_set_a_links_flair/,
1356048140.0,3,self.redditdev,157391,"My API posted comments aren't incrementing the ""comments"" count underneath the link.",5,2,3,http://www.reddit.com/r/redditdev/comments/157391/my_api_posted_comments_arent_incrementing_the/,"I've noticed that my comments aren't incrementing the comments count if they're posted through the API.  If I post comments through the web UI with the same account then the comment counter works correctly.  Even though the counter isn't incrementing, the comment seems to be visible.  Also, if I delete the comment, the comment counter reads -1!

You can see what I mean here.  Both link and comment were created by a script using the Ruby Snoo API wrappper:

http://www.reddit.com/r/illjustleavethishere/comments/1571ij/ahem_ill_just_ill_just_uh_leave_this_here_excuse/",,False,,t5_2qizd,False,,,True,t3_157391,http://www.reddit.com/r/redditdev/comments/157391/my_api_posted_comments_arent_incrementing_the/,
1356029821.0,6,self.redditdev,156i5k,Stuck with Reddit Oauth and Python,6,0,3,http://www.reddit.com/r/redditdev/comments/156i5k/stuck_with_reddit_oauth_and_python/,"Driving me nuts as the ""example"" has runtime errors (key error) and seems like they are using flask. . .

Here is where I am stuck: [Pastie](http://pastie.org/5558422)

This is the redirect script that Reddit forwards me to after confirming I want to allow access. This gets me ""{u'error': u'invalid_grant'}""
However, I am getting a code and state passed into my script. . . Ideas of where I have gone astray?


[Update](http://pastie.org/5581551) on code to make cleaner but still at the same pkace
",,False,,t5_2qizd,1356555961.0,,,True,t3_156i5k,http://www.reddit.com/r/redditdev/comments/156i5k/stuck_with_reddit_oauth_and_python/,
1355850645.0,4,self.redditdev,1524k3,api/morechildren issues,7,3,3,http://www.reddit.com/r/redditdev/comments/1524k3/apimorechildren_issues/,"Does anyone have any experience with this API call? I am not using a wrapper because I am trying to learn as much as possible. I have built successful bots with Reddit API before, but I can not get this API call to return anything but 404.

here is my Python code using urllib library:

    children = 'c7i4fuw','c7i4ldu'
    sort = 'top'
    url='http://www.reddit.com/api/morechildren'
    data = urllib.parse.urlencode({'link_id':'t3_1509ki','children':children})
    data = data.encode('utf-8')
    request = urllib.request.Request(url,data,USER_AGENT)
    response = request_json(request)
    pprint(response)


Yes, it's not efficient but I am in full debug mode at this point. I can't band-aid this enough to work. Any suggestions are appreciated!

",,False,,t5_2qizd,False,,,True,t3_1524k3,http://www.reddit.com/r/redditdev/comments/1524k3/apimorechildren_issues/,
1354937609.0,3,self.redditdev,14hcfu,"Sorry newb question here, can someone help me understand what the oauth tag means in the API documentation?",6,3,1,http://www.reddit.com/r/redditdev/comments/14hcfu/sorry_newb_question_here_can_someone_help_me/,"I'm currently in the planning stage for a nodejs reddit API wrapper, and i'm not clear on what the oauth tag means in the API docs.  I intend for the wrapper to be as comprehensive as possible, mainly because i'm using this project to further my knowledge of node.  If someone could provide a bit of insight into what the purpose of ""oauth"" is on reddit I would appreciate it.  ",,False,,t5_2qizd,False,,,True,t3_14hcfu,http://www.reddit.com/r/redditdev/comments/14hcfu/sorry_newb_question_here_can_someone_help_me/,
1351885021.0,4,self.redditdev,12j2c6,Problem with getting json from subreddits beyond the first page.,5,1,10,http://www.reddit.com/r/redditdev/comments/12j2c6/problem_with_getting_json_from_subreddits_beyond/,"I have been able to get json from the first page of a subreddit using a url like: http://www.reddit.com/r/redditdev/.json  but when I try to use /.json at the end of a url for a page beside the front: http://www.reddit.com/r/redditdev/?count=25&amp;after=t3_10omtd/.json
The information is not in json form.  Does anyone know how I can get json from pages other than the front?",,False,,t5_2qizd,False,,,True,t3_12j2c6,http://www.reddit.com/r/redditdev/comments/12j2c6/problem_with_getting_json_from_subreddits_beyond/,
1350876055.0,5,self.redditdev,11vm02,Who's updating the wiki/ documentation? Where's the support for newbies getting into devving?,6,1,2,http://www.reddit.com/r/redditdev/comments/11vm02/whos_updating_the_wiki_documentation_wheres_the/,"Hi newbie here,
I have a problem trying to understand this whole development process and I might be a lonebug when I say this but I really wish there was more support for people trying to do the same. 
It might seem silly to most of you but I finally got a reddit serv up and going on my linux machine and it only took a day or two of figuring out how everything works. Great, still, I wish I wasn't spending all my time, trying to figure out the nuts and bolts of what everything does and honestly getting to manipulating parts of code. Oh and I still can't access what i've installed. Bummer. 
On the way of firguring everything out I have found that the subreddit doesnt have many questions and answers, the documentation on github, while fascinating is quite mindboggingly cryptic and not sure how that would help me and people on the irc chat are stonewall deadsilent. 

Where am i supposed to go, how am i supposed to learn, or am i mistaken ? Granted this is my first attempt at open source so I dont even know if my nagging is warranted and my expectations too high. 

Mainly I wanted to ask, is there any support going on, or are the documents even going to be updated and expanded? 
",,False,,t5_2qizd,False,,,True,t3_11vm02,http://www.reddit.com/r/redditdev/comments/11vm02/whos_updating_the_wiki_documentation_wheres_the/,
1350033781.0,6,self.redditdev,11cxd3,Is there a subreddit where you can hire reddit devs?,6,0,4,http://www.reddit.com/r/redditdev/comments/11cxd3/is_there_a_subreddit_where_you_can_hire_reddit/,"Or if anyone's interested, please send me a PM.",,False,,t5_2qizd,False,,,True,t3_11cxd3,http://www.reddit.com/r/redditdev/comments/11cxd3/is_there_a_subreddit_where_you_can_hire_reddit/,
1349279244.0,6,self.redditdev,10vodt,How do I get the links submitted to reddit for a period of time ?,9,3,1,http://www.reddit.com/r/redditdev/comments/10vodt/how_do_i_get_the_links_submitted_to_reddit_for_a/,"For example, how do I get all submissions (all subreddits included) within the last one hour or five hours ? Which url shows them ?",,False,,t5_2qizd,False,,,True,t3_10vodt,http://www.reddit.com/r/redditdev/comments/10vodt/how_do_i_get_the_links_submitted_to_reddit_for_a/,
1347934245.0,5,self.redditdev,1022k3,Using the api to mark a message as read doesn't seem to change the has_mail attribute from me.json. Any way around this?,6,1,0,http://www.reddit.com/r/redditdev/comments/1022k3/using_the_api_to_mark_a_message_as_read_doesnt/,"So, using the API to mark a message as read works just fine. After calling the api on a message, *message/unread.json* will not contain that message.

However, me.json will still return ""true"" for the has_mail attribute even though there are no unread messages. On reddit, the orangered  icon stays lit after the api is called as well. Clicking the orangered and visiting *message/unread* seems to be the only way to make it go away.

So, is there anything I can do to make me.json refresh its ""has_mail"" attribute after *api/read_message* is called?",,False,,t5_2qizd,False,,,True,t3_1022k3,http://www.reddit.com/r/redditdev/comments/1022k3/using_the_api_to_mark_a_message_as_read_doesnt/,
1347429879.0,5,self.redditdev,zr5rd,Recommended way to parse comment/wiki formatting,5,0,6,http://www.reddit.com/r/redditdev/comments/zr5rd/recommended_way_to_parse_commentwiki_formatting/,"Hi, is there anywhere that clearly documents the wiki-formatting syntax for comments? I'm referring to bold, links, etc.

I'm building a reddit client, and am not sure how to parse the comments to get the correct formatting.
Thanks!",,False,,t5_2qizd,False,,,True,t3_zr5rd,http://www.reddit.com/r/redditdev/comments/zr5rd/recommended_way_to_parse_commentwiki_formatting/,
1347283991.0,5,self.redditdev,znh1l,Error on Reddit load,5,0,2,http://www.reddit.com/r/redditdev/comments/znh1l/error_on_reddit_load/,"&lt;resolved&gt; When I browse to my site i get the ""you broke reddit"" page ... on the back end. this is the error generated. [pastebin](http://pastebin.com/LEgvBJ8K)",,False,,t5_2qizd,1347303932.0,,,True,t3_znh1l,http://www.reddit.com/r/redditdev/comments/znh1l/error_on_reddit_load/,
1347209033.0,4,self.redditdev,zlvd7,Is there any way to scrape more than the last 30 pages of an user account ?,6,2,7,http://www.reddit.com/r/redditdev/comments/zlvd7/is_there_any_way_to_scrape_more_than_the_last_30/,In one step. Thanks if anyone could help out. This could be really helpful for http://redditinvestigator.com !,,False,,t5_2qizd,False,,,True,t3_zlvd7,http://www.reddit.com/r/redditdev/comments/zlvd7/is_there_any_way_to_scrape_more_than_the_last_30/,
1347033717.0,5,self.redditdev,zielk,Using vote in the RedditAPI,5,0,2,http://www.reddit.com/r/redditdev/comments/zielk/using_vote_in_the_redditapi/,"I'm trying to register a vote with the API but it doesn't appear to be working.  I have logged in and received my Cookie/modhash.  Here is what I'm currently doing:

POST to http://www.reddit.com/api/vote

   

    Header:

        Content-Type: application/x-www-form-urlencoded; charset=UTF-8
        Content-Length: 86
        Host: www.reddit.com
        Cookie: reddit_session=&lt;cookie&gt;

    Payload:

        id=zi12t&amp;dir=1&amp;uh=&lt;modhash&gt;

When I send this it returns '{}'  which is what I expect.  However, if I go directly to the site I do not see the post as upvoted.  What am I doing wrong?",,False,,t5_2qizd,1347034531.0,,,True,t3_zielk,http://www.reddit.com/r/redditdev/comments/zielk/using_vote_in_the_redditapi/,
1346443407.0,6,self.redditdev,z5hl7,Working on RES: GM_xmlHttpRequest for http://reddit.com/r/.../about/modqueue/.json returns empty string 90% of the time.  Suggestions?,6,0,3,http://www.reddit.com/r/redditdev/comments/z5hl7/working_on_res_gm_xmlhttprequest_for/,"Hi, I'm working on a new module for RES, but am having some difficulties.  When attempting to retrieve http://reddit.com/r/somesubreddit/about/modqueue/.json using GM_xmlHttpRequest I receive an empty string 90% of the time.  The weird thing is that the other 10% of the time, I receive an appropriate response.  When entering the URL directly into Chrome, I receive the appropriate response 100% of the time.

Does anyone have any insight here?  I'm a little new to JS scripting, so hopefully this isn't something stupid, but hopefully it is something simple.

Thanks!",,False,,t5_2qizd,False,,,True,t3_z5hl7,http://www.reddit.com/r/redditdev/comments/z5hl7/working_on_res_gm_xmlhttprequest_for/,
1346163338.0,5,self.redditdev,yymt4,Is there a WSDL or WADL for the Reddit API?,6,1,4,http://www.reddit.com/r/redditdev/comments/yymt4/is_there_a_wsdl_or_wadl_for_the_reddit_api/,Does anyone know of a WSDL or WADL definition for the Reddit API? I can't seem to find one...,,False,,t5_2qizd,False,,,True,t3_yymt4,http://www.reddit.com/r/redditdev/comments/yymt4/is_there_a_wsdl_or_wadl_for_the_reddit_api/,
1345506774.0,5,self.redditdev,yjvgm,"When was the subreddit finder added, and is there an API for it? A direct URL where I can see it again?",5,0,3,http://www.reddit.com/r/redditdev/comments/yjvgm/when_was_the_subreddit_finder_added_and_is_there/,"[just ran across this today](http://i.snag.gy/mDw0j.jpg) for the first time.. neat!  Didn't see anything in /r/changelog ... 

It appeared at the top of my front page and after a few refreshes I can't seem to get it back to try it out again.

you can ignore the ""direct URL"" portion of my question as I've herp-derped that from the screenshot. duh.",,False,,t5_2qizd,False,,,True,t3_yjvgm,http://www.reddit.com/r/redditdev/comments/yjvgm/when_was_the_subreddit_finder_added_and_is_there/,
1345479069.0,4,self.redditdev,yj184,why might api/me.json return has_mail=false every 60 seconds for an hour after mail arrived?,5,1,3,http://www.reddit.com/r/redditdev/comments/yj184/why_might_apimejson_return_has_mailfalse_every_60/,"Is this the correct subreddit? Could I have asked this question more effectively?

I wrote my own little Reddit mail notifier in Python. It logs in then loops first making a reddit.com/api/me.json request then sleeping for 60 seconds. It logs a message and rings a bell every time 'has_mail' changes state. I returned to my PC after a couple hours and my notifier indicated that no Reddit mail had arrived. I refreshed a Reddit tab in Firefox and noticed an 'orange envelope'. In less then 60 seconds my notifier logged 'You have mail' and rang a bell. The mail had been there for over an hour! Any suggestions on how I might fix this?

Edit: Win7 64 bit, Python3.2 urllib
",,False,,t5_2qizd,1345489804.0,,,True,t3_yj184,http://www.reddit.com/r/redditdev/comments/yj184/why_might_apimejson_return_has_mailfalse_every_60/,
1345463851.0,3,self.redditdev,yip02,Can archiving be disabled? / How much freedom is there is manipulating the reddit source?,5,2,5,http://www.reddit.com/r/redditdev/comments/yip02/can_archiving_be_disabled_how_much_freedom_is/,"I am in the process of learning more about Reddit's functionality, and one of the projects I have in mind that could utilize the system appears to have a bit of an issue due to the archiving that occurs with new posts.

I could not find details of when this archiving takes place, but I have noticed on occasion when old articles can no longer be commenting on due to this feature. Is it possible to turn this off? It would be a huge advantage for me. If articles were always available for commenting is the best outcome for me.

I am struggled to learn how to get into the source of the code so if anybody has insight into this enquiry, a walkthrough or guide on how I could do this would be doubly beneficial.


**Edit (Solution from *slyf*):** 

*VOTE_AGE_LIMIT* and *REPLY_AGE_LIMIT* are both variables in the *config/development.ini* file (so update the *development.update* file and execute *make ini* in shell). Must be a positive value (eg, -1 does not allow unlimited).

Alternatively, edit the link.py file at https://github.com/reddit/reddit/blob/master/r2/r2/models/link.py
and replace any conditional statements using either config constant with *True* to allow unlimited comments and voting.",,False,,t5_2qizd,1345551197.0,,,True,t3_yip02,http://www.reddit.com/r/redditdev/comments/yip02/can_archiving_be_disabled_how_much_freedom_is/,
1345337869.0,4,self.redditdev,ygaf1,How do you use /api/v1/authorize?,5,1,6,http://www.reddit.com/r/redditdev/comments/ygaf1/how_do_you_use_apiv1authorize/,"I noticed that redditgifts.com uses an API endpoint that is currently undocumented to verify that you are the reddit account you say you are, without putting your reddit username and password in. Here's some data I collected:

     Request URL: https://ssl.reddit.com/api/v1/authorize
     Request Method: GET

    Query String Parameters
     
     scope: identity
     state: f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0
     redirect_uri: http://redditgifts.com/profiles/return/
     response_type: code
     client_id: A-BUncHoFsP3C1AL-cH4RacT3Rs

Eventually, after I [clicked to allow redditgifts.com to access my reddit identity](http://i.imgur.com/azkhb.png), it redirected to:

     Request URL: http://redditgifts.com/profiles/return/
     Request Method: GET

    Query String Parameters
     
     state: f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0
     code: A-BUncHoF-D1fF3ReNT-sP3C1AL-cH4RacT3Rs

I assume this works by storing the `code` in our application and sending further requests to the API with that code somehow? How can I best leverage this so I no longer have to ask the user for their username and password? What other scopes are there besides identity?",,False,,t5_2qizd,False,,,True,t3_ygaf1,http://www.reddit.com/r/redditdev/comments/ygaf1/how_do_you_use_apiv1authorize/,
1345180205.0,5,self.redditdev,yd2c3,PRAW Question - Resolving a Comment Object from Comment URL,6,1,2,http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/,"Hey, I'm having a ton of trouble with this, I know it's gotta be really easy, but basically I have a reddit comment url and I need to get a comment object from it, like similar functionality get_submission, do you have any idea how I could do this given the current library?

I'm guessing I can do it somehow with get_content, but I'm not at all understanding the object type that get_content is returning (new to python). When I try to iterate through the elements in the object using a for loop it, it always throws:

File ""...\reddit\__init__.py, line 263, in get_content
root = page_data[root_field]

TypeError: list indices must be integers, not str

If I had a solid debugger for python I'd totally pick apart the object and see if it has what I need in it, but I don't, and I'm getting kinda frustrated trying to implement what should be pretty simple code and failing, so any help you could throw my way would be much appreciated :D",,False,,t5_2qizd,False,,,True,t3_yd2c3,http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/,
1344709577.0,5,self.redditdev,y222u,KeyError: 'live_config',7,2,1,http://www.reddit.com/r/redditdev/comments/y222u/keyerror_live_config/,"Just installed the latest reddit code using the Ubuntu 11.04 script on github. But when I try to run:

'~/reddit/r2# paster run run.ini r2/models/populatedb.py -c 'populate()' '  I do get the following

'Traceback (most recent call last):
  File ""/usr/local/bin/paster"", line 9, in &lt;module&gt;
    load_entry_point('PasteScript==1.7.3', 'console_scripts', 'paster')()
  File ""/usr/lib/pymodules/python2.7/paste/script/command.py"", line 84, in run
    invoke(command, command_name, options, args[1:])
  File ""/usr/lib/pymodules/python2.7/paste/script/command.py"", line 123, in invoke
    exit_code = runner.run(args)
  File ""/usr/lib/pymodules/python2.7/paste/script/command.py"", line 218, in run
    result = self.command()
  File ""/home/reddit/reddit/r2/r2/commands.py"", line 78, in command
    load_environment(conf.global_conf, conf.local_conf)
  File ""/home/reddit/reddit/r2/r2/config/environment.py"", line 58, in load_environment
    g.setup()
  File ""/home/reddit/reddit/r2/r2/lib/app_globals.py"", line 282, in setup
    self.live_config = extract_live_config(parser, self.plugins)
  File ""/home/reddit/reddit/r2/r2/lib/app_globals.py"", line 56, in extract_live_config
    live_config = config._sections[""live_config""].copy()
KeyError: 'live_config''",,False,,t5_2qizd,False,,,True,t3_y222u,http://www.reddit.com/r/redditdev/comments/y222u/keyerror_live_config/,
1344700773.0,6,self.redditdev,y1utr,Bacon - Reddit extension for Safari: help needed (testing/artwork/code review).,7,1,2,http://www.reddit.com/r/redditdev/comments/y1utr/bacon_reddit_extension_for_safari_help_needed/,"Hello.

I've been developing, for the last few days, **Bacon**, a Reddit extension for Safari.

The extension implements a toolbar button that, when clicked on an open page, checks if that page was already submitted to Reddit and, if so, shows a popover with some info:

* thumbnail;
* score;
* author;
* subreddit;
* number of comments;
* the submission title.

It also provides links to the discussion page and a link to submit the page if it's not found.

The extension is fully functional, but I'm currently needing help on three fronts:

* **code review**: it's my first incursion in javascript and Safari extension development, so there may be some major design flaws or room for improvement;
* **testing**: I've been testing the extension, but extra testing is always welcome;
* **artwork**: there are two art assets needed: one icon for the toolbar and one icon for the extension itself (which is also used in the popover).

This last step is important if the extension is to be submitted to Apple's extension gallery. I'm currently using the Reddit alien for the extension icon, but for the toolbar icon I'm using an envelope icon from the [Mail Open Tabs](http://www.macosxtips.co.uk/extensions/) extension, which is not acceptable. And I really can't make a decent toolbar icon (I've tried).

You can get the extension here: [download](http://smux.net/bacon/Bacon.safariextz).

Thanks! :)",,False,,t5_2qizd,False,,,True,t3_y1utr,http://www.reddit.com/r/redditdev/comments/y1utr/bacon_reddit_extension_for_safari_help_needed/,
1344575243.0,4,self.redditdev,xzdf9,Admin control on certain domain,5,1,1,http://www.reddit.com/r/redditdev/comments/xzdf9/admin_control_on_certain_domain/,"I'm making an application in which I show popular and new posts submitted on my domain. I want to have admin control on these two pages, meaning I would like to hide certain posts just from these pages. I believe http://www.reddit.com/top/ also does something like this.

Is this possible with reddit api?",,False,,t5_2qizd,False,,,True,t3_xzdf9,http://www.reddit.com/r/redditdev/comments/xzdf9/admin_control_on_certain_domain/,
1344298166.0,5,self.redditdev,xsmv5,How to Go about implementing this with the API?,5,0,6,http://www.reddit.com/r/redditdev/comments/xsmv5/how_to_go_about_implementing_this_with_the_api/,"Hi!

I've been browsing the API on [Github](https://github.com/reddit/reddit/wiki) and I was wondering whether someone could explain how I can possibly do the following:

1. I'd like to see all submissions by a particular user in a specific subreddit only.

I think this can be possible because the Reditt Search allows the granularity of author and subreddit.

There is an API call to get all submissions by a particular user [here](http://www.reddit.com/dev/api#GET_{username}_{where}) however I'm not sure how I can change the parameters to specify the subreddit.

",,False,,t5_2qizd,False,,,True,t3_xsmv5,http://www.reddit.com/r/redditdev/comments/xsmv5/how_to_go_about_implementing_this_with_the_api/,
1343801496.0,6,self.redditdev,xhs8y,Posting via API lands in moderation queue.,7,1,6,http://www.reddit.com/r/redditdev/comments/xhs8y/posting_via_api_lands_in_moderation_queue/,"I post from the API to reddit to /r/mozillamemes.  I've noticed that whenever I use the script I've written to post from RSS to reddit, the post automatically goes into moderation queue.  I'm a mod myself, is there a way to make sure this post does not land in the moderation queue? The only way I've thought of so far is approving it from the moderation queue with another API call.",,False,,t5_2qizd,False,,,True,t3_xhs8y,http://www.reddit.com/r/redditdev/comments/xhs8y/posting_via_api_lands_in_moderation_queue/,
1343157768.0,6,self.redditdev,x36y2,How do I run reddit installation on mydomain.com instead of 10.0.0.1:8081?,8,2,7,http://www.reddit.com/r/redditdev/comments/x36y2/how_do_i_run_reddit_installation_on_mydomaincom/,"Need help on how to set up reddit installation on mydomain.com. I have managed to install it on a VPS using the reddit official guide but I can only run it via IP address eg 10.0.0.1:8081 while running ""paster serve --reload example.ini http_port=8081"" in terminal. How should I set this up to run on mydomain.com?",,False,,t5_2qizd,False,,,True,t3_x36y2,http://www.reddit.com/r/redditdev/comments/x36y2/how_do_i_run_reddit_installation_on_mydomaincom/,
1342715764.0,4,self.redditdev,wtnzn,New comer hoping to contribute,5,1,3,http://www.reddit.com/r/redditdev/comments/wtnzn/new_comer_hoping_to_contribute/,"Hi guys, I'm currently a newbie to contributing to open-source projects (or large codebases for that matter), and I thought I'd try to help build on the code of Reddit, since I'm already familiar with the site.  

I have a pretty good working (but not professional) knowledge of Java and Python.  However, I'm willing to learn any new languages or skills on the go (as is expected for this sort of thing).  I've looked at the Github repository for the Reddit code, but I'm really not sure how to approach the whole contribution process.  

e.g. Is there a pecking order for the developers?
Are there any 'tickets' or feature requests that I can see?
How and where do I contribute my code?

Thanks and sorry if these questions seem obvious to the more experienced developers out there.  ",,False,,t5_2qizd,False,,,True,t3_wtnzn,http://www.reddit.com/r/redditdev/comments/wtnzn/new_comer_hoping_to_contribute/,
1342218175.0,4,self.redditdev,witfv,/r/edmproduction's humble flairbot.... so many 429's!,6,2,10,http://www.reddit.com/r/redditdev/comments/witfv/redmproductions_humble_flairbot_so_many_429s/,"I'm a novice when it comes to this stuff. We over at /r/edmproduction have written a flairbot. The problem I'm having is that i'm getting a lot of HTTP 429 errors (too many requests). I've made my bot very persistent in trying to get and send data in that it keeps trying until reddit allows it. it waits at least a minute each time it gets a 429, but reddit will frequently make me wait up to 5 minutes in between successful requests!

Is there something I'm doing wrong? Why don't I get 429's in my browser, which is clearly processing far more requests than my bot? also, are my browser requests interfering with my bot's if it's behind the same router?",,False,,t5_2qizd,False,,,True,t3_witfv,http://www.reddit.com/r/redditdev/comments/witfv/redmproductions_humble_flairbot_so_many_429s/,
1341798112.0,5,self.redditdev,w8xh0,One API request to retrieve all of a user's overview?,6,1,4,http://www.reddit.com/r/redditdev/comments/w8xh0/one_api_request_to_retrieve_all_of_a_users/,"Hello,

I am developing app that requires retrieving all of the user's comments/submitted links.

I am aware I can get the next page of comments/submitted links by using the ""after"" tag, but the delay(2seconds) between API requests makes this take a long time to retrieve everything.

Is there an API request to retrieve all comments/submitted links in 1 single API request?

Thanks.",,False,,t5_2qizd,False,,,True,t3_w8xh0,http://www.reddit.com/r/redditdev/comments/w8xh0/one_api_request_to_retrieve_all_of_a_users/,
1341281930.0,5,self.redditdev,vyhi7,Interesting story score histories - sharp drops in story scores?,11,6,1,http://www.reddit.com/r/redditdev/comments/vyhi7/interesting_story_score_histories_sharp_drops_in/,"I've been working on a machine learning project of mine for a while now. The first stage of this project is to collect as much information on reddit stories and their progression to the front page. I collect a TON of information about each story and crawl a couple thousand stories per day. But I've noticed something interesting. Some front page stories progress very well, and then suddenly have a drop in score (within 5 minutes) of several hundred or more. 

I had a couple theories on my side - but I ruled out any programming error on my crawler. I've even witnessed this personally on the reddit front page. 

Here are screenshots from my interface I am using to visualize story histories: 
http://imgur.com/a/0ZccI

Can anyone offer an explanation as to why this happens? It happens more on stories that reach the frontpage 100 than when they dont make it. Is this some database synchronization bug, some bot accounts downvoting stores in synchronicity, or something else? Or maybe it is part of the spambot deterrent system. All theories or actual explanation is welcome!

By the way, if interested, I'll PM a link to my interface but it is in a very beta format running on a micro instance of EC2 so it can really only handle a couple of users for now.",,False,,t5_2qizd,1341297157.0,,,True,t3_vyhi7,http://www.reddit.com/r/redditdev/comments/vyhi7/interesting_story_score_histories_sharp_drops_in/,
1340910848.0,4,self.redditdev,vr7f3,Trying to make my install of reddit accept IPv6 links (ie. http://[...]/),7,3,5,http://www.reddit.com/r/redditdev/comments/vr7f3/trying_to_make_my_install_of_reddit_accept_ipv6/,"I found a [pull request](https://github.com/reddit/reddit/pull/235) that fixes this, I tried replacing the bits of code that were modified (specifically [this](https://github.com/k21/reddit/commit/2cfd44f8aef9cc30c33aeefcf728e4da8e637263) and [this](https://github.com/k21/reddit/commit/6ea6a6dc1381bef813cf597cd48e493607fb08c5)), yet to no avail. Do I need to rebuild something? restart something? ",,False,,t5_2qizd,False,,,True,t3_vr7f3,http://www.reddit.com/r/redditdev/comments/vr7f3/trying_to_make_my_install_of_reddit_accept_ipv6/,
1340437545.0,4,self.redditdev,vh5gl,Getting more data using  python reddit api wrapper,6,2,8,http://www.reddit.com/r/redditdev/comments/vh5gl/getting_more_data_using_python_reddit_api_wrapper/,"Using the python reddit api wrapper I'm looking to get as many submission objects as I can from a subreddit.  

    submissionsGenerator = r.get_subreddit( 'somesubreddit' ).get_new(limit=None)
    
But this will only return 50 submissions.  Why is this?  How can I use the generator to get more?

",,False,,t5_2qizd,False,,,True,t3_vh5gl,http://www.reddit.com/r/redditdev/comments/vh5gl/getting_more_data_using_python_reddit_api_wrapper/,
1340130290.0,3,self.redditdev,vabre,Storing Reddit credentials [x-post from r/coding],6,3,7,http://www.reddit.com/r/redditdev/comments/vabre/storing_reddit_credentials_xpost_from_rcoding/,"Hey everyone,

I'm working on a project that will pull videos from subreddits that Redditors are subscribed to. Problem is, I want to be able to scrape subreddits in the background while the user isn't logged in, and Reddit doesn't support OAuth, which would mean I'd have to store credentials.

Anyone have any insight into a good way to pull this off without screwing shit up, security wise? 

What I have thought of already:
- AES/Rjindael encryption for passwords (planning on using http://crypt.rubyforge.org/rijndael.html)
- Encrypted data kept on a different server than the secret key
- SSL for /api/login
- Server security precautions (iptables, firewall, etc)

Another option would just be to cache the user's subreddits once, and never store the credentials, but any time the user updated their subreddits they'd have to refresh it my app. The idea of the project is that it becomes your feed of personalized video content, so I'd like to make the user experience as smooth as possible. 

Any tips/advice/rants are welcome!",,False,,t5_2qizd,False,,,True,t3_vabre,http://www.reddit.com/r/redditdev/comments/vabre/storing_reddit_credentials_xpost_from_rcoding/,
1337920503.0,4,self.redditdev,u402g,"morechildren API, how do I get the Author of the post?",5,1,2,http://www.reddit.com/r/redditdev/comments/u402g/morechildren_api_how_do_i_get_the_author_of_the/,"Hey guys,

I'm currently working on an app for reddit and I am trying to pull more comments within the comment tree. I'm using the morechildren api to do this. Sadly, it does not return any info in regards to the author of the post, making comments look like this: http://i.imgur.com/O8T9I.png

Do you guys know how to get the author data?",,False,,t5_2qizd,False,,,True,t3_u402g,http://www.reddit.com/r/redditdev/comments/u402g/morechildren_api_how_do_i_get_the_author_of_the/,
1337827911.0,4,self.redditdev,u24q9,"""wrapped"" python module?",7,3,2,http://www.reddit.com/r/redditdev/comments/u24q9/wrapped_python_module/,"I'm fairly new to this, so I apologize in advance if this is a stupid question. I just downloaded the source code, installed dependencies, and ran ""setup.py"" a few times. Eventually, setup.py finished without any error codes. However, when I go to import a module, for example r2.models.subreddit, I get the error:

    from wrapped import Templated, CacheStub
    ImportError: No module named wrapped

I've scoured the internet, and I am unable to find any relevant information. I suspect it has something to do with the dependencies that I had to manually install. Any help would be greatly appreciated.",,False,,t5_2qizd,False,,,True,t3_u24q9,http://www.reddit.com/r/redditdev/comments/u24q9/wrapped_python_module/,
1337070114.0,6,self.redditdev,tnxsj,A couple PRAW notes (domain issue and make_moderator),6,0,5,http://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/,"Two things I noticed earlier tonight while using the python reddit api wrapper [PRAW](https://github.com/mellort/reddit_api):

1)  The domain in .config/reddit_api/reddit_api.cfg has to match the .ini config for the server, even if they both point to the same IP.  I had a couple DNS names configured for my site, and PRAW started complaining that I hadn't logged in even though I had.  E.G., if the .ini file has reddit1.example.com and your reddit_api.cfg file has reddit2.example.com (both of which point to your server), PRAW will not recognize your login (and not throw an error either until you try to do something that requires the login).  It took a while to debug :/

2) Here is an example for make_moderator, which is not shown on the wiki:

    import reddit
    r = reddit.Reddit('some user agent','myreddit')
    sub = r.get_subreddit('mysubreddit')
    r.login('mymoderator','password')
    sub.make_moderator('thenewmoderator')
    ",,False,,t5_2qizd,False,,,True,t3_tnxsj,http://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/,
1336838379.0,6,self.redditdev,tjr4s,Protocol (https) and port numbers get changed in links,6,0,2,http://www.reddit.com/r/redditdev/comments/tjr4s/protocol_https_and_port_numbers_get_changed_in/,"I've given up on dropping pound SSL reverse-proxy in front of my reddit install (links get rewritten as http for some reason), but now I'm noticing that some links have a hard-coded port of 8000 (like the preferences link).  Where is it getting these port numbers from and why would it be hard-coding the protocol and port number in the first place?",,False,,t5_2qizd,False,,,True,t3_tjr4s,http://www.reddit.com/r/redditdev/comments/tjr4s/protocol_https_and_port_numbers_get_changed_in/,
1336151558.0,4,self.redditdev,t73fr,HTML in Subreddit Description?,7,3,7,http://www.reddit.com/r/redditdev/comments/t73fr/html_in_subreddit_description/,"I noticed that /r/pics has a neat little subtitle below the header that reads 
&gt; Looking for an image subreddit with minimal rules? Check out /r/images

I was a bit curious how they did this, and it looks like they put an h3 tag in the subreddit's description, then used css to simply absolutely position it below the header (and add additional margin to the .content).

They also have an h1 element in their description.  Everything I've tried gets escaped to actual characters rather than html elements.  Am I missing something?  How did they do this?

This may not be the right place to ask, but it is for a project I'm working on where this may be a better alternative to hardcoding a subtitle in elsewhere (since it would let the mods change it).  I tried messaging the mods of /r/pics asking them how they did it, but they haven't gotten back to me.",,False,,t5_2qizd,False,,,True,t3_t73fr,http://www.reddit.com/r/redditdev/comments/t73fr/html_in_subreddit_description/,
1335693232.0,4,self.redditdev,sy0do,"Amyone know the the amount of space used on S3 for the reddit thumbs, etc?",8,4,4,http://www.reddit.com/r/redditdev/comments/sy0do/amyone_know_the_the_amount_of_space_used_on_s3/,"Any ballpark would be great. ""TIA""",,False,,t5_2qizd,False,,,True,t3_sy0do,http://www.reddit.com/r/redditdev/comments/sy0do/amyone_know_the_the_amount_of_space_used_on_s3/,
1335484134.0,4,self.redditdev,suf3p,Reddit upvote script,23,19,16,http://www.reddit.com/r/redditdev/comments/suf3p/reddit_upvote_script/,"Hello there, you wont be happy about this. My friends asked me to write them a small python script which logs in an account upvotes a post and repeats (for the same post). They figured that if they could get a lot of fast upvotes it would give the reddit post a little bump. They aren't spammers etc. My script runs perfectly, it loops through a block of code that logs in an account from a pre defined list, upvotes a predefined submission and loops back. When I check the post it only has gained one upvote. A) Why is this. and B) how can I get my program to work properly. I think that the website searches for votes coming from one ip and will only take one. If this is the case can someone please provide a bit of code that will log into a proxy? If this is not the case, can someone please enlighten me as to how I fix my script? 

Gracias,

          Seor Brujo 

P.S. Perhaps the reddit api only allows one upvote/downvote request from one api connection?",,False,,t5_2qizd,False,,,True,t3_suf3p,http://www.reddit.com/r/redditdev/comments/suf3p/reddit_upvote_script/,
1333250986.0,6,self.redditdev,rnapp,Reddit takes 2-5 minutes to respond,8,2,2,http://www.reddit.com/r/redditdev/comments/rnapp/reddit_takes_25_minutes_to_respond/,"I'm using the [reddit_api](https://github.com/mellort/reddit_api) Python wrapper, and it seems it takes a good 2-5 minutes before my site gets a response. 

I experienced it on an Ubuntu 11.04 64bit Virtual Machine (while developing), and also on my hosting server. (WebFaction Centos6-64bit).

I tried it on Mac OS X Lion, but I'm not getting the long response times there. (so the wrapper seems to be working well?)

Also, I have an old version of my site (http://r.doqdoq.com), which isn't experiencing the long delay as well. It's using an *old* version of the wrapper, runs on Linode (Ubuntu 10.04 LTS), built on [Flask][http://flask.pocoo.org]. (I'm switching to WebFaction, which is why I decided to rewrite it using the latest version of the wrapper, and built with Django instead of Flask, since WebFaction doesn't have Flask).

You can see what I mean by visiting the site (http://coderedd.net), and opening up your browser's dev panel, network tab, and click on any link. 

I'd really appreciate any info. 

(EDIT: I'll try out my old version (Flask) on my Mac and Ubuntu VM to see if I get similar experience. I'll update this post with the results.)",,False,,t5_2qizd,True,,,True,t3_rnapp,http://www.reddit.com/r/redditdev/comments/rnapp/reddit_takes_25_minutes_to_respond/,
1331650796.0,4,self.redditdev,qulau,"""clicked"" field in a subreddit story list",6,2,2,http://www.reddit.com/r/redditdev/comments/qulau/clicked_field_in_a_subreddit_story_list/,"Has this always been here? Does it actually work? I have been playing with this and it doesn't seem to work right now. 

       ""kind"": ""t3"",
        ""data"": {
          ""domain"": ""i.imgur.com"",
          ""media_embed"": {},
          ""subreddit"": ""funny"",
          ""selftext_html"": null,
          ""selftext"": """",
          ""likes"": null,
          ""saved"": false,
          ""id"": ""quhfd"",
          ""clicked"": false, &lt;-------------------------------------------
          ""title"": ""Welcome to the friend zone..."",
          ""media"": null,
          ""score"": 975,
          ""over_18"": false,
          ""hidden"": false,
          ""thumbnail"": """",
          ""subreddit_id"": ""t5_2qh33"",
          ""author_flair_css_class"": null,
          ""downs"": 468,
          ""is_self"": false,
          ""permalink"": ""/r/funny/comments/quhfd/welcome_to_the_friend_zone/"",
          ""name"": ""t3_quhfd"",
          ""created"": 1331669805.0,
          ""url"": ""http://i.imgur.com/ol7tD.png"",
          ""author_flair_text"": null,
          ""author"": ""face_phuck"",
          ""created_utc"": 1331644605.0,
          ""num_comments"": 169,
          ""ups"": 1443
        }
      },",,False,,t5_2qizd,False,,,True,t3_qulau,http://www.reddit.com/r/redditdev/comments/qulau/clicked_field_in_a_subreddit_story_list/,
1328986402.0,4,self.redditdev,pl0jz,Trouble parsing json from the Reddit API with python,7,3,5,http://www.reddit.com/r/redditdev/comments/pl0jz/trouble_parsing_json_from_the_reddit_api_with/,"Originally posted [in r/python](http://www.reddit.com/r/Python/comments/pk022/trouble_parsing_json_from_the_reddit_api_python_32/), I was advised to cross-post here.

So I'm playing around with python and the Reddit API, and I've hit a roadblock when trying to fetch the json from /new pages. For example, here's some code that works as expected:

    import urllib.request
    import json

    response = urllib.request.urlopen('http://www.reddit.com/r/pics/.json').read()
    jsonResponse = json.loads(response.decode('utf-8'))
    print(jsonResponse['data']['children'])

This outputs the array of submissions on the frontpage of r/pics. But if I attempt to get the submissions from the 'new' section of [/r/pics](/r/pics) (so the url is now ""http://www.reddit.com/r/pics/new/.json""), then I get '[]' returned.

    jsonResponse['data'].items()

shows

    dict_items([('modhash', ''), ('children', []), ('after', None), ('before', None)])

If I load the url in my browser (with a json viewer extension) I can see and read the children array just fine for both the front page and /new/. For some reason, python can't. Other pages like /controversial/ and /top/ also work fine in python.

Someone suggested spoofing my user agent, but that is explicitly against the rules of using the API, and I wasn't sure if it was okay for debugging at least. Thought it better to post here first. Someone else pointed out that it's because I'm logged out, and the json returned by a logged-out browser is the same as what python is getting. I don't know if that's a feature or a bug, though.

Ideas?",,False,,t5_2qizd,False,,,True,t3_pl0jz,http://www.reddit.com/r/redditdev/comments/pl0jz/trouble_parsing_json_from_the_reddit_api_with/,
1328941213.0,4,self.redditdev,pki4l,[API] Is there a way to load comments for a post incrementally?,7,3,1,http://www.reddit.com/r/redditdev/comments/pki4l/api_is_there_a_way_to_load_comments_for_a_post/,"I've been messing with the API using PHP, trying to make a simple bot that watches comments and replies to some (based on irrelevant criteria). One problem I've run into is that no matter how much memory management I try to enforce, I keep running out of memory while processing posts with more than a few hundred comments.

I know that I can use the GET variable `limit` to control how many comments I get, but can I tell it to grab me comments 100-200, or 100 comments after comment t1_c3q20hq using some other variable? Any other ideas are more than welcome.

Also, if this problem has been solved by some API wrapper, please don't tell me about it. This is less about me getting a bot working and more about me *making* a working bot. Thanks. :)",,False,,t5_2qizd,False,,,True,t3_pki4l,http://www.reddit.com/r/redditdev/comments/pki4l/api_is_there_a_way_to_load_comments_for_a_post/,
1328201923.0,5,self.redditdev,p7vfs,Updating Subreddit Description,6,1,6,http://www.reddit.com/r/redditdev/comments/p7vfs/updating_subreddit_description/,"In the API I see you can create a subreddit using the following format:

	@reddit.decorators.require_login
	def create_subreddit(self, name, title, description='', language='en',
	subreddit_type='public', content_options='any',
	  over_18=False, default_set=True, show_media=False,
			 domain=''):
		""""""Create a new subreddit""""""
		params = {'name': name,
				  'title': title,
				  'description': description,
				  'lang': language,
				  'type': subreddit_type,
				  'link_type': content_options,
				  'over_18': 'on' if over_18 else 'off',
				  'allow_top': 'on' if default_set else 'off',
				  'show_media': 'on' if show_media else 'off',
				  'domain': domain,
				  'uh': self.modhash,
				  'id': '#sr-form',
				  'api_type': 'json'}
		return self.request_json(self.config['site_admin'], params)

But... using a method similar to this, can I update just the description of a subreddit?

**EDIT**

I have just found [this record](https://github.com/mellort/reddit_api/pull/52) to show that something is being created to allow updating. Does anyone have any ideas how I could use it? Calling it (update_community_settings) currently gives a 404.

Please let me know. Any help would be much appreciated as my subreddit, [/r/RedditDayOf](/r/RedditDayOf), is really in need of this functionality.",,False,,t5_2qizd,True,,,True,t3_p7vfs,http://www.reddit.com/r/redditdev/comments/p7vfs/updating_subreddit_description/,
1328027721.0,6,self.redditdev,p4p6d,"Is the subreddit creator(user) ID exposed in the API 
anywhere?",8,2,2,http://www.reddit.com/r/redditdev/comments/p4p6d/is_the_subreddit_creatoruser_id_exposed_in_the/,"I'm looking for the name of the user that created a subreddit.

It doesn't seem to be exposed in /about.json or /about/moderators.json 

Thanks!",,False,,t5_2qizd,False,,,True,t3_p4p6d,http://www.reddit.com/r/redditdev/comments/p4p6d/is_the_subreddit_creatoruser_id_exposed_in_the/,
1324113702.0,7,self.redditdev,ng7oc,Question on storing session_id,9,2,6,http://www.reddit.com/r/redditdev/comments/ng7oc/question_on_storing_session_id/,"I'm doing something wrong here as I keep getting: {errors=[[USER_REQUIRED, please login to do that, null]]}

I'm able to login, but I'm not passing the cookie back properly:
    
    String apiParams = ""api_type=json&amp;id=c38ghjg&amp;dir=1&amp;uh=96j453g7xfe25c5d82d6261576c1bc5141622604b3e2712aa5"";
				
		URL voteURL = new URL(""http://www.reddit.com/api/vote"");
		HttpURLConnection	connection = (HttpURLConnection) voteURL.openConnection ();
		//connection.setDoInput (true);
		connection.setDoOutput (true);

		connection.setRequestMethod (""POST"");
		
		connection.setUseCaches (false);
		connection.setRequestProperty (""Content-Type"",
	            ""application/x-www-form-urlencoded; charset=UTF-8"" );
		connection.setRequestProperty(""cookie"", ""8103138%2C2011-12-17T01%3A10%3A27%2C4c5e0230099737eed8"");
		connection.setRequestProperty (""Content-Length"", String.valueOf( apiParams.length() ));
		DataOutputStream wr = new DataOutputStream(
            connection.getOutputStream() );
        wr.writeBytes( apiParams );
        //System.out.println(wr);
        wr.flush();
        wr.close();",,False,,t5_2qizd,True,,,True,t3_ng7oc,http://www.reddit.com/r/redditdev/comments/ng7oc/question_on_storing_session_id/,
1323965563.0,4,self.redditdev,ndwbi,Implementation of Bayesian average scoring,6,2,1,http://www.reddit.com/r/redditdev/comments/ndwbi/implementation_of_bayesian_average_scoring/,"Hi All, 

I asked this in r/learnprogramming and was kindly pointed in your direction.

I have been researching algorithmic sorting systems like the one used here on Reddit and I wonder if anybody has any experience actually implimenting one into a system that could share their knowledge.

To test it I grabbed reddit's Python code (https://github.com/reddit/reddit/blob/master/r2/r2/lib/db/_sorts.pyx) and while I can happily pass in perameters and have it return a score, I'm still completely baffled as to how this would work in practice. 

You would surely not have a server iterating in an eternal loop over each submission in the database, passing the data to the python script to re-calculate the score, passing it back to the db to store and re-index?

Is this process run when a vote is logged, perhaps? Or am I completely off the mark?

Would anybody be able to share a rough description of how the algorithm works in practise and how it interfaces with the database? It would really help me plug an infuriating gap in my understanding. :)
",,False,,t5_2qizd,False,,,True,t3_ndwbi,http://www.reddit.com/r/redditdev/comments/ndwbi/implementation_of_bayesian_average_scoring/,
1323302732.0,5,self.redditdev,n414h,Questions about setting up reddit clone,8,3,14,http://www.reddit.com/r/redditdev/comments/n414h/questions_about_setting_up_reddit_clone/,"Hey guys,

I don't really have CS background but I'm interested in how works reddit works. I think there's a ton of interesting applications for it and so I'm trying to get started by creating my own local clone. However I've been running into some problems getting it set up and I was hoping you guys could help.

I ran the Ubuntu installer and it seemed for the most part to go smoothly. I don't remember seeing any of the error messages that were displayed in the troubleshooting guide. However, I still cannot see the page in my browser, even using the reddit.local link.

any ideas/suggestions?",,False,,t5_2qizd,False,,,True,t3_n414h,http://www.reddit.com/r/redditdev/comments/n414h/questions_about_setting_up_reddit_clone/,
1316783325.0,6,self.redditdev,kp0dl,Correct reddit button syntax for links with special characters,6,0,1,http://www.reddit.com/r/redditdev/comments/kp0dl/correct_reddit_button_syntax_for_links_with/,"Hopefully this is the right place to ask this, I'm trying to use a reddit button iframe with a specified URL that has an ampersand in it but I can't get it to link to the right post. 

For example, I have tried to create a reddit button for [this link](http://www.reddit.com/r/videos/comments/kon9i/james_earl_jones_will_out_act_anybody/) using the following iframes without success:

 * Unencoded (obviously won't work):

&lt;iframe src=""http://www.reddit.com/static/button/button1.html?url=http://www.youtube.com/watch?v=K_kGtQmvrVI&amp;feature=related&amp;sr=videos""&gt;&lt;/iframe&gt;

 * Encoded using javascript's escape function

&lt;iframe src=""http://www.reddit.com/static/button/button1.html?url=http%3A//www.youtube.com/watch%3Fv%3DK_kGtQmvrVI%26feature%3Drelated&amp;sr=videos""&gt;&lt;/iframe&gt;

 * Using encodeURIComponent:

&lt;iframe src=""http://www.reddit.com/static/button/button1.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DK_kGtQmvrVI%26feature%3Drelated&amp;sr=videos""&gt;&lt;/iframe&gt;

 * Replacing the &amp; with &amp; amp;:

&lt;iframe src=""http://www.reddit.com/static/button/button1.html?url=http://www.youtube.com/watch?v=K_kGtQmvrVI&amp;amp;feature=related&amp;sr=videos""&gt;&lt;/iframe&gt;

 * However links without a special character work as expected:

&lt;iframe src=""http://www.reddit.com/static/button/button1.html?url=http://imgur.com/LT8vx&amp;sr=pics""&gt;&lt;/iframe&gt;

Is there any way to get the reddit button working with URLs like the first one?",,False,,t5_2qizd,False,,,True,t3_kp0dl,http://www.reddit.com/r/redditdev/comments/kp0dl/correct_reddit_button_syntax_for_links_with/,
1315047876.0,7,self.redditdev,k3fom,Would this be accepted: option to optimize reddit for kindle ?,8,1,4,http://www.reddit.com/r/redditdev/comments/k3fom/would_this_be_accepted_option_to_optimize_reddit/,"I'm thinking about working on this feature:
For people using the kindle(or if the community wants, other mobile devices), external links will be rendered in a similar way to kinstant.com .

example:
http://google.com/gwt/x?u=http://www.escapistmagazine.com/news/view/111442-OnLive-Founder-Claims-Impossible-Wireless-Breakthrough

This is faster , and better fit for the kindle. 

Another option is some kind of setting to control this. 

Admins , would this kind of change be accepted to reddit ?
",,False,,t5_2qizd,False,,,True,t3_k3fom,http://www.reddit.com/r/redditdev/comments/k3fom/would_this_be_accepted_option_to_optimize_reddit/,
1313703811.0,5,github.com,jn75e,"RSS to reddit uploader - In case you was searching for it. 
Just coded. Be careful.",8,3,5,http://www.reddit.com/r/redditdev/comments/jn75e/rss_to_reddit_uploader_in_case_you_was_searching/,,,False,,t5_2qizd,False,,,False,t3_jn75e,https://github.com/baryluk/ressit,
1313674823.0,6,self.redditdev,jmpz3,Is there a way to calculate subreddit overlap? (crosspost from r/help),9,3,4,http://www.reddit.com/r/redditdev/comments/jmpz3/is_there_a_way_to_calculate_subreddit_overlap/,"I'm interested in seeing how subreddits are connected to each other. How many people subscribe to, for example, r/europe and r/germany? Is it possible to see such statistics?

I'm sure there's a database relating users and subreddits to manage subscriptions, so basically I'm asking whether an anonymised or condensed (i.e. containing only the sum of connections between two subreddits) version of that table could be released?",,False,,t5_2qizd,False,,,True,t3_jmpz3,http://www.reddit.com/r/redditdev/comments/jmpz3/is_there_a_way_to_calculate_subreddit_overlap/,
1306258090.0,3,self.redditdev,hj1ms,"Trying to fetch Reddit .json data from Google App Engine, but I only get a teeny bit of data compared to what I get with the exact same URL requested from a web browser",8,5,2,http://www.reddit.com/r/redditdev/comments/hj1ms/trying_to_fetch_reddit_json_data_from_google_app/,"I started developing a little utility last week, and need to populate my App Engine datastore with some posts from my subreddit.

So I have a little cron job which does a urlfetch to grab the .json data, which is then parsed and stored. Unfortunately, I only get a few entries in my first page request. If I then try grabbing the following page using ""after"", I rarely get anything at all - just an empty ""children"" list in the data element.  

In total, I'm able to get maybe 5 to 8 posts - that's it!

What could be the reason? The URL is correct: when I use a web browser (Chrome) to access the same URL, I get 25 elements.",,False,,t5_2qizd,False,,,True,t3_hj1ms,http://www.reddit.com/r/redditdev/comments/hj1ms/trying_to_fetch_reddit_json_data_from_google_app/,
1302177750.0,4,self.redditdev,gko5x,Passing ModHash to /reddits/mine.json to display a list of users subscribed reddits,5,1,6,http://www.reddit.com/r/redditdev/comments/gko5x/passing_modhash_to_redditsminejson_to_display_a/,"I'm trying to return a list of users subscribed reddits once they have authenticated in my app. How can I pass a cookie and/or modhash to this json page?

I have tried passing the uh parameter during a get request however it just redirects me to login.json

Maybe i'm barking up the wrong tree?",,False,,t5_2qizd,False,,,True,t3_gko5x,http://www.reddit.com/r/redditdev/comments/gko5x/passing_modhash_to_redditsminejson_to_display_a/,
1292456585.0,6,self.redditdev,emi4h,Do you think it is worthwhile to finish my reddit slideshow app?,9,3,7,http://www.reddit.com/r/redditdev/comments/emi4h/do_you_think_it_is_worthwhile_to_finish_my_reddit/,"I made an app on google app engine to browse reddit posted pictures in a full-size slideshow, the app is not finished yet, you will notice it has no css for example, but before I continue with development I want to know if it is of any value for you. 

The plan is to keep it open source and free of ads.

Right now I'm the only one making commits but collaborators are welcome.

app: [http://reddit-slides.appspot.com](http://reddit-slides.appspot.com)
src: [http://github.com/johnblanco/jb_slides](http://github.com/johnblanco/jb_slides)",,False,,t5_2qizd,False,,,True,t3_emi4h,http://www.reddit.com/r/redditdev/comments/emi4h/do_you_think_it_is_worthwhile_to_finish_my_reddit/,
1289219399.0,3,self.redditdev,e2w75,"Hey RedditDev, I'm trying to use Reddit's comments as part of some research into opinion mining. Can someone help me with your source?",9,6,4,http://www.reddit.com/r/redditdev/comments/e2w75/hey_redditdev_im_trying_to_use_reddits_comments/,"Hey all, I don't know if this is the right place to post this but I hope so. Feel free to mod otherwise.

I'm interested in seeing the number of votes up and down for a comment on Reddit, and for links themselves too. I know this data is sort of available in the source HTML for each page, but I wanted to confirm I'm looking in the right place (is it the score dislikes tag?) and also ask if there was an API or better way of retrieving this data.

I can't use the dumps becuase I need up-to-date information on specific reddit threads. What's the best of way of getting through to the votes?",,False,,t5_2qizd,False,,,True,t3_e2w75,http://www.reddit.com/r/redditdev/comments/e2w75/hey_redditdev_im_trying_to_use_reddits_comments/,
1284665941.0,6,self.redditdev,deup3,Visual sub reddits?,10,4,7,http://www.reddit.com/r/redditdev/comments/deup3/visual_sub_reddits/,"I started reddit about a month ago and I had a really hard time locating subreddits. 

I know know the error of my noob ways but it did get me thinking, what if reddit had a visual starting point for ""new"" people or just a page that gave a visualization of the different subreddits. 

What I am talking about is something like this:
http://csinsider.homeip.net/config/mchrisco/resume/proj3/

or this:

http://thejit.org/


So you would have a visual graph and be able to navigate the graph to subreddits easily. Or is there already something like this already?

Do you think it would be a good feature?

EDIT: 
Here is a a (close to finished) model of the Visual subreddit: 
http://csinsider.homeip.net/config/reddit_hypertree/hypertree.html
It is based on Karmanaut's Redditmap. 
I am taking a break (been doing this for about 1 hour) so ill be back in a bit. 

Subreddits I have yet to do:

Fun

Tech

Music

Educational

Usergenerated",,False,,t5_2qizd,True,,,True,t3_deup3,http://www.reddit.com/r/redditdev/comments/deup3/visual_sub_reddits/,
1282945845.0,5,self.redditdev,d6bxw,Discount fork w/ strikethrough support,8,3,10,http://www.reddit.com/r/redditdev/comments/d6bxw/discount_fork_w_strikethrough_support/,"**Edit** temporarily disabled the repo while I reorganize some things.

Per Sephr's comment, I'm changing it so it's `&lt;del&gt;` tag, among other things.",,False,,t5_2qizd,True,,,True,t3_d6bxw,http://www.reddit.com/r/redditdev/comments/d6bxw/discount_fork_w_strikethrough_support/,
1282924452.0,4,self.redditdev,d66uc,"Is there a way, via the API, to know if a user is a gold member?",9,5,16,http://www.reddit.com/r/redditdev/comments/d66uc/is_there_a_way_via_the_api_to_know_if_a_user_is_a/,"Mostly I actually only care about the logged in user, for now, but it'd be nice to know generally.  I know about about.json for the user, but that doesn't seem to include any indication of gold or not.

Thanks!",,False,,t5_2qizd,False,,,True,t3_d66uc,http://www.reddit.com/r/redditdev/comments/d66uc/is_there_a_way_via_the_api_to_know_if_a_user_is_a/,
1282903806.0,5,self.redditdev,d6309,"I'm sick of searching for my comments in a long 
thread, so I wrote some JavaScript to take care of 
the problem.",8,3,5,http://www.reddit.com/r/redditdev/comments/d6309/im_sick_of_searching_for_my_comments_in_a_long/,"I've not contributed to the reddit code before and don't know if this would be of wider interest. It's a fairly simple bit of JavaScript, using jQuery to find your comments and provide a way to jump to them.

The simplest way to see it in action is to copypasta this code into your address bar (tested in FF and Opera on Linux):

    javascript:(function() {if (reddit.logged) {var yc = $('.noncollapsed a[href=http://www.reddit.com/user/' + reddit.logged + ']'); if(yc.size() &gt; 0) { $('.cloneable').eq(0).after('&lt;br&gt;&lt;div class=""rounded gold-accent comment-list-box""&gt;&lt;div class=""title""&gt;Jump to your comments:&lt;/div&gt;&lt;div&gt;');for(var i=0;i&lt;yc.size();i++){var text = yc.eq(i).parent().parent().find('.usertext-body .md').text(); if (text.length &gt; 54) { text = text.substring(0, 50)+' ...';} $('.comment-list-box div:last').eq(0).after('&lt;div id=""yourcomment' + i + '""&gt;#'+(i+1)+': '+text+'&lt;/div&gt;');}for(var i=0;i&lt;yc.size();i++){var cs = yc.eq(i).offset().top;$('#yourcomment'+i).bind('click',(function (lcs) { return function(){$('html,body').animate({scrollTop: lcs},500);}})(cs));}};};})();

I'm after feedback, suggestions for improvement, and some idea of whether this is worth submitting to the main codebase.

Here's a more readable version of the code for those of you geeky enough to be interested. Which I would guess is a reasonable percentage in /r/redditdev :)

    (function() {
        if (reddit.logged) {
            var yc = $('.noncollapsed a[href=http://www.reddit.com/user/' + reddit.logged + ']'); 
            if (yc.size() &gt; 0) { 
                $('.cloneable').eq(0).after('&lt;br&gt;&lt;div class=""rounded gold-accent comment-list-box""&gt;&lt;div class=""title""&gt;Jump to your comments:&lt;/div&gt;&lt;div&gt;');
                for(var i=0;i&lt;yc.size();i++){
                    var text = yc.eq(i).parent().parent().find('.usertext-body .md').text(); 
                    if (text.length &gt; 54) { 
                        text = text.substring(0, 50)+' ...';
                    } 
                    $('.comment-list-box div:last').eq(0).after('&lt;div id=""yourcomment' + i + '""&gt;#'+(i+1)+': '+text+'&lt;/div&gt;');
                }
                for(var i=0;i&lt;yc.size();i++){
                    var cs = yc.eq(i).offset().top;
                    $('#yourcomment'+i).bind('click',
                        (function (lcs) { 
                            return function(){
                                $('html,body').animate({scrollTop: lcs},500);
                            }
                        })(cs)
                    );
                }
            };
        };
    })();",,False,,t5_2qizd,True,,,True,t3_d6309,http://www.reddit.com/r/redditdev/comments/d6309/im_sick_of_searching_for_my_comments_in_a_long/,
1282827547.0,4,self.redditdev,d5oz0,anyone willing to build/skin a reddit for cash?,8,4,4,http://www.reddit.com/r/redditdev/comments/d5oz0/anyone_willing_to_buildskin_a_reddit_for_cash/,"I'm toying with the idea of an internal reddit at work, but I don't really have the time to git it running, and looking nice.  I just tried building one on Debian Testing, put it puked up a bunch of errors about src/lxml/lxml.etree.c

Just wondering is anyone does this stuff for a living, wondering how much?",,False,,t5_2qizd,False,,,True,t3_d5oz0,http://www.reddit.com/r/redditdev/comments/d5oz0/anyone_willing_to_buildskin_a_reddit_for_cash/,
1282662013.0,5,self.redditdev,d4v5q,VM image won't boot in VirtualBox (blank screen with unblinking cursor),5,0,5,http://www.reddit.com/r/redditdev/comments/d4v5q/vm_image_wont_boot_in_virtualbox_blank_screen/,"Found a similar complaint a month old with out a successful update or any replies: http://www.reddit.com/r/redditdev/related/ckigm/new_installation_using_the_ubuntu_64bitvmdk_but/

I spent several hours trying different VBox settings trying to get it boot.  Running virtualbox in OSX on an Intel Core Duo 2 GHz machine.  Edit: using virtualbox 3.2.8
",,False,,t5_2qizd,True,,,True,t3_d4v5q,http://www.reddit.com/r/redditdev/comments/d4v5q/vm_image_wont_boot_in_virtualbox_blank_screen/,
1279711180.0,5,self.redditdev,crzum,Problems submitting links on fresh Reddit install (Debian),7,2,3,http://www.reddit.com/r/redditdev/comments/crzum/problems_submitting_links_on_fresh_reddit_install/,"Hi Guys, 

I'm having some issues trying to submit links on this brand new reddit installation.

I don't see any error showing up in the log, even when turing debug mode on. (Probably I'm not looking in the right place..)

This is what happens: 

I try to submit text: Browser (all of them) 'hangs' on showing red 'Submitting...' message.. nothing happnes. HOWEVER, the message GETS posted once you return to the subreddit manually. 

If I try to submit a URL: Same behaviour in the browser.. get to 'Submitting...' then nothing. The post never gets into the subreddit (nor can I find any trace of it in the database.. )

The only other 'abnormality' I can see is that the submit page is a bit different formated then the current running production page on reddit. 

It's missing the blue frames around the input windows, the the default size of those windows are wrong (very small) . In IE you can resize them properly, but in FF and Chrome you are stuck with the wrong size..   I assuming the CSS is not loaded properly but this is something I will look into next. 

Other then that the installation seems to be working fine. Only futher issue I had was running populatedb(). This seemed to do what its suppose to do (post the google.com url to the subreddit reddit_test2 etc..) , but hangs once finnished. 

Ps. I have downgraded to Paste &amp; PasteScript 1.7.2 (from 1.7.3 that gets installed with the build) . It would not run at all with 1.7.3 

Python mod versions I'm running:

/usr/local/lib/python2.6/dist-packages/pytz-2010h-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Pylons-0.9.6.2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Mako-0.3.2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/PIL-1.1.6-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/PasteDeploy-1.3.3-py2.6.egg
/usr/local/lib/python2.6/dist-packages/PyCAPTCHA-0.4-py2.6.egg
/usr/local/lib/python2.6/dist-packages/decorator-3.2.0-py2.6.egg
/usr/local/lib/python2.6/dist-packages/nose-0.11.3-py2.6.egg
/usr/local/lib/python2.6/dist-packages/FormEncode-1.2.2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/amqplib-0.6.1_devel-py2.6.egg
/usr/local/lib/python2.6/dist-packages/pycountry-0.12.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/python_cassandra-0.6.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/pycassa-0.1.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Cython-0.12.1-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/cssutils-0.9.5.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Routes-1.8-py2.6.egg
/usr/local/lib/python2.6/dist-packages/SQLAlchemy-0.5.3-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Routes-1.12.3-py2.6.egg
/usr/local/lib/python2.6/dist-packages/lxml-2.2.6-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/PasteScript-1.7.2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/pylibmc-1.0_reddit_04-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/Paste-1.7.2_reddit_0.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/boto-2.0a2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/BeautifulSoup-3.0.8.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/pycrypto-2.1.0-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/Paste-1.7.2-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Babel-0.9.5-py2.6.egg
/usr/local/lib/python2.6/dist-packages/Beaker-1.5.4-py2.6.egg
/usr/local/lib/python2.6/dist-packages/psycopg2-2.2.1-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/Thrift-0.2.0-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/WebHelpers-0.6.4-py2.6.egg
/usr/local/lib/python2.6/dist-packages/simplejson-2.1.1-py2.6-linux-i686.egg
/usr/local/lib/python2.6/dist-packages/chardet-1.0.1-py2.6.egg
/usr/local/lib/python2.6/dist-packages/flup-1.0.3.dev_20100525-py2.6.egg
/usr/local/lib/python2.6/dist-packages/py_interface-0.93-py2.6.egg


Thanks!",,False,,t5_2qizd,False,,,True,t3_crzum,http://www.reddit.com/r/redditdev/comments/crzum/problems_submitting_links_on_fresh_reddit_install/,
1275590988.0,4,self.redditdev,cb4fm,Can't stay logged in... (Java),5,1,8,http://www.reddit.com/r/redditdev/comments/cb4fm/cant_stay_logged_in_java/,"I hit up /api/login and get back a Set-Cookie with reddit_session=xxxx

I then do 

			String newCookie = conn.getHeaderField(""Set-Cookie"");
			cookie = newCookie;
			cookie = cookie.substring(0, cookie.indexOf("";""));

Then when I make a request, I do

		if (cookie != null) {
			System.out.println(""Setting cookie to "" + cookie);
			conn.setRequestProperty(""Cookie"", cookie);
		}

I'm printing out the right ""setting cookie to"" value, and when I look in Wireshark, it seems that I'm sending the right data over, but somehow I get back a "" ["".error.USER_REQUIRED""]]"" every time I try to post a comment using the API.

Relevant code: http://pastebin.com/YwRDrFzq",,False,,t5_2qizd,False,,,True,t3_cb4fm,http://www.reddit.com/r/redditdev/comments/cb4fm/cant_stay_logged_in_java/,
1274733607.0,6,self.redditdev,c7o03,"Would it be possible to use reddit logins to 
authentificate on a reddit clone?",8,2,16,http://www.reddit.com/r/redditdev/comments/c7o03/would_it_be_possible_to_use_reddit_logins_to/,"My first guess would be yes. Could you point me in the right direction before I'll give it a try? Thanks!

edit: I'm sorry for the misspelling ",,False,,t5_2qizd,True,,,True,t3_c7o03,http://www.reddit.com/r/redditdev/comments/c7o03/would_it_be_possible_to_use_reddit_logins_to/,
1273367826.0,6,self.redditdev,c1mad,I'm not sure if this is even the place to post this... (Question),7,1,7,http://www.reddit.com/r/redditdev/comments/c1mad/im_not_sure_if_this_is_even_the_place_to_post/,"Let's say hypothetically that I would like to start a reddit-driven site such as:

http://www.thecutelist.com

Where do I begin? Where do I go for help? What programs do I need to edit my new site?

I'm a completely beginner, so sorry for not giving many leads on an actual problem, but at least this may be extremely easy for some of you to help me with.
",,False,,t5_2qizd,False,,,True,t3_c1mad,http://www.reddit.com/r/redditdev/comments/c1mad/im_not_sure_if_this_is_even_the_place_to_post/,
1272641539.0,6,self.redditdev,bydv2,Trying to make a photo blog reddit ready,8,2,9,http://www.reddit.com/r/redditdev/comments/bydv2/trying_to_make_a_photo_blog_reddit_ready/,"I'm working on a photo blog and I'm wondering what's the algorithm that reddit use to retrieve thumbnails.

Any specific tags? Any way to make sure it always pick the same tag?

**EDIT:** I confirm that adding &lt;link rel=""image_src"" href=""${url}"" /&gt; works. However, reddit doesn't like ${url} being a relative URL. Changed it to absolute and it works fine now.

Thanks to ketralnis for the help. :)",,False,,t5_2qizd,True,,,True,t3_bydv2,http://www.reddit.com/r/redditdev/comments/bydv2/trying_to_make_a_photo_blog_reddit_ready/,
1272404094.0,5,self.redditdev,bwxq5,Updated sprite.png,6,1,5,http://www.reddit.com/r/redditdev/comments/bwxq5/updated_spritepng/,"I note that you have updated the sprite.png to make the video button a bit more conspicuous.  For the sake of consistency I have attempted to update the text button sprites.  [Imgur.com: Sprite Image](http://imgur.com/4uSGu.png)

Most of it was straight forward, but I didn't have the matching font, so there is a tiny (maybe six pixels total) difference there.  I also removed extra whitespace (four pixels per) from the corners of the updated video buttons.  ",,False,,t5_2qizd,False,,,True,t3_bwxq5,http://www.reddit.com/r/redditdev/comments/bwxq5/updated_spritepng/,
1270766753.0,5,self.redditdev,bodet,Unwanted Jailbait.,20,15,37,http://www.reddit.com/r/redditdev/comments/bodet/unwanted_jailbait/,"There's a gray bar along the top of Reddit that normally displays all the sub-reddits I visit. Great! However, today it's massively truncated and full of things I find rather disagreeable:

* Politics
* Reddit.com
* Jailbait

This last one, especially, is troublesome. I've _never_ visited this sub-reddit and I'd rather not have to explain to my wife that, honest to God, I don't dig kiddie porn.

What's up, Reddit? ",,False,,t5_2qizd,False,,,True,t3_bodet,http://www.reddit.com/r/redditdev/comments/bodet/unwanted_jailbait/,
1268526210.0,5,self.redditdev,bd2ux,How do I grab voting data off Reddit's API?,5,0,2,http://www.reddit.com/r/redditdev/comments/bd2ux/how_do_i_grab_voting_data_off_reddits_api/,"Hi all,
I want to have a look at the pattern of how redditors are voting per by_id. 

Let's say I have this by_id: t3_bcwip (GabeNewell as the rolling stone in Indiana Jones) - and I want to have a look at the voting patterns that this by_id has. [This](http://www.reddit.com/by_id/t3_bcwip.json) doesn't show anything granular about the votes

How do I do this? Is there something like digg.getAll method (don't crucify me for bringing up digg)? 

Thanks",,False,,t5_2qizd,False,,,True,t3_bd2ux,http://www.reddit.com/r/redditdev/comments/bd2ux/how_do_i_grab_voting_data_off_reddits_api/,
1266190645.0,4,self.redditdev,b21ri,Reddit Start to Finish Ubuntu - updates needed!,7,3,3,http://www.reddit.com/r/redditdev/comments/b21ri/reddit_start_to_finish_ubuntu_updates_needed/,"Hello redditdev, I've been having a hard time to put up my own instance of reddit!

the [Reddit Start to Finish Ubuntu](http://code.reddit.com/wiki/RedditStartToFinishIntrepid) wiki is kinda outdated, I suggest you to change the following:

* I had no problem using it with postgresql 8.4

* run setup.py this way: ( otherwise ""BeautifulSoup==3.0.7a"" won't be found )
      sudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/

* There is no need to setup separate databases, we can use only 1 by changing the example.ini (I don't know it there's a better way of doing it):
      main_db =        newreddit,   127.0.0.1, ri,   password
      comment_db =     newreddit,   127.0.0.1, ri,   password
      comment2_db =    newreddit,   127.0.0.1, ri,   password
      vote_db =        newreddit,   127.0.0.1, ri,   password
      change_db =      newreddit,   127.0.0.1, ri,   password
      email_db =       newreddit,   127.0.0.1, ri,   password
      authorize_db =   newreddit,   127.0.0.1, ri,   password
      award_db =       newreddit,   127.0.0.1, ri,   password


* added to example.ini:
     [DEFAULT]
       takedown_sr = 

* in the table *reddit data subreddit* I had to modify the col. author_id value from -1 to 1 and then restart memcached.. I think that the populate() function adds bad data to the database 
found this discussion: http://groups.google.com/group/reddit-dev/browse_thread/thread/19f3b894796f16d8 

* I had to edit a flag in 

      sudo vim /etc/default/memcached   #and set it to YES before starting memcached

* added a amqp server (rabbitmq-server and python-amqplib)",,False,,t5_2qizd,True,,,True,t3_b21ri,http://www.reddit.com/r/redditdev/comments/b21ri/reddit_start_to_finish_ubuntu_updates_needed/,
1264673658.0,5,self.redditdev,av5vg,The tab order in a reddit link list is strange,6,1,1,http://www.reddit.com/r/redditdev/comments/av5vg/the_tab_order_in_a_reddit_link_list_is_strange/,"If you go to a regular link listing such as the one you clicked to get here and you tab through it from the address bar, the tab order of the ""share"" button seems to be out of order. When you tab through, you first go through the share button for each link and then every other link on the page. I consider this a bug but couldn't find the correct place to submit it.

Update: You can even check this on this page, if you tab from the address field, you will go to ""share"", ""formatting help"" and then to the logical first link, ""pics"". It seems like tab order was not added to the share and formatting help links.",,False,,t5_2qizd,True,,,True,t3_av5vg,http://www.reddit.com/r/redditdev/comments/av5vg/the_tab_order_in_a_reddit_link_list_is_strange/,
1263602151.0,4,self.redditdev,aq7cs,Normalizing subreddits: How do you tiebreak the top posts from each subreddit?,5,1,2,http://www.reddit.com/r/redditdev/comments/aq7cs/normalizing_subreddits_how_do_you_tiebreak_the/,"It seems that the top post in each subscribed subreddit has a normalized hotness of 1. This means there is a tie between the first N posts on your front page (where N is the number of subscribed subreddits). How/where is this tie broken?

I'm looking at normalized_hot_cached in [normalized_hot.py](http://code.reddit.com/browser/r2/r2/lib/normalized_hot.py).
",,False,,t5_2qizd,False,,,True,t3_aq7cs,http://www.reddit.com/r/redditdev/comments/aq7cs/normalizing_subreddits_how_do_you_tiebreak_the/,
1262534759.0,5,self.redditdev,al2xj,What are the implications of a reddit update - 2008 to 2009,6,1,3,http://www.reddit.com/r/redditdev/comments/al2xj/what_are_the_implications_of_a_reddit_update_2008/,I've noticed there is a *newreddit* database. How can I update without losing any data?,,False,,t5_2qizd,False,,,True,t3_al2xj,http://www.reddit.com/r/redditdev/comments/al2xj/what_are_the_implications_of_a_reddit_update_2008/,
1262132773.0,5,reddit.com,ajpod,Is there an equivalent to about.json for subreddits?,5,0,1,http://www.reddit.com/r/redditdev/comments/ajpod/is_there_an_equivalent_to_aboutjson_for_subreddits/,,,False,,t5_2qizd,False,,,False,t3_ajpod,http://reddit.com/user/mindbrane/about.json,
1259110736.0,4,self.redditdev,a7v8k,question about reddit trac single sign on,6,2,2,http://www.reddit.com/r/redditdev/comments/a7v8k/question_about_reddit_trac_single_sign_on/,could someone point me in the direction of some docs about how to setup trac for single sign on like code.reddit.com?,,False,,t5_2qizd,False,,,True,t3_a7v8k,http://www.reddit.com/r/redditdev/comments/a7v8k/question_about_reddit_trac_single_sign_on/,
1259089199.0,6,self.redditdev,a7rsj,Does reddit's API support JSONP? Any chance of supporting it?,7,1,5,http://www.reddit.com/r/redditdev/comments/a7rsj/does_reddits_api_support_jsonp_any_chance_of/,"I hope this is the right subreddit to ask this. I am hoping to use jQuery to fetch reddit's json data in a mashup experiment. To be able to do a HTTP request outside of my domain, the server needs to support [JSONP convention as explained here](http://niryariv.wordpress.com/2009/05/05/jsonp-quickly/). Reddit does not seem to support this based on my experiments event though [api.pi](http://code.reddit.com/browser/r2/r2/controllers/api.py) has some code that indicates that it should in ApiController.response_func.

Searching on the internet it appears that many people setup a [proxy on their server](http://www.elctech.com/snippets/using-the-reddit-api-via-ruby).

Is anyone else able to use something like http://www.reddit.com/.json?callback=myFunc
Thanks!",,False,,t5_2qizd,False,,,True,t3_a7rsj,http://www.reddit.com/r/redditdev/comments/a7rsj/does_reddits_api_support_jsonp_any_chance_of/,
1257285090.0,5,reddit.com,a0n2z,Trailing &gt; should not be url-ized,6,1,0,http://www.reddit.com/r/redditdev/comments/a0n2z/trailing_should_not_be_urlized/,,,False,,t5_2qizd,False,,,False,t3_a0n2z,http://www.reddit.com/r/programming/comments/a0bvy/why_do_we_have_an_img_element/c0fapdj,
1253111414.0,4,self.redditdev,9l5i2,iReddit bugs and various features,7,3,7,http://www.reddit.com/r/redditdev/comments/9l5i2/ireddit_bugs_and_various_features/,"I use iReddit a lot, and I paid a good two or three minute's worth of salary for the privilege, so I DESERVE TO HAVE ALL OF MY DEMANDS MET, RAWR.  Totally kidding, I know that sometimes bug reports or enhancement requests can come off the wrong way.  I would do this myself if I had access to the source, but I don't, so.

First, a bug:

 * When scrolling to the bottom of the list, the ""Updating..."" banner pops up.  If you scroll down again while the banner is still up, it seems to kick off another request in the background, and when it finishes you will have two (or more) copies of the next page of stories added to the bottom of the list.  (My high score is currently like six or seven copies, but it doesn't save the score either -- is that another bug?)

Some assorted enhancement requests:

 * In the main story view, it's difficult to click on the comment button to go directly to the comments.  Could the active area for that button be made to take up the entire rightmost side of the list item?
 * It would be nice to be able to swipe left-to-right across a story in the main list view, the way you delete items in other list views -- in iReddit's case, to reveal a button to hide the story.  Especially the Glenn Beck stories.
 * There is an already-reported bug (I assume) about going to a specific comment within a story from your mailbox -- sometimes you'll click on the comment and be taken to a blank page.  Although this may actually have been a problem with the site instead of iReddit because now I can't actually reproduce the behavior in order to describe it accurately.
 * I'd like to be able to see profile pages via iReddit, especially my own, because I like to obsess over whether or not my latest comments are getting the proper number of upvotes.  YES I'M NEUROTIC, LEAVE ME ALONE.
 * When there are many (many [many]) comments on a story, the comments view doesn't show all of them, and it doesn't offer a way to load the comments that aren't displayed.  This is disconcerting when you're looking for a comment on a story that you know is present (because you're the author [NEUROSES]) and it's nowhere to be found.
 * Ideally I'd like the WebView-based comments view to be replaced with something more custom, except I don't know exactly what that would be.  It gets pretty unresponsive when there are tons of comments -- just highlighting a comment can sometimes cause a painful delay.
 * When clicking on a link from a comment, it should load in a new WebView and cause the breadcrumb arrow at the top left corner to add a context frame.  As it stands right now, you have to click on the little arrow in the web-browsing widget in the top right in order to get back to the comments, and if you're not paying attention and you click on the arrow in the top left, you go all the way back to the main story list.

I really dig the more recent improvements -- interpretation of Markdown, ability to browse a reddit you're not subscribed to, less crashy crashy, and so on -- these are just things I noticed and wanted to share.

Edit: More things I noticed!

 * Self-posts aren't getting their Markdown processed.  This post in particular looks hysterical.
 * Self-posts have the book/comment icon present in the bottom bar, but it doesn't do anything.  Maybe hide it for self-posts?

Edit: More things!  I'm getting down to pretty petty pitter patter at this point, but I wanted to share because I PAY YOUR SALARY, RAWR

 * Edited posts don't have a visual indicator like they do on the website.  This is not a huge deal, but probably wouldn't take much to add, except now that I look at the JSON feed it seems like the indicator is not present, so WTF SHIT'S BROKEN SOMEONE HAD BETTER FIXXIT
 * The current HTML-riffic comments view has very small hitboxes for the collapse and expand widget, and since it's right next to the username, it will be very easy to miss and go to the user page instead of collapsing the comments (edit before I actually finish this edit: I just realized that you can click anywhere on the line, not just on the teeny [-] icon, so NEVERMIND but I WON'T DELETE THIS, NOT EVER)
 * It would be nice to be able to upvote or downvote articles without clicking through, because I enjoy judging a book by its cover.  The same goes with comments -- it's kind of weird to have to select the comment and hit the teeny 'vote up' button underneath the comment box.  I think I just miss coloring in white H's, to be honest.
 * Also, if you switched to a custom view for the comments, then you could have a dedicated full-screen comment editing dealy, which might be nice, similar to how the Facebook app does comment editing (yes, I just admitted to both being a Facebook user and to being so addicted to Facebook that I use the iPhone app, PLEASE KILL ME) but then you might not be able to scroll through the list of comments, so I'm not sure this is a good idea.
 * Also, would it be possible to save the list of stories in the main view to some sort of permanent storage?  In low-memory situations, such as when you click on just about any story that's not a 16x16 pixel image on imgur, the entire story list appears to get reloaded from the server (apparently one page at a time), and sometimes the results don't match up with what *had* been there before the iPhone gave up trying to remember what it was doing a minute ago.  I don't know enough about the iPhone SDK to know whether or not using permanent storage like this is a good idea, but at least it would be faster than loading them over the net again.
 * And, hey, if you're putting them in storage, maybe you could dedupe them?  Due to the way Reddit works, when you request the 'next' page, you're actually requesting stories 'after' a specific story, but if that story moves in the ordering that you've requested, then you can wind up with stories you've already seen.  On the web, there's not much you can do about this since you don't have enough context to remove the duplicates, but since the story list is being downloaded to the phone, you can just compare the story ids to the story ids you've already got in the list, and just kick out the duplicates.
 * Also, and I REALIZE I AM BEING KIND OF HYPERACTIVE AND I NEED TO TAKE MY MEDICATION BUT BEAR WITH ME PLEASE, it would be neat if there was a persistent envelope widget in the upper right that would turn orangered when you had a new reply, and that took you directly to your mailbox.  Also, push notifications, but I realize that's not as easy as it is to type out ""push notifications"".
 * Also, search would be nice.

Also also, I have about as much free time as a mother of sixteen who runs a large corporation and plays professional hockey, but I also have some obj-c chops, and if you're interested I could try to put together a comments view if you wanted to move away from the WebView that you're using now.  I don't have the iPhone SDK but I could just do it using desktop-style Cocoa and let you tidy it up when it's done.

ALSO IS THERE A REWARD FOR BEING THE MOST ANNOYING USER OF iREDDIT BECAUSE I THINK I MAY HAVE JUST WON IT

Another edit, because I CANNOT STOP, SOMEONE SEDATE ME PLEASE:

 * The ability to edit and delete comments would be nice.",,False,,t5_2qizd,True,,,True,t3_9l5i2,http://www.reddit.com/r/redditdev/comments/9l5i2/ireddit_bugs_and_various_features/,
1376190226.0,4,self.redditdev,1k4ibk,What's the best way to batch user requests against the reddit API,4,0,2,http://www.reddit.com/r/redditdev/comments/1k4ibk/whats_the_best_way_to_batch_user_requests_against/,"I'm writing a program in go for my server that will act as the authentication client for whatever users login to my site. It needs to be able to vote, and comment on behalf of the user that sort of thing. Right now when a user submits a request it goes into a queue that's executed separately and it appears to the user that their comment and vote was made, even if it hasn't yet, and I defer the post so that I don't exceed my request limit of one request per 2 seconds. I was wondering how i can optimize things so that even if I'm getting a lot of user requests I won't be deferring these posts to the point that there's significant lag. What sort of batch functionality exists for this?",,False,,t5_2qizd,False,,,True,t3_1k4ibk,http://www.reddit.com/r/redditdev/comments/1k4ibk/whats_the_best_way_to_batch_user_requests_against/,
1375866023.0,5,self.redditdev,1jvfik,Reddit Clone authenticate with LDAP,7,2,12,http://www.reddit.com/r/redditdev/comments/1jvfik/reddit_clone_authenticate_with_ldap/,"I have been looking for a way to have a reddit clone that would authenticate users against LDAP (Active Directory). 
This is because that it will  be something for work and they would like to not have anonymous users. I figure the best way would be to pull the users info from LDAP. At least for authentication.
Has anyone done this? If not does anyone have a place to start looking of what could need to be changed to allow this to work?",,False,,t5_2qizd,False,,,True,t3_1jvfik,http://www.reddit.com/r/redditdev/comments/1jvfik/reddit_clone_authenticate_with_ldap/,
1375099493.0,4,self.redditdev,1j9s9m,Mentions of other subreddits,5,1,5,http://www.reddit.com/r/redditdev/comments/1j9s9m/mentions_of_other_subreddits/,"Is there anyway to look through one subreddit to see what other subreddits have been mentioned? Either in the comments or by crossposted submissions

I basically want to find other related subreddits for a subreddit that I already know that I like.

For example is I enjoy /r/baseball then there would probably be mentioned of /r/redsox and /r/yankees in the comments and links.

Is there a website or existing bot that does this already?",,False,,t5_2qizd,False,,,True,t3_1j9s9m,http://www.reddit.com/r/redditdev/comments/1j9s9m/mentions_of_other_subreddits/,
1374850914.0,5,twitter.com,1j3lx1,Web accessibility of Reddit?,5,0,3,http://www.reddit.com/r/redditdev/comments/1j3lx1/web_accessibility_of_reddit/,,,False,,t5_2qizd,False,,,False,t3_1j3lx1,https://twitter.com/patconnolly/status/360776497440632832,
1374784509.0,3,self.redditdev,1j1v91,Using OAuth to pull authenticated user's votes,4,1,0,http://www.reddit.com/r/redditdev/comments/1j1v91/using_oauth_to_pull_authenticated_users_votes/,"I have a php authentication system set up on a server of mine (super basic right now, just lets me connect my app to my reddit account and then spits out the basic /me.json page). I was wondering if it is possible to use the access token to then pull my voting history, i.e. see all the posts which I have upvoted? Thanks!",,False,,t5_2qizd,False,,,True,t3_1j1v91,http://www.reddit.com/r/redditdev/comments/1j1v91/using_oauth_to_pull_authenticated_users_votes/,
1374709743.0,2,self.redditdev,1izqfy,Help with installing PRAW,4,2,9,http://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/,"Okay, so, I would like help with installing PRAW. At this point, I need step-by-step instructions.

I went on the PRAW website, and it said that the reccomended way to install PRAW was to use ""pip"".

I went and tried installing pip manually, but it didn't work well. So I decided to go on the website to find installation instructions. It said to use ""virtualenv"" to install pip.

On the virtualenv website, it says to use pip to install virtualenv.

So, what is the easiest, quickest way to install PRAW? At this point all I have is Python 3.3.",,False,,t5_2qizd,False,,,True,t3_1izqfy,http://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/,
1373921311.0,6,self.redditdev,1id53y,[PRAW] How to get the full text of a message?,6,0,5,http://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/,"I can't seem to find anything in the PRAW docs detailing a variable that holds the text of a message retrieved with get_unread(). I tried selftext in hopes that would work, as it was the closest I could find, but it didn't work. What is the magic variable?",,False,,t5_2qizd,False,,,True,t3_1id53y,http://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/,
1373510959.0,4,self.redditdev,1i1zez,How to check if specific url was submitted to reddit,4,0,2,http://www.reddit.com/r/redditdev/comments/1i1zez/how_to_check_if_specific_url_was_submitted_to/,"So i want to check if there is a discussion for a link (which is shown by my own rss reader). I parse the relevant data from this.     *http://www.reddit.com/api/info.json?url=$lookforthisurl*

My problem is that the url can look like domain.com/article or domain.com/article/ or domain.com/article/?somestuff

Even if all links are the same article the api may show that there is no submission for domain.com/article/?somestuff, even if there is one for domain.com/article

Is there some way to get the discussions to a link without querying all possible formats of that link?",,False,,t5_2qizd,False,,,True,t3_1i1zez,http://www.reddit.com/r/redditdev/comments/1i1zez/how_to_check_if_specific_url_was_submitted_to/,
1373394247.0,5,self.redditdev,1hy7xd,[PRAW] Looking for some help creating a bot to enable a user editable active livestream list.,5,0,6,http://www.reddit.com/r/redditdev/comments/1hy7xd/praw_looking_for_some_help_creating_a_bot_to/,"Over in /r/battlefield we want to add a livestream list in a drop down menu, like the menus already visible there.

The idea is that we'll keep a wiki page full of users activated for this functionality. Those users would then add ""ON"" or ""OFF"" to their flair tag, the bot would check the flairs of the users on the wiki page and then edit the page to show them in a drop down menu. Either ""upcoming"" or ""established"", dependent on whether people just generally stream for fun or stream as a major name in the community.

I'm willing to take guidance on creating this but if there's anyone out there with the capability that's also interested in the community we'd really be interested in adding a new moderator, there's several other features that we'd like down the line too and that would obviously be a more ideal scenario. Deimorz suggested someone might be interested.

Can anyone help out, or is anyone interested?",,False,,t5_2qizd,False,,,True,t3_1hy7xd,http://www.reddit.com/r/redditdev/comments/1hy7xd/praw_looking_for_some_help_creating_a_bot_to/,
1373323021.0,4,self.redditdev,1hw8ge,"How are ""modes"" handled in the API?",4,0,1,http://www.reddit.com/r/redditdev/comments/1hw8ge/how_are_modes_handled_in_the_api/,"Hi!
I'd like to know whether it's possible to filter entries by text flair as it's done in some subreddits (eg. starcraft). Is there a way to filter by link flairs arbitrarily through the API, or are modes preset?
 I would also be interested in knowing how this works ; does it use the fulltext search system or is it an internal query?

Thanks,",,False,,t5_2qizd,False,,,True,t3_1hw8ge,http://www.reddit.com/r/redditdev/comments/1hw8ge/how_are_modes_handled_in_the_api/,
1373053656.0,3,self.redditdev,1hpicu,"What's this ""syntax=cloudsearch"" do?",7,4,10,http://www.reddit.com/r/redditdev/comments/1hpicu/whats_this_syntaxcloudsearch_do/,"I couldn't help but notice on my traffic report:

&gt; http://www.reddit.com/r/MusicGuides/search?sort=top&amp;q=timestamp%3A1372982400..1373068800&amp;restrict_sr=on&amp;syntax=cloudsearch

What is that thing I said?",,False,,t5_2qizd,False,,,True,t3_1hpicu,http://www.reddit.com/r/redditdev/comments/1hpicu/whats_this_syntaxcloudsearch_do/,
1372783824.0,5,self.redditdev,1hi8no,Is there a way to grab the title and url from the top submissions in a sub using JSON?,6,1,6,http://www.reddit.com/r/redditdev/comments/1hi8no/is_there_a_way_to_grab_the_title_and_url_from_the/,Im trying     http://www.reddit.com/r/redditdev.json?sort=top but its returning too much data in the JSON. I just need the url and and title         ,,False,,t5_2qizd,False,,,True,t3_1hi8no,http://www.reddit.com/r/redditdev/comments/1hi8no/is_there_a_way_to_grab_the_title_and_url_from_the/,
1372523920.0,4,self.redditdev,1hbejp,Is it possible to load only some comments at a time to improve loading speeds?,7,3,1,http://www.reddit.com/r/redditdev/comments/1hbejp/is_it_possible_to_load_only_some_comments_at_a/,,,False,,t5_2qizd,False,,,True,t3_1hbejp,http://www.reddit.com/r/redditdev/comments/1hbejp/is_it_possible_to_load_only_some_comments_at_a/,
1372520925.0,3,self.redditdev,1hbbnz,"API : get JSON of the third page, skipping second one",5,2,4,http://www.reddit.com/r/redditdev/comments/1hbbnz/api_get_json_of_the_third_page_skipping_second_one/,"Hi there, 

Let's say I'm trying to get to the 3rd page of a subreddit, skipping the second one. Is there a way to do that ?

The only way i see to navigate through pages, is by using the 'after' argument provided by the JSON. But that means you have to load all preceding pages before you can get to a specific page.

ps: I'm using Javascript. 
",,False,,t5_2qizd,False,,,True,t3_1hbbnz,http://www.reddit.com/r/redditdev/comments/1hbbnz/api_get_json_of_the_third_page_skipping_second_one/,
1372487213.0,4,self.redditdev,1has5q,(PRAW) How do you add moderators with non-full permissions?,5,1,1,http://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/,"So, if you say e.g. `subreddit.add_moderator(""someusername"")`, then it adds that user as a moderator to the subreddit (assuming you're logged in as someone with sufficient permissions).

How do you restrict permissions upon adding a moderator? E.g., suppose we want to add only ""mail"" permissions. I couldn't figure out how to do this from [reading the docs](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.add_moderator).

Thanks",,False,,t5_2qizd,False,,,True,t3_1has5q,http://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/,
1371926329.0,5,self.redditdev,1gv8g1,Rate limiting on commenting?,6,1,2,http://www.reddit.com/r/redditdev/comments/1gv8g1/rate_limiting_on_commenting/,"Can't really find anyone else having this issue. I am using this ruby library for reddit api wrapper: https://github.com/paradox460/snoo

It seems when I comment only the first one goes through, and any others fail. It does not seem to return a message, but I am guessing it's because of the message that only comment so much in so much time. How can I get around that? Or is something else causing it? Thanks.",,False,,t5_2qizd,False,,,True,t3_1gv8g1,http://www.reddit.com/r/redditdev/comments/1gv8g1/rate_limiting_on_commenting/,
1370835202.0,4,self.redditdev,1g0umc,How long is a modhash good for? Is it a one time use deal or can the same one be used multiple times?,4,0,4,http://www.reddit.com/r/redditdev/comments/1g0umc/how_long_is_a_modhash_good_for_is_it_a_one_time/,"Instead of wasting API calls to me.json to get a modhash, can the same one be used for X minutes?  

How does that work?
",,False,,t5_2qizd,False,,,True,t3_1g0umc,http://www.reddit.com/r/redditdev/comments/1g0umc/how_long_is_a_modhash_good_for_is_it_a_one_time/,
1370679017.0,3,self.redditdev,1fx1qr,Running a reddit bot on my router.,8,5,4,http://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/,"I'm not sure if this is the best subreddit, but I've seen similar posts here so I guess I'll try it. If there is a better subreddit, please point me to it.

I wanted to run /u/AndrewNeo's [groompbot](https://github.com/AndrewNeo/groompbot). However, I don't have a 24/7 server to run it on. But then it hit me. I actually do. My router.

I have ASUS RT-N66U running [Toastman](http://toastmanfirmware.yolasite.com/)'s [TomatoUSB](http://tomatousb.org/) and it is super rock solid and quite beefy in terms of specs. It also includes [busybox](http://www.busybox.net/). I added [Entware](https://code.google.com/p/wl500g-repo/) and added Python 2.7.3.

So, now I'm stuck. I can SSH into the router and run Python, but past that I can't figure out how to install the dependencies ([praw](https://github.com/praw-dev/praw/) and [gdata](https://code.google.com/p/gdata-python-client/)). They call for [pip](https://pypi.python.org/pypi/pip), but I can't figure out how to install that either. I've read all the documentation and most of it goes over my head.

Can anyone point me in the right direction?

If I'm able to successfully figure this out, I plan to put together a little tutorial/guide about running reddit bots on routers, just to kind of give back to the community and consolidate the knowledge for next person who stumbles on this while Googling.",,False,,t5_2qizd,False,,,True,t3_1fx1qr,http://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/,
1370485113.0,3,self.redditdev,1frj0r,Can't compose PM with ReddiWrap?,5,2,2,http://www.reddit.com/r/redditdev/comments/1frj0r/cant_compose_pm_with_reddiwrap/,"I've been trying to send a PM through ReddiWrap, and I'm having trouble getting its cooperation... When I use the following code, nothing sends:

    from ReddiWrap import ReddiWrap
    
    reddit = ReddiWrap(user_agent='ReddiWrap')
    
    USERNAME = 'Username'
    PASSWORD = 'Password'
    
    reddit.load_cookies('cookies.txt')
    
    if not reddit.logged_in or reddit.user.lower() != USERNAME.lower():
    	print('logging into %s' % USERNAME)
    	login = reddit.login(user=USERNAME, password=PASSWORD)
    	if login != 0:
    		print('unable to log in: %d' % login)
    		print('remember to change USERNAME and PASSWORD')
    		exit(1)
    	reddit.save_cookies('cookies.txt')
    
    pm = reddit.get('/message/compose')
    msg_recipient = 'Username'
    subject_line = 'Here is a sample subject.'
    text_body = 'Here is a sample text body.'
    
    reddit.compose(msg_recipient, subject_line, text_body)    

Any idea where I'm hitting a snag?",,False,,t5_2qizd,False,,,True,t3_1frj0r,http://www.reddit.com/r/redditdev/comments/1frj0r/cant_compose_pm_with_reddiwrap/,
1370395197.0,5,self.redditdev,1fowzz,Does praw support getting the modmail from one particular subreddit?,5,0,3,http://www.reddit.com/r/redditdev/comments/1fowzz/does_praw_support_getting_the_modmail_from_one/,"I see there's `Reddit.get_modmail()`, but that seems to get all modmail.",,False,,t5_2qizd,False,,,True,t3_1fowzz,http://www.reddit.com/r/redditdev/comments/1fowzz/does_praw_support_getting_the_modmail_from_one/,
1370135500.0,6,self.redditdev,1fhtsf,"Site still shows unread messages, even after marking messages read with API.",6,0,2,http://www.reddit.com/r/redditdev/comments/1fhtsf/site_still_shows_unread_messages_even_after/,"When a user reads a new message, I call the mark_unread api call. 
However when I go on reddit using my computer browser, it shows the orange envelope indicating there are unread messages. When I click on it, it takes me to my inbox, which also shows no unread messages. So all my messages are successfully being marked as read, however the site doesn't seem affected until after I navigate to my inbox.
Is there another API I need to call on? Or is this just how its supposed to behave? Thanks!

edit: Same issue as here - http://www.reddit.com/r/redditdev/comments/1022k3/using_the_api_to_mark_a_message_as_read_doesnt/",,False,,t5_2qizd,1370135683.0,,,True,t3_1fhtsf,http://www.reddit.com/r/redditdev/comments/1fhtsf/site_still_shows_unread_messages_even_after/,
1369938834.0,6,self.redditdev,1fcolg,m.mark_as_read() is broken since this morning - known/old issue?,6,0,23,http://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/,"I run /r/ALTcointip bot, and beginning this morning, bot began processing the messages in its inbox multiple times. It seems PRAW m.mark_as_read() call doesn't have proper effect. I'm also seeing this issue through browser, where messages in my inbox stay unread even though I've already clicked on them.",,False,,t5_2qizd,False,,,True,t3_1fcolg,http://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/,
1369846230.0,3,self.redditdev,1fa01t,Need help testing back-end API for redditanalytics.,6,3,4,http://www.reddit.com/r/redditdev/comments/1fa01t/need_help_testing_backend_api_for_redditanalytics/,"I am creating an API that will allow developers to retrieve submissions and comments far faster than by using Reddit itself.  I am also creating a ""comment stream"" that is similar to Twitter's streaming API.

The API is located at http://api.redditanalytics.com/

Now, here's what it can currently do:

It currently has around 25 million submissions and around 250 million comments.  The submission API is in alpha and I need some testers.  

Here's how you use it:

**Required GET variables:**

*key* (set this to anything right now)

**Optional GET variables:**

*beforedate* Retrieve (up to limit) submissions in descending order.  This can be set to two different values.  You may either use unixtime or YYYY-MM-DD HH:MM:SS.  Keep in mind that the second option is in UTC time (not your time-zone, unless you're in UTC :)  This cannot be used at the same time as beforeid 


*limit* Return X results.  Maximum value is currently 50.  Once we get out of alpha I will increase it.  


*beforeid*  The same as before date, but using submission ID's.  (i.e. t3_z2317)  This cannot be used at the same time as beforedate.


*subreddit*  If present, only return submissions from that subreddit.  This can be used in conjunction with either beforeid or beforedate


**Example:  You want to scrape everything from a specific subreddit from most recent working backwards**

You want to retrieve only submissions from askreddit starting with the most recent and working back.

First call your program makes will be to:
http://api.redditanalytics.com/?limit=50&amp;subreddit=askreddit&amp;key=pleasework

The return will be in JSON and will have two variables that you will be interested.  The variable ""data"" is an array with all of your results (in the same format that reddit provides them).  The second variable is ""info"" and it will return another beforeid to you.  You can then pass:

http://api.redditanalytics.com/?limit=50&amp;subreddit=askreddit&amp;key=pleasework&amp;beforeid=&lt;whatever beforeid was returned to you&gt;

You can put that in a loop to scrape all the submissions from a particular subreddit.


**Example:  You want to get submissions from before UTC April 1, 2013 at noon**

http://api.redditanalytics.com/?limit=50&amp;key=yaddayadda&amp;beforedate=2013-04-01%2012:00:00 (%20 is URI encoding for a space)





Groundrules:

**Please don't destroy my server.**  It is a piece of shit cloud server with 1 gig of ram  and a one core processor.  If I get funding, I will get a better server that will be much faster.  Please realize that the server may be slow to return results at times.

Please limit your requests to 2 a second.  If you serialize your requests and go above 2 a second, I won't mind.  The point is to wait until one request is completed before you make another.  Please don't make parallel requests.  

If the server is unavailable, it has probably crashed.  Let me know if this happens (or if I see it happen, I will correct it).  

I have set the server to FORCE deflating the returned data which means your client should expect to handle the compression correctly (most do).  I am doing this because I have limited outbound bandwidth and I have no idea how much data will end up going out (knock on wood). 

If you have any questions, feel free to ask.  I will be including more GET attributes like author, q (keyword search) and others in the coming days / weeks.  I will also be putting together the comment API if I get more funding.

Thanks!",,False,,t5_2qizd,1369847732.0,,,True,t3_1fa01t,http://www.reddit.com/r/redditdev/comments/1fa01t/need_help_testing_backend_api_for_redditanalytics/,
1369303330.0,4,self.redditdev,1ew8mb,Websockets are the way to go -- case study.,4,0,0,http://www.reddit.com/r/redditdev/comments/1ew8mb/websockets_are_the_way_to_go_case_study/,"For my [Reddit analytics project](http://goo.gl/kVJMx), I was using Ajax calls to update the information and graphs on the main screen.  Unfortunately, this calls for making a request each second to the back-end server.  While the back-end server can handle that, there is latency involved during the round-trip request for data retrieval.  

Also, there appears to be some type of memory leakage with some browsers when tons of ajax requests are made over an hour (3,600 to be exact).  

After switching to Websockets, the application runs much smoother both on the client-side and the back-end.  I am using Mojolicious for Perl as the websocket server on the back-end.  With one processor, it is able to handle around 100 open connections simultaneously while still managing to sort and process the Reddit stream.

If anyone has any questions on how I set this up, please ask!  I like to share with the community.
",,False,,t5_2qizd,False,,,True,t3_1ew8mb,http://www.reddit.com/r/redditdev/comments/1ew8mb/websockets_are_the_way_to_go_case_study/,
1368630922.0,2,self.redditdev,1edwxi,Need help with sidebar bot creation.,4,2,8,http://www.reddit.com/r/redditdev/comments/1edwxi/need_help_with_sidebar_bot_creation/,"I recently became a moderator for /r/redsox and would like to create a bot that would be able to fetch/receive MLB W-L record and current divisional standings information and update it within the sidebar. I have minimal coding experience, but am willing to do the groundwork to learn how to do it. I just need some help/advice on where to start/where to look, suggestions, and if it is even possible. Thanks in advance for your help!

P.S. This seemed like the best subreddit to post this question to. If there is a better for this type of question please let me know and I will post there instead. Thanks!",,False,,t5_2qizd,False,,,True,t3_1edwxi,http://www.reddit.com/r/redditdev/comments/1edwxi/need_help_with_sidebar_bot_creation/,
1367881184.0,5,self.redditdev,1dto9f,[PRAW] Best way to alter a bot's original comment based on a user's reply?,10,5,3,http://www.reddit.com/r/redditdev/comments/1dto9f/praw_best_way_to_alter_a_bots_original_comment/,"I want to be able to automatically change (either edit or delete) a comment made by a bot based on a user's reply to that comment. I tried looping through all the replies of every comment but that took way to long. I also thought about checking the bot's inbox and finding a matching reply and then finding the parent of that reply, but I don't know of a way of getting a comment's parent. Does anyone have any ideas? ",,False,,t5_2qizd,False,,,True,t3_1dto9f,http://www.reddit.com/r/redditdev/comments/1dto9f/praw_best_way_to_alter_a_bots_original_comment/,
1367140643.0,5,self.redditdev,1d9jli,Is Google Appengine Blocked permanently?,6,1,1,http://www.reddit.com/r/redditdev/comments/1d9jli/is_google_appengine_blocked_permanently/,"Hi

As RBLSTR also [noticed](http://www.reddit.com/r/redditdev/comments/1d4018/http_429_with_gae/c9nar53) , all GAE calls to the API are rejected with error:429 response. My code was working fine just couple of days ago and now I am blocked. As far as I am concerned GEA sends user agent in this form : ""AppEngine-Google; (+http://code.google.com/appengine; appid: APPID)"", where APPID is your app's identifier. you can try any .json page in a browser and it fails too.",,False,,t5_2qizd,False,,,True,t3_1d9jli,http://www.reddit.com/r/redditdev/comments/1d9jli/is_google_appengine_blocked_permanently/,
1366874843.0,3,self.redditdev,1d2jnw,comments on post are not fetched/shown unless rabbit-mq is running,4,1,5,http://www.reddit.com/r/redditdev/comments/1d2jnw/comments_on_post_are_not_fetchedshown_unless/,"The problem was encountered recently when the default port of rabbit-mq was changed mistakenly and reddit was restarted (our own instance). In such case, writing the comments was possible on any post as usual however on a page refresh (on the same post) they disappeared. 

On the default views (/r/all, /r/anysubreddit) it was showing the correct number of comments for a post but when clicked on it, there were not comments and also in red text (nothing here). 

I figured that only those comments were fetched by reddit which were submitted using reddit-mq. Because of this, many comments are not viewed anymore on our instance. After the port number was fixed and reddit restarted, things worked fine only for new posts and new comments however still old comments are not visible. Help will be highly appreciated :)",,False,,t5_2qizd,False,,,True,t3_1d2jnw,http://www.reddit.com/r/redditdev/comments/1d2jnw/comments_on_post_are_not_fetchedshown_unless/,
1366812202.0,4,self.redditdev,1d0cln,PRAW store submission data,4,0,4,http://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/,"Hey there!

I am currently trying to permanently store submission objects to have later access to them. I also want to leave the option open to later easily have possibilities to retrieve further data like comments or user infos.

My code looks the following way:

    import praw
    import datetime   
 
    r = praw.Reddit('void')
    
    submissions_per_day = list()
    curr_date = None
    i = 0
    
    for submission in r.get_subreddit('all').get_new(limit=None):
        date = datetime.datetime.fromtimestamp(int(submission.created_utc)).strftime('%Y-%m-%d')
        submissions_per_day.append(submission) #or vars(submission)
        if i == 0:
            curr_date = date
        
        if date != curr_date:
            SOMEHOW STORE THE LIST OF SUBMISSION OBJECTS

So my goal is to get all submissions of one day store them, then for the next day and store them again. I have tried various things like the most obvious cPickle solution. But this is very slow, even when just crawling a few submissions and using the newest protocol.

I have also tried to only take the dictionary of the object (i.e. vars(submission)) without any improvements. I know that I could just go ahead and manually parse the object and e.g. store the values to a csv file. But this seems so odd to me and I want a more fluent solution. I could also think about getting the json objects and storing them. Another solution would be to use databases but this is also not my prefered method.

I hope anyone of you can give me some hints of how to cope with my problems. 

Thanks, 
Philipp",,False,,t5_2qizd,False,,,True,t3_1d0cln,http://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/,
1366630607.0,3,self.redditdev,1cuw5e,Testing question,4,1,3,http://www.reddit.com/r/redditdev/comments/1cuw5e/testing_question/,"I'm in the process of writing my own C# wrapper for the reddit API and I was wondering if there was any test service I could call to check if my API calls are correct?

I want to do a full C# implementation of the API and it seems kinda pointless for me to create a subreddit just for testing things like moderation / posting etc. Also there are a few API calls for the wiki that I would like to have in my wrapper but Id prefer if there was a nicer way to test that than actually making wiki changes etc.",,False,,t5_2qizd,False,,,True,t3_1cuw5e,http://www.reddit.com/r/redditdev/comments/1cuw5e/testing_question/,
1365637588.0,5,self.redditdev,1c3l7p,get link to image instead of its page?,7,2,6,http://www.reddit.com/r/redditdev/comments/1c3l7p/get_link_to_image_instead_of_its_page/,"I'm writing a reddit viewer in Qt5. I can get the json for any subreddit, parse it with QJsonDocument, and list its content. Since I write it especially for viewing subreddits with mostly pictures (/r/pics or /r/AdviceAnimals), I want to get the images directly. but many links just point to a website containing the image.
Is there a way to get to the image instead or do I have to add each domain with its rules separately, meaning one for imgur.com and one for quickmeme.com and so on?

Edit: looks like I'll have to code it per domain. I'll upload the code when it gets usefull to others.
Thanks for all replies!",,False,,t5_2qizd,1365691409.0,,,True,t3_1c3l7p,http://www.reddit.com/r/redditdev/comments/1c3l7p/get_link_to_image_instead_of_its_page/,
1365384945.0,4,self.redditdev,1bw0vk,having issues retrieving subreddits,4,0,4,http://www.reddit.com/r/redditdev/comments/1bw0vk/having_issues_retrieving_subreddits/,"i am using similar code to this guys blog post: http://blog.tankorsmash.com/?p=378

to retrieve submissions to a subreddit, and i get around the 100 limit limitation by using the after with the ID as part of the function call.  it works and retrieves the subreddits 100 at a time, but then after going so far back it doesnt return anything.  I tried this with 2 different subreddits, and it seems it only goes back 998 submissions.

is this some sort of limitation? is there a way to get around it?

here is some of my code:
def subredditInfo(client, limit=25, sr='urbanplanning',myAfter='t3_12so4w',
                  sorting='listing', return_json=False, **kwargs):

parameters = {'limit': 1,'after': myAfter}
    parameters.update(kwargs)
 
    url = r'http://www.reddit.com/r/{sr}/{top}.json'.format(sr=sr, top=sorting)
    r = client.get(url,params=parameters)
    j = json.loads(r.text)
",,False,,t5_2qizd,False,,,True,t3_1bw0vk,http://www.reddit.com/r/redditdev/comments/1bw0vk/having_issues_retrieving_subreddits/,
1364583062.0,4,self.redditdev,1b99xb,Need some help on resource planning,4,0,0,http://www.reddit.com/r/redditdev/comments/1b99xb/need_some_help_on_resource_planning/,"I need to setup a live instance of Reddit for a school project and need some help on resource planning. In the real world, how is this normally done? How far can I get with 4gb of ram and access to 4 cores@3.3ghz? As in pageviews/min? I'll take any advice I can get on maximizing my limited resources as well. Thanks!",,False,,t5_2qizd,False,,,True,t3_1b99xb,http://www.reddit.com/r/redditdev/comments/1b99xb/need_some_help_on_resource_planning/,
1364066934.0,3,self.redditdev,1avgfe,Larger thumbnails?,8,5,2,http://www.reddit.com/r/redditdev/comments/1avgfe/larger_thumbnails/,"Hi all,

I've noticed some of reddit's thumbnails are larger than others. How exactly can you convert the smaller thumbnails into larger ones?

Thanks!",,False,,t5_2qizd,False,,,True,t3_1avgfe,http://www.reddit.com/r/redditdev/comments/1avgfe/larger_thumbnails/,
1363682058.0,3,self.redditdev,1akzxt,Front page not updating without server restart,5,2,6,http://www.reddit.com/r/redditdev/comments/1akzxt/front_page_not_updating_without_server_restart/,"Hey,

I am having an issue with the front page on my clone. The content is not being update as it should be, it shows some older posts and newer one with much much more karma is not being showed. I tried this yesterday, created a lot of test posts and the front page was static. I gave it a lot of time, until this morning, and still nothing changed.

I tried restarting cron jobs, running them manually, consumers, rebooting server, but nothing helped.

The only way I managed it to update was to restart the whole server, and that updated it, but its now ""stuck"" with that content. I checked the connectivity to memcached, amqp, cassandra, postgresql, everything is working. /var/syslog shows no errors as all, only consumers which are working correctly. Everything else seems to be working as it should, only that front page remains an issue.

Thanks in advance for your time.",,False,,t5_2qizd,False,,,True,t3_1akzxt,http://www.reddit.com/r/redditdev/comments/1akzxt/front_page_not_updating_without_server_restart/,
1362986907.0,2,self.redditdev,1a2kl3,"How hard would it be to create a ""cluster"" feature, where instead of being limited to reading one subreddit at a time, you could view the content of multiple subreddits in one space",6,4,7,http://www.reddit.com/r/redditdev/comments/1a2kl3/how_hard_would_it_be_to_create_a_cluster_feature/,"Let's say I wanted to read about programming stuff.  I may put /r/programming/, /r/javascript, /r/coding, /r/webdev, etc in the cluster.  So when reading the cluster, I would get all my programming reading as opposed to having to go to each subreddit or read the front page with every other subreddit.

I am a pretty experienced developer but have never looked at reddit's codebase.  Thoughts?",,False,,t5_2qizd,False,,,True,t3_1a2kl3,http://www.reddit.com/r/redditdev/comments/1a2kl3/how_hard_would_it_be_to_create_a_cluster_feature/,
1362307333.0,5,self.redditdev,19klsk,Wipe all data?,6,1,5,http://www.reddit.com/r/redditdev/comments/19klsk/wipe_all_data/,"Hey,

Can anyone point to the best way of dumping all of the data from clone to start of with fresh clone? I made some modifications to the code, I wouldn't like to lose those, and wouldn't like to mess up the database with manual deletion/dropping of tables. Also, I am worried about the cassandra, memcached and amqp, how to deal with those?

Thanks!",,False,,t5_2qizd,False,,,True,t3_19klsk,http://www.reddit.com/r/redditdev/comments/19klsk/wipe_all_data/,
1362122673.0,3,self.redditdev,19g9u5,Recent posts from a specific user (me),4,1,2,http://www.reddit.com/r/redditdev/comments/19g9u5/recent_posts_from_a_specific_user_me/,"Sorry if this is an extremely basic question, but my google-fu seems to be a bit off and I figured someone might be willing to throw me a bone here.

So I am attempting to pull my most recent posts using PHP, so that I can summarize and provide links/pages of that in a widget on my personal site. I am unable to figure out a way to do this...so far the closes I've gotten at the moment is to read http://www.reddit.com/user/reflectiveSingleton/comments.json and use that data...the stopgap issue I am running into is that data lacks the permalink url, and I am unable to figure out a way to compute or somehow query to get that.

Any ideas?",,False,,t5_2qizd,False,,,True,t3_19g9u5,http://www.reddit.com/r/redditdev/comments/19g9u5/recent_posts_from_a_specific_user_me/,
1361560353.0,4,self.redditdev,191bn8,"Does PRAW support the ""before"" field for getting content?",6,2,10,http://www.reddit.com/r/redditdev/comments/191bn8/does_praw_support_the_before_field_for_getting/,"I know   `subreddit.get_comments(after_field=thing_id)` works but it seems like I'd have to specify `before` as an extra GET argument using:

`subreddit.get_comments(params ={'before':before})`

Is this an oversight, or am I missing something?",,False,,t5_2qizd,False,,,True,t3_191bn8,http://www.reddit.com/r/redditdev/comments/191bn8/does_praw_support_the_before_field_for_getting/,
1361507444.0,3,self.redditdev,19047d,How would I go about mod boting,6,3,1,http://www.reddit.com/r/redditdev/comments/19047d/how_would_i_go_about_mod_boting/,I found some code for a mod bot I just don't know how to use it could someone help?,,False,,t5_2qizd,False,,,True,t3_19047d,http://www.reddit.com/r/redditdev/comments/19047d/how_would_i_go_about_mod_boting/,
1361053562.0,2,self.redditdev,18nr51,"I've used the Reddit API on my website, gingerBill.org, to create a subReddit viewer (Still in Alpha so please share bugs)",5,3,6,http://www.reddit.com/r/redditdev/comments/18nr51/ive_used_the_reddit_api_on_my_website/,"I've been working on [gingerBill.org](http://www.gingerbill.org/) for a little while now and one of its features is that it has little 'treats'. 

If you type in [/r/earthporn](http://www.gingerbill.org/?q=%2Fr%2Fearthporn), it shows the top 24 post from /r/earthporn. If you add the tag '+i', [/r/earthporn +i](http://www.gingerbill.org/?q=%2Fr%2Fearthporn+%2Bi), it will the top 8 images from /r/earthporn directly within gingerBill.

What other features to this subReddit viewer should I add that would make it much more useful?

Thanks,

Bill (Yes I named a site after myself...)",,False,,t5_2qizd,False,,,True,t3_18nr51,http://www.reddit.com/r/redditdev/comments/18nr51/ive_used_the_reddit_api_on_my_website/,
1360732749.0,4,self.redditdev,18fg96,"Reddit wiki API, getting the raw content of a page",4,0,3,http://www.reddit.com/r/redditdev/comments/18fg96/reddit_wiki_api_getting_the_raw_content_of_a_page/,"I'm looking for a way to get the 'raw' (pre-markup) content of a specific wiki page and I was looking at the [reddit API](http://www.reddit.com/dev/api) however there doesn't appear to be any API calls for getting the contents of a wiki page even though there is one for editing it.

Anyone know of an easy way to do it?",,False,,t5_2qizd,False,,,True,t3_18fg96,http://www.reddit.com/r/redditdev/comments/18fg96/reddit_wiki_api_getting_the_raw_content_of_a_page/,
1360438323.0,2,self.redditdev,187f1m,How does reddit get the thumbnails/embeds for media?,4,2,1,http://www.reddit.com/r/redditdev/comments/187f1m/how_does_reddit_get_the_thumbnailsembeds_for_media/,With all the different services out there this seems to me like it would be a nightmare. Could someone explain the hows of this?,,False,,t5_2qizd,False,,,True,t3_187f1m,http://www.reddit.com/r/redditdev/comments/187f1m/how_does_reddit_get_the_thumbnailsembeds_for_media/,
1359383539.0,4,self.redditdev,17fguw,Subreddit names in different languages?,4,0,10,http://www.reddit.com/r/redditdev/comments/17fguw/subreddit_names_in_different_languages/,"Hey, 

I am trying to figure this out and I have been searching for some resources and sample code, but without luck. I did find one user wanting to do the similar thing, here: https://groups.google.com/forum/?fromgroups=#!topic/reddit-dev/FLwJKuny6TU

But the issue is that that code is old more than 3 years, so I couldn't to the exact same thing on the current version. This is what I did, but I am still getting the ""that name isn't going to work"" message:

1.  As suggested, I added sys.setdefaultencoding('utf8') to the sitecustomize.py in /etc/python2.7/sitecustomize.py and in /usr/lib/python2.7/sitecustomize.py
(first I did import sys, then added that line at the end)
2. After that, I followed the instructions the suggested, and did the changes in the following lines (this is for the **current** version of the code)

&gt;r2/r2/models/subreddit.py:162: 
&gt;name = name.decode(""utf-8"").lower()

I couldnt figure out how to implement this change,    
&gt;q =  cls._query(lower(cls.c.name) == name.decode(""utf-8"").lower(),

the updated code receives a list there, lnames,
&gt;q = cls._query(lower(cls.c.name) == lnames, 

and this code too
&gt;#lower name here so there is only one cache
&gt;names, single = tup(names, True)

this thing was previously was simpler, probably just name = name.lower(), and now there are changes to the code which I don't understand :(

3 . In r2/r2/models/link.py:305, I changed the line to:
&gt;res = ""/r/%s/%s"" % (sr.name.decode(""utf-8""), p) 

4 . In r2/r2/models/account.py:288, I changed it to:
&gt;cls._by_name_cache(name.decode(""utf-8"").lower(), allow_deleted, 
_update = _update) 

5 . In r2/r2/lib/subreddit_search.py:44 and :71, I changed them to:
&gt; name = sr.name.decode(""utf-8"").lower() 

In the  filters.py, I changed the line in the _force_unicode() method, from ""latin1"" to ""euc-kr"", since my goal is to allow Korean characters there.

So, those are the changes I have made. But I am still getting that error. I think the main issue is the change in code, in r2/r2/models/subreddit.py:156, where the method to make subreddit name lowercase is not affected by these .decode('utf8') modifications.

If anyone has some hints, I would be very, very, very, very grateful!",,False,,t5_2qizd,False,,,True,t3_17fguw,http://www.reddit.com/r/redditdev/comments/17fguw/subreddit_names_in_different_languages/,
1359089555.0,5,self.redditdev,178mk1,Praw + Encoding Question,5,0,9,http://www.reddit.com/r/redditdev/comments/178mk1/praw_encoding_question/,"What is the preferred method of dealing with all their strange characters in the comments?  
I have tried this but dosnt really seem to work.  
  
    thisbody=str(comment.body.decode('utf-8', errors='ignore'))  
  
on this string  
  
The guy who submitted the link titled it, ""Turn Thabeet around, let's check for concussion.""   
  
Gives an encoding error  
  
as does this  
 know you are right, and I wish I didnt have to feel bad for it, but I cant control the feels. :(
",,False,,t5_2qizd,1359089740.0,,,True,t3_178mk1,http://www.reddit.com/r/redditdev/comments/178mk1/praw_encoding_question/,
1359058692.0,5,self.redditdev,177jau,"Difficulty resolving ""reddit.local"" externally.",7,2,6,http://www.reddit.com/r/redditdev/comments/177jau/difficulty_resolving_redditlocal_externally/,"I'm trying to run Reddit from an Amazon EC2 instance running Ubuntu x32 - I ran the install script, and after a bit of hunting around, added ""127.0.0.1 reddit.local"" to my hosts file in an attempt to get ""reddit.local"" to resolve correctly. If I go after reddit.local with ping or wget, I get a proper response, but visiting the IP of the instance in my browser redirects to reddit.local and fails to find it. (this is after redirecting incoming connections to :80). Any idea where to start with getting this set up correctly? I don't have a whole lot of experience with ports/networking like this.",,False,,t5_2qizd,False,,,True,t3_177jau,http://www.reddit.com/r/redditdev/comments/177jau/difficulty_resolving_redditlocal_externally/,
1358552912.0,5,self.redditdev,16uio8,Does /r/redditdev take small requests?,13,8,0,http://www.reddit.com/r/redditdev/comments/16uio8/does_rredditdev_take_small_requests/,"I'm not sure if anyone here takes requests, but here it goes. I'd like a web app or chrome extension similar to [Group Reddit Saved Links] (https://chrome.google.com/webstore/detail/group-reddit-saved-links/gkchebpjpehcnlbjamhfgoconkmoalaf?hl=en) (which doesn't work for me for some reason). In a recent thread, /u/ButtCrackFTW provided a simple program to [download all saved links] (http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/). In addition to the saved link's title, URL, and comments, I'd like to get the subreddit and, if possible, the date on which it was saved. I'd also like to be able to add my own ""tag"" to a post and change its title.

I now need a way to organize the information for each link. I'm thinking if it was exported to a table structure with (sortable) headers something like this:

Link Title (editable) | URL | Subreddit | Comments Link | Tags |  Date

It may be a longshot, but if anyone could make this a reality it would be much appreciated!

Bonus functionality: Any saved comments that are in a saved link get displayed in the Comments Link column field ",,False,,t5_2qizd,False,,,True,t3_16uio8,http://www.reddit.com/r/redditdev/comments/16uio8/does_rredditdev_take_small_requests/,
1358539756.0,4,self.redditdev,16u2ny,Praw Submission Question,6,2,3,http://www.reddit.com/r/redditdev/comments/16u2ny/praw_submission_question/,What is the best way to get the ID of a new submission? Is there anything passed back at successful post?,,False,,t5_2qizd,False,,,True,t3_16u2ny,http://www.reddit.com/r/redditdev/comments/16u2ny/praw_submission_question/,
1358429203.0,4,self.redditdev,16r0m0,Has anyone built a web app that works with PRAW?,5,1,3,http://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/,"Basically, I'd like to be able to go to a website that has an online Python compiler/IDE that will run small python scripts that use PRAW, rather than having to set up a local environment. Does something like this exist? Or is there a way to do it fairly easily? Any help is much appreciated.",,False,,t5_2qizd,False,,,True,t3_16r0m0,http://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/,
1358385471.0,4,self.redditdev,16q02d,Praw + karma,4,0,2,http://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/,"Im sorry to have to come back here and ask yet another question. The praw documentation is too complicated and I dont really understand it. I cant find any information on the methods, functions, etc. I cant seem to figure out how to get a comments karma bu its id, consider the following  
  
     for submission in reddit.get_subreddit(sub).get_top(limit=40):  
         flat_comments = submission.all_comments_flat  
         for comment in flat_comments:  
             ident = str(comment.id)   
             karma=comment.??    
I cant seem to find the karma score of the comment  ",,False,,t5_2qizd,1358385799.0,,,True,t3_16q02d,http://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/,
1355945408.0,5,self.redditdev,154ge6,What's the easiest way to convince Reddit that my API submissions don't need a captcha?,5,0,5,http://www.reddit.com/r/redditdev/comments/154ge6/whats_the_easiest_way_to_convince_reddit_that_my/,"I'm working on a personal project that scrapes Reddit for links and comments that contain the phrase ""I'll just leave this here"" and reposts the findings at /r/illjustleavethishere .  Because the account for my bot is brand new, Reddit is requiring a captcha for each submission.  After digging around I found that there is an undocumented amount of karma that will turn off the captcha requirement for an account, and I can confirm that my main account doesn't need a captcha.  Is there another way I can remove the captcha requirement other than building up karma?  Can I create an app and use oauth instead of the basic API?  How about if I buy reddit gold?  How about if I just promise that I won't do bad stuff?

If I have to go the captcha route, how long does a captcha last?  Can I request a new captcha but then submit the correct answer a few days later?

My apologies if this is a noob question.  I'm happy to read docs if this is already documented somewhere.",,False,,t5_2qizd,False,,,True,t3_154ge6,http://www.reddit.com/r/redditdev/comments/154ge6/whats_the_easiest_way_to_convince_reddit_that_my/,
1355871393.0,3,self.redditdev,152qvj,A reddit development question regarding the possibility / ethics / policy of a submission bot.,5,2,9,http://www.reddit.com/r/redditdev/comments/152qvj/a_reddit_development_question_regarding_the/,"I have a question regarding development of an auto-submit bot for [/r/radioreddit](/r/radioreddit).  Basically, we use the reddit code to track voting data for songs on [radioreddit.com](http://radioreddit.com).  Listeners can submit and vote on tracks, and it affects how our streams are build on the hour, as well as our charts.  

Here is the issue: We use the voting data to have the less popular (read: bad) songs filtered out of our stream over time, however no one wants to submit the stinkers because they don't want the negative karma that would be associated.  

Is having a bot that auto submits songs somehow something that is possible, ethical, and not frowned upon? ",,False,,t5_2qizd,False,,,True,t3_152qvj,http://www.reddit.com/r/redditdev/comments/152qvj/a_reddit_development_question_regarding_the/,
1355857718.0,4,self.redditdev,152c5b,PRAW: can't access saved links - no attribute get_saved,4,0,9,http://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/,"I'm using PRAW and trying to access the saved links of a logged in user.


Basically, I'm following the steps mentioned under ""[A Few Short Examples](https://github.com/praw-dev/praw/wiki)"", which include an example on how to get the saved links.

Unfortunately, I'm getting an error (""has no attribute 'get_saved'""). 


    C:\&gt;python  
    Python 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)] on win32  
    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.  
    &gt;&gt;&gt; import praw  
    &gt;&gt;&gt; r = praw.Reddit(user_agent='example')  
    &gt;&gt;&gt; r.login('LudoA', 'foobar')  
    &gt;&gt;&gt; r.user.link_karma  
    2592  
    &gt;&gt;&gt; r.user.get_saved()  
    Traceback (most recent call last):  
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;  
      File ""C:\Python27\lib\site-packages\praw\objects.py"", line 70, in __getattr__  
        attr))  
    AttributeError: '&lt;class 'praw.objects.LoggedInRedditor'&gt;' has no attribute 'get_saved'  
    &gt;&gt;&gt; r.user  
    Redditor(user_name='LudoA')  
    &gt;&gt;&gt; user = r.get_redditor('ludoa')  
    &gt;&gt;&gt; user  
    Redditor(user_name='ludoa')  
    &gt;&gt;&gt; user.link_karma  
    2592  
    &gt;&gt;&gt; user.get_saved()  
    Traceback (most recent call last):  
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;  
      File ""C:\Python27\lib\site-packages\praw\objects.py"", line 70, in __getattr__  
        attr))  
    AttributeError: '&lt;class 'praw.objects.Redditor'&gt;' has no attribute 'get_saved'  
    &gt;&gt;&gt;  


So it does see *r.user* as being of the right type (i.e. *LoggedInRedditor*), but it can't find the method *get_saved()*, just like with the non-logged in user object.


Any idea as to what I'm doing wrong? Thanks!",,False,,t5_2qizd,False,,,True,t3_152c5b,http://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/,
1355839326.0,4,self.redditdev,151v4h,Coloring the spaces to the left of the different 'levels' of comments in a thread.,5,1,2,http://www.reddit.com/r/redditdev/comments/151v4h/coloring_the_spaces_to_the_left_of_the_different/,"It can sometimes be really hard to read which comments relate to their parent comments as all posts are linearly lined out. Especially when the comments are 'backing' upon themselves and come closer to the left hand side again.

Is there anyway we can have some mellow alternate coloring of the different levels of comments of a thread?",,False,,t5_2qizd,False,,,True,t3_151v4h,http://www.reddit.com/r/redditdev/comments/151v4h/coloring_the_spaces_to_the_left_of_the_different/,
1355086020.0,2,self.redditdev,14kakv,Does PRAW cache results with iPython? ,4,2,6,http://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/,"Doing a quick r.user.has_mail, it continues to return false, even after sending myself a message.  End the session, poll Reddit again, I have a message.  Check messages, mark as read via web...poll via ipython/cli and it still continues to return true.  

Anyone have any experience with this? I normally just use requests + json to interact with Reddit, but thought I'd use PRAW this time.  

Any thoughts, advice, etc would be greatly appreciated!

Thanks! ",,False,,t5_2qizd,False,,,True,t3_14kakv,http://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/,
1355024268.0,3,self.redditdev,14j4qq,Using the Reddit API to edit a subreddit,7,4,7,http://www.reddit.com/r/redditdev/comments/14j4qq/using_the_reddit_api_to_edit_a_subreddit/,"[/api/site_admin](http://www.reddit.com/dev/api#POST_api_site_admin) appears to be capable of creating, but not updating, a subreddit. Is there something I'm missing? Can the API change, for example, the contents of a sidebar?

EDIT: To clarify, when I send the subreddit's name as the `name` parameter, and some text as the `description` parameter, I get `error.SUBREDDIT_EXISTS.field-name`. 'course it exists! That's why I'm tryin' to edit it :)

Setting the subreddit's name as `sr` instead of `name` yields `error.BAD_SR_NAME.field-name`, so that's apparently not it either.",,False,,t5_2qizd,1355037848.0,,,True,t3_14j4qq,http://www.reddit.com/r/redditdev/comments/14j4qq/using_the_reddit_api_to_edit_a_subreddit/,
1354759371.0,3,self.redditdev,14cxx2,"Am I mistaken, or is there a java API?",6,3,11,http://www.reddit.com/r/redditdev/comments/14cxx2/am_i_mistaken_or_is_there_a_java_api/,,,False,,t5_2qizd,False,,,True,t3_14cxx2,http://www.reddit.com/r/redditdev/comments/14cxx2/am_i_mistaken_or_is_there_a_java_api/,
1352615677.0,4,stackoverflow.com,1300y6,API returning useless JSON,7,3,2,http://www.reddit.com/r/redditdev/comments/1300y6/api_returning_useless_json/,,,False,,t5_2qizd,False,,,False,t3_1300y6,http://stackoverflow.com/questions/13328798/reddit-api-returning-useless-json,
1351314733.0,5,self.redditdev,125uwo,"Hi, long time web-service writer... I'm having much greater success with page-scraping than reddit-api.",10,5,7,http://www.reddit.com/r/redditdev/comments/125uwo/hi_long_time_webservice_writer_im_having_much/,"Hi,

I must be missing something with the Web-API.... most of my requests apart from login are failing.

A page-scraper I've written that updates its cookies as Reddit sends them, and posts data in faux-forms is working far better! =(

Is there a good ""getting started"" post?

I've started here: http://www.reddit.com/dev/api#POST_api_login

And looked through this: https://github.com/reddit/reddit/wiki/thing

And I'm failing to get anything to work on this: https://apigee.com/console/reddit

I think it's to do with the ""thing"" object?

But I could be getting too old and senile... it's always a possibility!

-------------------

p.s. Here's a super-easy to use RedditBot VBS script: http://untamed.co.uk/miscfolder/RedditBot.zip

The example gets a list of post URLs and titles from the test-accounts ""homepage"".    
No ""thing"" objects required!

If nothing else, it's an interesting demo of a VBS bot. (You could run it as a scheduled task on Windows computers)

I suppose the file RedditWebObject.vbs could be extended to support ALL the API, in which case it's a complete Bot framework in easy to use VisualBasicScript !!",,False,,t5_2qizd,1351315357.0,,,True,t3_125uwo,http://www.reddit.com/r/redditdev/comments/125uwo/hi_long_time_webservice_writer_im_having_much/,
1351045628.0,2,self.redditdev,11zg3r,Looking to make a RSS -&gt; Reddit bot. Can you help me get off the ground?,9,7,12,http://www.reddit.com/r/redditdev/comments/11zg3r/looking_to_make_a_rss_reddit_bot_can_you_help_me/,"The idea is, I want to make a bot that will automatically look at an RSS feed and post new content to a subreddit for that feed. I think it'd be a great way to turn RSS feed content into a more social thing. I'm looking to forego my usage of google reader, as it'd be so much easier to spend my time only reading the more highly-upvoted content from a feed.

What can I do to get going? Does anyone know of any tutorials on how to make a bot to post to reddit? I can probably figure the rest out (parsing RSS feeds, connecting to the web, hosting, etc), I just have no idea where to start on the bot: what language(s) to use, which API's I should fiddle with. Those are the biggest things I'm unsure about.

Any help would be greatly appreciated! Thanks!

EDIT: Thanks for all your help! After a lot of fiddling with the API, I was able to get my bot working! I'll be refining it and making it more extensible and accessible. I'm hoping to add a UI to it and host it on a website, so it can be configured on the fly to create new bots to post from any provided RSS feed to any subreddit! The goal here is to help move RSS feeds to a more social environment, where the best posts can rise to the top, and be visible with all the other news you get from reddit every day!

I set up a test subreddit to test this action out. Hit up /r/ArsTechnicaRSS to see it in action! (I'd recommend the ""New"" tab)",,False,,t5_2qizd,1351100438.0,,,True,t3_11zg3r,http://www.reddit.com/r/redditdev/comments/11zg3r/looking_to_make_a_rss_reddit_bot_can_you_help_me/,
1350485518.0,3,self.redditdev,11mr32,Reddit API and voting. Not accepting modhash/cookie. .error.USER_REQUIRED,5,2,1,http://www.reddit.com/r/redditdev/comments/11mr32/reddit_api_and_voting_not_accepting_modhashcookie/,"I posted this question on SO, then I found this place. Hope it is all right to crosspost.

http://stackoverflow.com/questions/12936274/reddit-api-and-voting-not-accepting-modhash-cookie-error-user-required

I'm not great at python, so I'm just doing this for fun/training.

Thanks in advance :)",,False,,t5_2qizd,False,,,True,t3_11mr32,http://www.reddit.com/r/redditdev/comments/11mr32/reddit_api_and_voting_not_accepting_modhashcookie/,
1350470657.0,4,self.redditdev,11mh92,Make reddit.com (or similar) render altered json?,4,0,9,http://www.reddit.com/r/redditdev/comments/11mh92/make_redditcom_or_similar_render_altered_json/,"Hi, I am working on a project to decorate reddit texts/comments, by asking for the .json version and processing this. This should result in a version where the comment text has been marked up.

Is there a way to get this displayed with the usual reddit web GUI? 
(Can I install some js and css and run it locally to get my result?)

Clarification: I'd like to take a reddit URL, add .json to it to extract json content, put it into a datastructure (AESON in Haskell), apply a transform to it that tags/intersperses the comment text with additional data, leaving the rest of the json in place. Then I want to transform this into a string, so it looks almost exactly like the original json. This doesn't seem all that difficult. 

But: then I want the new json content rendered so it looks like and ordinary reddit page. But I don't know how or whether it is possible.",,False,,t5_2qizd,1350478326.0,,,True,t3_11mh92,http://www.reddit.com/r/redditdev/comments/11mh92/make_redditcom_or_similar_render_altered_json/,
1347702585.0,4,self.redditdev,zx5o2,Can someone walk me through the process of logging in to reddit and then loading a subreddit?,6,2,3,http://www.reddit.com/r/redditdev/comments/zx5o2/can_someone_walk_me_through_the_process_of/,"I'm trying to implement this in java, and I'm having a hard time understanding what I need to do. The way I understand it, it goes:
   
1. POST http://www.reddit.com/api/login/?user=$user&amp;passwd=$passwd  
2. save the cookie (how?)
3. GET http://www.reddit.com/r/subreddit/ and attach the cookie",,False,,t5_2qizd,False,,,True,t3_zx5o2,http://www.reddit.com/r/redditdev/comments/zx5o2/can_someone_walk_me_through_the_process_of/,
1347244492.0,4,self.redditdev,zmt4e,"Switched domains on my reddit install, old domain is still being linked to occasionally",4,0,0,http://www.reddit.com/r/redditdev/comments/zmt4e/switched_domains_on_my_reddit_install_old_domain/,"The domain was switched like a month ago. It seems to do it on accounts that logged on when it was the old domain, but it's cropped up occasionally on other people's accounts. Is there anything I can do about this?",,False,,t5_2qizd,False,,,True,t3_zmt4e,http://www.reddit.com/r/redditdev/comments/zmt4e/switched_domains_on_my_reddit_install_old_domain/,
1346797861.0,4,self.redditdev,zcyot,Batch command for requesting user information?,5,1,6,http://www.reddit.com/r/redditdev/comments/zcyot/batch_command_for_requesting_user_information/,"Ideally we would like to request metadata (comment karma, link karma, etc) for a vector of user id's. Since we need the information for around 500 users, and the data is time-sensitive (we want the user data recorded as it was at the time of the initial request), we would like to request them all at once, rather than one at a time, eating the 2 second delay per request. I couldn't find a solution in the API documentation or glossary, although I may have simply missed it.",,False,,t5_2qizd,False,,,True,t3_zcyot,http://www.reddit.com/r/redditdev/comments/zcyot/batch_command_for_requesting_user_information/,
1346678053.0,5,self.redditdev,za1dp,"Getting the list of ""top"" posts for last 2 months?",5,0,1,http://www.reddit.com/r/redditdev/comments/za1dp/getting_the_list_of_top_posts_for_last_2_months/,"Say you read /r/Science but haven't for 2 months. Now you wanna know what big news happened in the science world for those 2 months. Using the ""top"" view shows you everything form years ago. I tried the timestamp search modifier and that did didn't limit topic dates (or is that not for the code on the live site?). Seems like this would be a really useful feature - does it not exist?",,False,,t5_2qizd,False,,,True,t3_za1dp,http://www.reddit.com/r/redditdev/comments/za1dp/getting_the_list_of_top_posts_for_last_2_months/,
1346375478.0,4,self.redditdev,z3z9s,Error while running the reddit install script on Ubuntu 11.04,5,1,2,http://www.reddit.com/r/redditdev/comments/z3z9s/error_while_running_the_reddit_install_script_on/,"I have installed all the dependencies by referring to reddit wiki on GitHub. I get this error while installing.

Please advise.

Using /usr/lib/python2.7/dist-packages
Finished processing dependencies for reddit-i18n==0.0.0dev
+ sudo -u reddit make
make: Nothing to be done for `all'.
+ cd /home/reddit/reddit/r2
+ sudo -u reddit make
[+] including definitions from Makefile.py
# remove mangled output symlinks, similar to above.
rm -f /home/reddit/reddit/r2/build/public/static/jquery.js
paster run standalone r2/lib/js.py -c ""build_module('jquery')""
Traceback (most recent call last):
  File ""/usr/local/bin/paster"", line 9, in &lt;module&gt;
    load_entry_point('PasteScript==1.7.3', 'console_scripts', 'paster')()
  File ""/usr/lib/pymodules/python2.7/paste/script/command.py"", line 73, in run
    commands = get_commands()
  File ""/usr/lib/pymodules/python2.7/paste/script/command.py"", line 115, in get_commands
    plugins = pluginlib.resolve_plugins(plugins)
  File ""/usr/lib/pymodules/python2.7/paste/script/pluginlib.py"", line 81, in resolve_plugins
    pkg_resources.require(plugin)
  File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 654, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 556, in resolve
    raise VersionConflict(dist,req) # XXX put more info here
pkg_resources.VersionConflict: (WebHelpers 1.1 (/usr/lib/pymodules/python2.7), Requirement.parse('webhelpers==0.6.4'))
make: *** [/home/reddit/reddit/r2/build/public/static/jquery.js] Error 1

",,False,,t5_2qizd,False,,,True,t3_z3z9s,http://www.reddit.com/r/redditdev/comments/z3z9s/error_while_running_the_reddit_install_script_on/,
1345692128.0,4,self.redditdev,yod7d,How do I alter my environment to work on the layout and styling without caching?,4,0,3,http://www.reddit.com/r/redditdev/comments/yod7d/how_do_i_alter_my_environment_to_work_on_the/,"I have installed the reddit source and starting to play with the template files and theme stylesheet. I have come up against the problem of server caching and do not have the experience in the source to turn this off while I work on the files.

I assume it is caching, and clearing/refreshing my local machine and browsers does not fix this. For example, I have altered my *reddit.css* stylesheet over 12 hours ago and yet to see a change. I also modified some template *.py* files and they took quite a while to update.

Any advice is appreciated, thanks.",,False,,t5_2qizd,False,,,True,t3_yod7d,http://www.reddit.com/r/redditdev/comments/yod7d/how_do_i_alter_my_environment_to_work_on_the/,
1345434446.0,4,self.redditdev,yi86p,Help appreciated for a fresh Reddit installation on a live staging server [Linode],5,1,21,http://www.reddit.com/r/redditdev/comments/yi86p/help_appreciated_for_a_fresh_reddit_installation/,"I am excited to check out the reddit source. I am trying set this up on a running Linode server I have reserved for such an install. It is running Ubuntu 11.04 (but can easily change the distro if required).

I have some experience with server management, but certainly have a lot to learn. I have successfully installed the source via both the installation script and the manual process, but neither allow me to actually access the interface.

The main problem I face is the redirect to **reddit.local**. I have changed this is the configuration file (and rebooted the server) to my node's accessible domain, but I still get pushed to **reddit.local**.

Could anybody please offer assistance? The main issue I seem to have is all the guides appear to be more directed and local installations, whereas I need my local machine/the web to be able to access my live server's service.",,False,,t5_2qizd,False,,,True,t3_yi86p,http://www.reddit.com/r/redditdev/comments/yi86p/help_appreciated_for_a_fresh_reddit_installation/,
1345060789.0,4,self.redditdev,ya1ex,"Is there really no better way to get ""more"" comments than to request them one at a time in a loop?",5,1,7,http://www.reddit.com/r/redditdev/comments/ya1ex/is_there_really_no_better_way_to_get_more/,"Comment listings do not make use of the before/after system like story listings do (despite taking up RAM by being assigned null), instead the last comment is denoted by a ""more"" type and an id or array of ids of comments that follow. To get the actual comment, you must make a request to reddit.com/comments/story_id/_/id_from_more.json. Unfortunately, this returns only one comment at a time. There currently doesn't appear to be a way to batch request these ids and get usable JSON in return.

reddit.com/api/morechildren is what reddit.com uses to solve the problem, but its responses are not usable for my purposes.

I really don't want to break the one-request-every-two-seconds rule, which requesting all of these items in a loop would certainly do. Any help?

-------------------------------

I should add that usually I just see if a comment has a ""more"" designation on it, and reload the entire reply list of its parent. In 99% of cases, the API returns enough comments to eliminate all the mores. However, for the top level comments this won't work. Doing so would merely refresh all of the comments, not add any more. Requesting all of the mores in a loop is seemingly the only solution in this case. Hence my post here.",,False,,t5_2qizd,1345061200.0,,,True,t3_ya1ex,http://www.reddit.com/r/redditdev/comments/ya1ex/is_there_really_no_better_way_to_get_more/,
1344604371.0,2,self.redditdev,xztbo,503 Service Unavailable after Reddit Install,5,3,12,http://www.reddit.com/r/redditdev/comments/xztbo/503_service_unavailable_after_reddit_install/,"Hey guys,

I just installed reddit on my Ubuntu 11.04 Natty Narwhal. After starting reddit with

    me:~/reddit/r2$ paster serve --reload example.ini port=8080

I put in http://127.0.0.1:8080 or http://reddit.local in my browser and I get a 503 Service Unavailable, No server is available to handle this request in my browser message.

I'm sure there's a simple solution to this that I'm not seeing. It would great if you guys can help me out.

Thanks,",,False,,t5_2qizd,False,,,True,t3_xztbo,http://www.reddit.com/r/redditdev/comments/xztbo/503_service_unavailable_after_reddit_install/,
1344451819.0,3,self.redditdev,xw84x,Any writeups on replicating reddits comment tree structure/queries/sorting,5,2,1,http://www.reddit.com/r/redditdev/comments/xw84x/any_writeups_on_replicating_reddits_comment_tree/,"It's difficult for me to find/follow the code pertaining to comments. Any one could point me in the right direction for how reddit handles the comment insertion, how it links a comment to its parent nodes, and how it builds queries and builds the list to send to the template? Sorry if this has been asked a thousand times, the search is ""under heavy load""

Thanks!",,False,,t5_2qizd,False,,,True,t3_xw84x,http://www.reddit.com/r/redditdev/comments/xw84x/any_writeups_on_replicating_reddits_comment_tree/,
1343767983.0,4,self.redditdev,xgwf6,Whitelist for the API?,4,0,15,http://www.reddit.com/r/redditdev/comments/xgwf6/whitelist_for_the_api/,"I'm developing an iOS/Android application using the Reddit API.  As I'm sure everyone in this subreddit is aware, the rate limit is a huge hindrance.  Is there a whitelist for bypassing this rate limit?  Or are other applications manually inserting pauses between their requests so they don't run into any API issues?",,False,,t5_2qizd,False,,,True,t3_xgwf6,http://www.reddit.com/r/redditdev/comments/xgwf6/whitelist_for_the_api/,
1342592804.0,3,self.redditdev,wqx7o,Does reddit using Amazon Cloud Search?,7,4,5,http://www.reddit.com/r/redditdev/comments/wqx7o/does_reddit_using_amazon_cloud_search/,"Hi Redditors,

I read in the wiki that reddit is moving to indextrunk but when I reviewed run.py file I found that there is keys like Cloud_Search_Api_key ... So I guessed it is using cloud search .
If this true what are the values that should be changed in run.py to make cloudsearch works? and what is subreddit_cloud_api_key?

Thanks,",,False,,t5_2qizd,False,,,True,t3_wqx7o,http://www.reddit.com/r/redditdev/comments/wqx7o/does_reddit_using_amazon_cloud_search/,
1342309597.0,4,self.redditdev,wkjbx,Is the vote fuzzing code in the repo?,4,0,20,http://www.reddit.com/r/redditdev/comments/wkjbx/is_the_vote_fuzzing_code_in_the_repo/,...or is it something that kept private.,,False,,t5_2qizd,False,,,True,t3_wkjbx,http://www.reddit.com/r/redditdev/comments/wkjbx/is_the_vote_fuzzing_code_in_the_repo/,
1340641756.0,4,self.redditdev,vktbp,Full List of articles for a sub-reddit. How far back in time with the APIs allow us to go?,4,0,4,http://www.reddit.com/r/redditdev/comments/vktbp/full_list_of_articles_for_a_subreddit_how_far/,"I want to do some word-stat style things on Proggit postings+comments (as a data set).  There can't be that many, and it should be fairly easy to download each once (obeying the 2 second rate limiter). How far back in time does the API allow for timelines on sub-reddits ?",,False,,t5_2qizd,False,,,True,t3_vktbp,http://www.reddit.com/r/redditdev/comments/vktbp/full_list_of_articles_for_a_subreddit_how_far/,
1340048941.0,5,self.redditdev,v8ly3,Bulk steam key giveaway: Need a bot!,5,0,5,http://www.reddit.com/r/redditdev/comments/v8ly3/bulk_steam_key_giveaway_need_a_bot/,"Hey all,

I've tried contacting the developer of one bot here on reddit but to no avail.  I'm going to be giving away a bunch of keys for a game I have on Steam and I need an automated way to distribute the keys.  I'd especially like to avoid new accounts being made for this in specific, so a 1 day minimum membership test would be great.  If any of you have the means to do this, I'd love to chat with you!",,False,,t5_2qizd,False,,,True,t3_v8ly3,http://www.reddit.com/r/redditdev/comments/v8ly3/bulk_steam_key_giveaway_need_a_bot/,
1339998642.0,4,self.redditdev,v7ode,"More things that point back to themselves, infinite mores!?",4,0,1,http://www.reddit.com/r/redditdev/comments/v7ode/more_things_that_point_back_to_themselves/,"http://www.reddit.com/r/worldnews/comments/v6dxm/_/c51xzqj/.json

has a more thing:

    {
        ""kind"": ""more"",
        ""data"": {
            ""children"": [
                ""c51xzqj""
            ],
            ""id"": ""c51xzqj"",
            ""name"": ""t1_c51xzqj""
        }
    }

So there is a ""More"" thing that points to... itself? I'm a bit confused.",,False,,t5_2qizd,False,,,True,t3_v7ode,http://www.reddit.com/r/redditdev/comments/v7ode/more_things_that_point_back_to_themselves/,
1339821530.0,4,self.redditdev,v4lmk,parsing jsonp (comments) with jquery. stuck on how to get child comments. any help?,5,1,6,http://www.reddit.com/r/redditdev/comments/v4lmk/parsing_jsonp_comments_with_jquery_stuck_on_how/,"[here's a jsFiddle with what I have so far](http://jsfiddle.net/FRQ9b/3/)  the bottom most jquery is my current attempt...

[here's a link to the formatted json on pastebin](http://pastebin.com/R9NCZBNE) ",,False,,t5_2qizd,False,,,True,t3_v4lmk,http://www.reddit.com/r/redditdev/comments/v4lmk/parsing_jsonp_comments_with_jquery_stuck_on_how/,
1339638181.0,5,self.redditdev,v0w4s,"Introducing Alien8, a Windows 8 Metro client for Reddit (Initial ideas and discussion welcome!)",5,0,4,http://www.reddit.com/r/redditdev/comments/v0w4s/introducing_alien8_a_windows_8_metro_client_for/,"I'm in the process of laying the foundation for a Windows 8 Metro reddit client called **Alien8**.

Subreddit: /r/Alien8 (There's not much there now, I'm hoping to use it to show off features as they're coded.)

And here's a first look at the initial login screen: (it does work, I get a modhash/cookie back)  
http://i.imgur.com/k2SB1.jpg

At this point I'm welcome to any ideas / suggestions you might have for features, layout, options, anything. Since almost nothing is done yet, I'm open to anything you might want to see in this client!",,False,,t5_2qizd,False,,,True,t3_v0w4s,http://www.reddit.com/r/redditdev/comments/v0w4s/introducing_alien8_a_windows_8_metro_client_for/,
1338520552.0,6,self.redditdev,uf1d8,"Is there a way to get the name for ""readers"" through the reddit API?",8,2,6,http://www.reddit.com/r/redditdev/comments/uf1d8/is_there_a_way_to_get_the_name_for_readers/,"A lot of subreddits have clever names for their readers in place of ""readers."" For example, /r/rush has ""Modern Day Warriors."" There's no field for this in the subreddit's about.json. Is there some other way to get this?",,False,,t5_2qizd,False,,,True,t3_uf1d8,http://www.reddit.com/r/redditdev/comments/uf1d8/is_there_a_way_to_get_the_name_for_readers/,
1338135781.0,2,self.redditdev,u7eb2,API: Is it possible to use the api to change just the sidebar text of a subreddit?,5,3,3,http://www.reddit.com/r/redditdev/comments/u7eb2/api_is_it_possible_to_use_the_api_to_change_just/,,,False,,t5_2qizd,False,,,True,t3_u7eb2,http://www.reddit.com/r/redditdev/comments/u7eb2/api_is_it_possible_to_use_the_api_to_change_just/,
1335654757.0,4,self.redditdev,sxf0g,How to create new account with reddit api?,7,3,9,http://www.reddit.com/r/redditdev/comments/sxf0g/how_to_create_new_account_with_reddit_api/,"Hello all, I've been reading the github wiki https://github.com/reddit/reddit/wiki on how to use the api, but I can't seem to find out how to create a new account for a user. Any help or tips to the right direction would be appreciated. Thanks.",,False,,t5_2qizd,False,,,True,t3_sxf0g,http://www.reddit.com/r/redditdev/comments/sxf0g/how_to_create_new_account_with_reddit_api/,
1334594364.0,3,self.redditdev,scl05,Get a private Message by ID(s)?,5,2,3,http://www.reddit.com/r/redditdev/comments/scl05/get_a_private_message_by_ids/,"So, is it possible to get a PM by its ID? I'd like to access specific messages from a list of cached IDs if at all possible. Otherwise, I'm probably left doing a get on message/messages then discarding the results I care nothing about. ",,False,,t5_2qizd,False,,,True,t3_scl05,http://www.reddit.com/r/redditdev/comments/scl05/get_a_private_message_by_ids/,
1334432069.0,4,self.redditdev,s9twj,"Attempting to post a text story, receiving strange data on successful post.",5,1,9,http://www.reddit.com/r/redditdev/comments/s9twj/attempting_to_post_a_text_story_receiving_strange/,"Maybe not strange, but not matching the docs here https://github.com/reddit/reddit/wiki/API%3A-submit

This is the data I'm receiving :

    {u'json': {u'data': {u'cookie': u'7453188,2012-04-14T12:25:19,317641b46e64ef000000000000',
                         u'modhash': u'censoredForPrivacy'},
               u'errors': []}}

Seems like it should be something with jquery along with post information? The post goes through as intended, but I can only find it through my browser, since I'm not seeing the relevant data.

I'm using requests module in python 2.7 windows7x64. Here's what I'm using:

    url = r'http://www.reddit.com/api/submit'
    payload = {'title' : 'Test',
               'text': '{}'.format(time.asctime()),
               'sr': 'tankorsmash',
               'kind': 'self', 
               'uh': mh,
               'r': 'tankorsmash',
               }

    requests.post(url,payload,cookies=cookies) #the cookies api_type was 'json' though...",,False,,t5_2qizd,False,,,True,t3_s9twj,http://www.reddit.com/r/redditdev/comments/s9twj/attempting_to_post_a_text_story_receiving_strange/,
1333721635.0,5,self.redditdev,rw9wl,"Some help starting out with the API, and then continuing it.",5,0,6,http://www.reddit.com/r/redditdev/comments/rw9wl/some_help_starting_out_with_the_api_and_then/,"I have been doing web development for ~4 years now but I never really worked with an API and I seem to be having a lot of difficulty starting with the Reddit API.

I understand that if you go to reddit.com/r/anything/.json it will return a json file. But when I try doing this in the code it doesn't work. 

I've been seeing a lot of requests for new API documentation so, I thought maybe I would be able to take on the herculean task, and this time it would be from a (very) beginners perspective. I just need a push to start me off.

thanks for the help!",,False,,t5_2qizd,False,,,True,t3_rw9wl,http://www.reddit.com/r/redditdev/comments/rw9wl/some_help_starting_out_with_the_api_and_then/,
1333674828.0,4,self.redditdev,rvkmz,What sites are built on reddit besides reddit.com?,4,0,4,http://www.reddit.com/r/redditdev/comments/rvkmz/what_sites_are_built_on_reddit_besides_redditcom/,,,False,,t5_2qizd,False,,,True,t3_rvkmz,http://www.reddit.com/r/redditdev/comments/rvkmz/what_sites_are_built_on_reddit_besides_redditcom/,
1332543018.0,5,self.redditdev,rantu,Getting the most comments possible in a single request,5,0,1,http://www.reddit.com/r/redditdev/comments/rantu/getting_the_most_comments_possible_in_a_single/,"I'm working on a project where I'm trying to fetch a lot of comments quickly, preferably all the comments on the first 10 or so pages of a given subreddit. I am aware I can go to /r/_subreddit_/comments to get the latest comments, but the limit seems to be 100.
To get around this I've made requests to each post in the first 10 pages, and then looked at the comments of the posts, but that only gives me the top 500 comments of a given post. Afterwards I get a list of comment ids and I have to make one request per additional comment. This results in making hundreds if not thousands of requests which clearly isn't a good idea. 

Why do there seem to be arbitrary limits on how many comments I can get? Wouldn't it be better for me to make fewer requests for more data at once? I know reddit gold can supposedly increase the cap to 1500 but that still doesn't cover everything, and since I am using jsonp to make requests from client-side javascript, I can't log be logged in when doing this.

Could anyone point me in the correct direction for approaching this problem?",,False,,t5_2qizd,True,,,True,t3_rantu,http://www.reddit.com/r/redditdev/comments/rantu/getting_the_most_comments_possible_in_a_single/,
1330402380.0,5,self.redditdev,q97sk,Try to Clone reddit but error failed (VM used: java-6-openjdk) ,8,3,0,http://www.reddit.com/r/redditdev/comments/q97sk/try_to_clone_reddit_but_error_failed_vm_used/,"http://pastebin.com/wyA9fs5F

environment
vps
new ubuntu 11.04",,False,,t5_2qizd,False,,,True,t3_q97sk,http://www.reddit.com/r/redditdev/comments/q97sk/try_to_clone_reddit_but_error_failed_vm_used/,
1329862362.0,3,self.redditdev,pzxqf,How can I create/host my own Reddit type clone?,8,5,6,http://www.reddit.com/r/redditdev/comments/pzxqf/how_can_i_createhost_my_own_reddit_type_clone/,"Hey Reddit,
I have this great idea that I can use the reddit code in a more private community sense. Essentially what I want to do is host my own Reddit type thing (ideally using Reddit's source code since it's already done). I do have some experience with web design (php/RoR/html/css...) but I did some research and it said that Reddit is in python which I'm not used to. I rent a server for php and stuff but this seems like a whole new domain (pun intended). Is there an article/tutorial that I can read up on or an explanation somewhere? Is this even possible? Thanks 

**tl;dr** I want to host my own reddit clone but don't know about Python on web hosting.",,False,,t5_2qizd,False,,,True,t3_pzxqf,http://www.reddit.com/r/redditdev/comments/pzxqf/how_can_i_createhost_my_own_reddit_type_clone/,
1329764646.0,4,self.redditdev,py4a8,Add your client / wrapper to the Third Party Clients wiki page,7,3,3,http://www.reddit.com/r/redditdev/comments/py4a8/add_your_client_wrapper_to_the_third_party/,"I created a [third party client listing](https://github.com/reddit/reddit/wiki/API%3A-Third-Party-Clients
) on the reddit wiki, though it's fairly incomplete. If you are working on something, or are aware of something that isn't already listed can you please add?

I would suggest this list be exclusive to clients/wrappers that expose the API functionality to various languages, and not include everything that simply utilizes the API.",,False,,t5_2qizd,False,,,True,t3_py4a8,http://www.reddit.com/r/redditdev/comments/py4a8/add_your_client_wrapper_to_the_third_party/,
1328044324.0,6,self.redditdev,p52dr,"Can I use Mellort to get all new comments in a given subreddit?  Also, how do I reference a comment based on its id?",11,5,1,http://www.reddit.com/r/redditdev/comments/p52dr/can_i_use_mellort_to_get_all_new_comments_in_a/,"Basically what I want is this:

http://www.reddit.com/r/python/comments/.json

and this:

http://www.reddit.com/r/Python/comments/p4iug/python_books_i_am_confused_recommend_me_one/c3mijcm/.json

There is a method for getting new comments from the ""reddit"" object, and one from a submission, but I can't find one for the subreddit object.

For the second item above, is there an equivalent of the following for comments:

r.get_submission(submission_id=""p4iug"")



---- 

Also, is this the best place to ask questions like this?",,False,,t5_2qizd,False,,,True,t3_p52dr,http://www.reddit.com/r/redditdev/comments/p52dr/can_i_use_mellort_to_get_all_new_comments_in_a/,
1327171834.0,4,self.redditdev,oqgmp,Get User's Flair for a Subreddit,4,0,2,http://www.reddit.com/r/redditdev/comments/oqgmp/get_users_flair_for_a_subreddit/,"Is there any way to do this without being a moderator?  I know I could compile a list of flair by pulling a bunch of posts, but is there a way to specifically request the flair for a user?  Something like this:

POST ""r=subreddit&amp;name=username"" to
http://www.reddit.com/api/flair

Response: JSON containing flair details for that user+subreddit

**Edit:** Solved, sort of.  See [my comment](http://www.reddit.com/r/redditdev/comments/oqgmp/get_users_flair_for_a_subreddit/c3jgs3h).",,False,,t5_2qizd,True,,,True,t3_oqgmp,http://www.reddit.com/r/redditdev/comments/oqgmp/get_users_flair_for_a_subreddit/,
1325883244.0,3,self.redditdev,o5vi3,Creating a web form to let users edit their flair - is there a way to ensure that users are who they say they are?,7,4,2,http://www.reddit.com/r/redditdev/comments/o5vi3/creating_a_web_form_to_let_users_edit_their_flair/,"So you may have seen [my earlier post here](http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/) where I was having trouble with a php script to update flair. I got that working, but now I've run into another dilema - when a user enters their name on my form, is there a way to guarantee they are who they say they are? I was going to ask for the reddit username and password, but I wasn't sure how willing people would be to put their password in my site. I was thinking maybe I can grab a cookie or something and extract their username from it... Any ideas? I'm really excited about my little project because I think a lot of mods would be interested in it, but I just keep hitting these little speed bumps.",,False,,t5_2qizd,False,,,True,t3_o5vi3,http://www.reddit.com/r/redditdev/comments/o5vi3/creating_a_web_form_to_let_users_edit_their_flair/,
1325439732.0,3,self.redditdev,nypeg,&lt;type 'exceptions.AttributeError'&gt;: 'module' object has no attribute 'I18nController' ,5,2,5,http://www.reddit.com/r/redditdev/comments/nypeg/type_exceptionsattributeerror_module_object_has/,"Greetings, I have been getting this error on the /admin page of my reddit clone.  Has anyone seen it before and/or can anyone suggest a resolution?  Here is the rest of the stack trace:


URL: http://example.com/user/reddit/admin/

File '/usr/local/lib/python2.7/dist-packages/Pylons-0.9.6.2-py2.7.egg/pylons/error.py', line 245 in respond
  app_iter = self.application(environ, detect_start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 302 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 398 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 356 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 371 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 107 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 107 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 107 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 446 in __call__
  return self.app(environ, start_response)

File '/opt/reddit/r2/r2/config/middleware.py', line 472 in __call__
  return self.app(environ, custom_start_response)

File '/usr/local/lib/python2.7/dist-packages/Pylons-0.9.6.2-py2.7.egg/pylons/wsgiapp.py', line 314 in __call__
  return self.app(environ, start_response)

File '/usr/local/lib/python2.7/dist-packages/Beaker-1.6.2-py2.7.egg/beaker/middleware.py', line 73 in __call__
  return self.app(environ, start_response)

File '/usr/local/lib/python2.7/dist-packages/Beaker-1.6.2-py2.7.egg/beaker/middleware.py', line 155 in __call__
  return self.wrap_app(environ, session_start_response)

File '/usr/local/lib/python2.7/dist-packages/Routes-1.8-py2.7.egg/routes/middleware.py', line 99 in __call__
  response = self.app(environ, start_response)

File '/usr/local/lib/python2.7/dist-packages/Pylons-0.9.6.2-py2.7.egg/pylons/wsgiapp.py', line 94 in __call__
  controller = self.resolve(environ, start_response)

File '/usr/local/lib/python2.7/dist-packages/Pylons-0.9.6.2-py2.7.egg/pylons/wsgiapp.py', line 170 in resolve
  return self.find_controller(controller)

File '/opt/reddit/r2/r2/config/middleware.py', line 484 in find_controller
  mycontroller = getattr(sys.modules[full_module_name], class_name)

AttributeError: 'module' object has no attribute 'I18nController'",,False,,t5_2qizd,False,,,True,t3_nypeg,http://www.reddit.com/r/redditdev/comments/nypeg/type_exceptionsattributeerror_module_object_has/,
1325206614.0,6,self.redditdev,nvlmb,Does anyone have an example of an /api/flairlist request?,8,2,10,http://www.reddit.com/r/redditdev/comments/nvlmb/does_anyone_have_an_example_of_an_apiflairlist/,"Writing a C# reddit client, and I'm not sure why but I can't get flairlist to work no matter how closely I follow the documentation at https://github.com/reddit/reddit/wiki/API%3A-flairlist

I've looked at some other clients for guidance but I can't see anything being done differently. 

Any assistance will be greatly appreciated. For those curious the client is here https://github.com/pressf12/reddit

EDIT: Here is an example request I am making, which is resulting in a 403 error with an HTML page (Not a JSON response).

GET http://www.reddit.com/api/flairlist?r=csharpredditclient&amp;limit=1000&amp;uh=abc1234567890 HTTP/1.1  
Host: www.reddit.com  
Cookie: reddit_session=8010059%2c2011-12-30T01%3a49%3a45%abc1234567890  ",,False,,t5_2qizd,True,,,True,t3_nvlmb,http://www.reddit.com/r/redditdev/comments/nvlmb/does_anyone_have_an_example_of_an_apiflairlist/,
1324664482.0,4,self.redditdev,no5em,User Modhash questions... do I need to refresh the modhash every time I make a request?,5,1,1,http://www.reddit.com/r/redditdev/comments/no5em/user_modhash_questions_do_i_need_to_refresh_the/,"* When I log in the user I receive one modhash.
* Then I request the user profile and receive another.
* Then I request mine.json with the FIRST modhash, and it appears to work.

Do I need to update the user modhash every time I make a request, before I make another? Logically I thought that I would, but in practice it doesn't seem that way?",,False,,t5_2qizd,False,,,True,t3_no5em,http://www.reddit.com/r/redditdev/comments/no5em/user_modhash_questions_do_i_need_to_refresh_the/,
1324462967.0,5,self.redditdev,nl4ee,Reddit Api Captcha Problems,6,1,3,http://www.reddit.com/r/redditdev/comments/nl4ee/reddit_api_captcha_problems/,"I am making a little bot for reddit in C# and I'm having a bit of a problem with the captchas. What I am trying to do is compose a message. Now first I try send a message and if I receive bad_captcha I ask api/new_captcha for the unique code of a captcha. I then download the image at captcha/uniquecode.png. After presenting this to the user they then decode it. I then try to compose the message again using the captcha. The problem is I never get a success always bad_captcha.


Another issue I noticed was that when polling api/new_captcha I can't have the output in nice and simple json by specifying api_type=json in the parameter list.

It also doesn't help that as far as I can see there is no documentation about how to do this. Anyway thanks for reading.",,False,,t5_2qizd,False,,,True,t3_nl4ee,http://www.reddit.com/r/redditdev/comments/nl4ee/reddit_api_captcha_problems/,
1321878052.0,2,self.redditdev,mk31z,Is there an API for getting notified when new links are submitted to a subreddit or to get the list of links in a subreddit according to various criteria?,6,4,7,http://www.reddit.com/r/redditdev/comments/mk31z/is_there_an_api_for_getting_notified_when_new/,,,False,,t5_2qizd,False,,,True,t3_mk31z,http://www.reddit.com/r/redditdev/comments/mk31z/is_there_an_api_for_getting_notified_when_new/,
1321624021.0,2,self.redditdev,mgw8l,re-post from r/programming- Question about reddit api.,6,4,14,http://www.reddit.com/r/redditdev/comments/mgw8l/repost_from_rprogramming_question_about_reddit_api/,"My question is about the submit api. The JSON return is a single object ""jquery"" with an array of values representing either successes or errors. That is fine however, I would like to properly handle all the possible responses rather than just handling a success and giving back a generic error. Is there a list of possible responses from the submit api? The Wiki page is lacking.. Any help would be greatly appreciated! ",,False,,t5_2qizd,False,,,True,t3_mgw8l,http://www.reddit.com/r/redditdev/comments/mgw8l/repost_from_rprogramming_question_about_reddit_api/,
1321133144.0,4,self.redditdev,ma3ds,A suggestion engine for reddit (not a sub-reddit recommender) - Final semester project,4,0,0,http://www.reddit.com/r/redditdev/comments/ma3ds/a_suggestion_engine_for_reddit_not_a_subreddit/,"Hello redditors!
Reddit is definitely a better place with a personalized recommendation engine of new and upcoming links for each user. A ""suggested"" sub-reddit for each user, with the links that would interest him, would be awesome to have! 

I am a senior year college student, majoring in CS, and I have been giving some thought about taking this up as my final semester project. I haven't yet narrowed down the machine learning algorithms I would be using, but this (http://www.reddit.com/r/redditdev/comments/lowwf/attempt_2_want_to_help_reddit_build_a_recommender/) has given me lots of ideas. Basically, what I wish to see is a suggestion engine for links, based on the user activity over all other sub-reddits (comments, likes, dislikes, keywords in the titles upvoted, etc.). 

This is just a proposal to take up the project. I am not discussing the technicality as enough has been already discussed in the above link. Since I would be graded, you can be assured that I would be committed to the work. Your thoughts and suggestions would be great!

",,False,,t5_2qizd,True,,,True,t3_ma3ds,http://www.reddit.com/r/redditdev/comments/ma3ds/a_suggestion_engine_for_reddit_not_a_subreddit/,
1320659077.0,3,self.redditdev,m3dc3,Comment Table Structure,8,5,3,http://www.reddit.com/r/redditdev/comments/m3dc3/comment_table_structure/,"I am not currently in a location where I can install reddit, but I am mainly interested in one aspect and I figure I may be able to get my questions answered here.

The part of reddit I am interested in is the table structure for storing trees of comments (and the code for retrieving trees of comments). I am assuming comments are stored in PostgreSQL (correct?).

I've seen many ways to implement this ([common table expressions / recursive queries](http://wiki.postgresql.org/wiki/CTEReadme), [nested sets](http://en.wikipedia.org/wiki/Nested_set_model), [etc](http://www.google.com/search?q=trees+in+database)). How does reddit implement trees of comments (and why was this method chosen)? Where is the relevant code in the github repository so I can browse it at my current location?",,False,,t5_2qizd,False,,,True,t3_m3dc3,http://www.reddit.com/r/redditdev/comments/m3dc3/comment_table_structure/,
1320267957.0,3,self.redditdev,ly4z0,relationship between comments pages and cassandra/memcache,5,2,1,http://www.reddit.com/r/redditdev/comments/ly4z0/relationship_between_comments_pages_and/,"I'm still in the process of setting up reddit to drive the comments portion of my website.  Twice now I've had comments pages come up with ""404 Not Found The resource could not be found.""  I tried to debug but it was fruitless.

I'm on AWS, so I just started up new servers.  It was fine at first, and then it occured again.  Since I was restarting cassandra occasionally - I thought it might be that.

So - in the form of a question - are both cassandra and memcache operating in front of comment pages?  I assume memcache is a pure cache - recreatable.  In the case of cassandra though, I think it's used as a permacache.

So then - am I allowed to bring down/restart cassandra and expect it to come back in the same state?

Is there a way to recreate the comment pages so that they work after this occurrence?  Is there a way to avoid this occurrence?

Since I am solely using the comments system of reddit, is it possible to remove cassandra from operation?  I think this might be complex, as the comments queue writes its results into cassandra, I would think.",,False,,t5_2qizd,False,,,True,t3_ly4z0,http://www.reddit.com/r/redditdev/comments/ly4z0/relationship_between_comments_pages_and/,
1319072601.0,4,self.redditdev,li5ly,Getting more comments for post,6,2,3,http://www.reddit.com/r/redditdev/comments/li5ly/getting_more_comments_for_post/,"The API wiki page that I found ( https://github.com/reddit/reddit/wiki/API ) says the following:

&gt; Fetching more
&gt; 
&gt; If you're fetching comments from a thread with more comments than the API will return in a single response, the last comment will look like this:

&gt; {'data': {'id': 'abc1010', 'name': 't1_abc1010'}, 'kind': 'more'}

&gt; To get these comments, you can fetch the url http://reddit.com/comments/FULLNAME/abc1010.json, where FULLNAME is the FULLNAME of the story.

This seems to only get you the replies to a comment (in this example, abc1010).  

How do I get more comments for a particular post? ",,False,,t5_2qizd,False,,,True,t3_li5ly,http://www.reddit.com/r/redditdev/comments/li5ly/getting_more_comments_for_post/,
1319000622.0,4,self.redditdev,lh6b6,Getting a 500 on api login requests,4,0,3,http://www.reddit.com/r/redditdev/comments/lh6b6/getting_a_500_on_api_login_requests/,"Greetings fellow reddit devs!  I'm trying to use the login api from javascript.  I'm following the steps outlined here:

https://github.com/reddit/reddit/wiki/API%3A-login

I am also looking at the request in firebug and matching up my request headers to those in the site login.  I'm using chrome (with security disabled to allow for cross site requests), and I can't get this thing to give me anything but a 500.  I've tried using the ssl gateway and good 'ol http.  Here is the code I'm using:


var URL_LOGIN = ""http://www.reddit.com/api/login"";

var url = URL_LOGIN + ""/"" + username;
	var data = { ""user"": username, ""passwd"": password, ""api_type"": ""json"" };

	$.ajax({
		type: 'POST',
  		url: url,
  		data: data,
  		success: function(json) { handler(json); },
  		dataType: ""json"",
  		contentType: ""application/x-www-form-urlencoded; charset=UTF-8""
	});

I don't think the cross origin stuff is busted, since I can make GET requests for stuff all day long.  Any ideas?",,False,,t5_2qizd,False,,,True,t3_lh6b6,http://www.reddit.com/r/redditdev/comments/lh6b6/getting_a_500_on_api_login_requests/,
1317826132.0,3,self.redditdev,l1qw4,Problem installing translations,4,1,7,http://www.reddit.com/r/redditdev/comments/l1qw4/problem_installing_translations/,"I'm trying to install translations to my reddit. I'm following the steps in reddit-i18n/README.txt, but it fails when I run

&gt; paster setup-app config.ini

with this error message

&gt; ImportError: No module named config.middleware

What am I doing wrong?",,False,,t5_2qizd,False,,,True,t3_l1qw4,http://www.reddit.com/r/redditdev/comments/l1qw4/problem_installing_translations/,
1310951297.0,5,z3rb.net,iseee,I've made a C# wrapper for the Reddit API. Please check it out guys.,6,1,5,http://www.reddit.com/r/redditdev/comments/iseee/ive_made_a_c_wrapper_for_the_reddit_api_please/,,,False,,t5_2qizd,False,,,False,t3_iseee,http://z3rb.net/reddit-c-api/,
1310743184.0,3,self.redditdev,iqgzo,Dev Noob here; Can anyone help get me started?,4,1,0,http://www.reddit.com/r/redditdev/comments/iqgzo/dev_noob_here_can_anyone_help_get_me_started/,"I would like to install a Reddit clone on an internal server at my work.  I want to give the clone a fancy company-themed CSS skin, but otherwise I'd like it to operate almost entirely in the same way Reddit does.  Here's the catch: I have no idea what the hell I'm doing.  I'm very computer literate, and learn new coding languages quickly, but have no idea of the set of tools I'll need to get started.  Can anyone point me somewhere that can list exactly what I'll need, and a step-by-step method of getting a small community up and running?",,False,,t5_2qizd,False,,,True,t3_iqgzo,http://www.reddit.com/r/redditdev/comments/iqgzo/dev_noob_here_can_anyone_help_get_me_started/,
1310534616.0,5,self.redditdev,io62s,Still Setting Up Reddit: Cassandra Problems,5,0,0,http://www.reddit.com/r/redditdev/comments/io62s/still_setting_up_reddit_cassandra_problems/,"I'm running the automated installer of lucid lynx, and it all goes fine until it get's to:
""svscan start/running, process 32130
Exception connecting to localhost/9160. Reason: Connection refused.""
And then it stops. I can't figure out how to fix this one.",,False,,t5_2qizd,False,,,True,t3_io62s,http://www.reddit.com/r/redditdev/comments/io62s/still_setting_up_reddit_cassandra_problems/,
1308082442.0,3,self.redditdev,hzoz6,Debugging?,4,1,1,http://www.reddit.com/r/redditdev/comments/hzoz6/debugging/,"I'm still not very keen on debugging. Up to this point I've done okay figuring out errors and mistakes I've made, but I just want to know how I really should go about debugging.

For example, I'm trying to change some text. Error text in this case. in the validator.py file there's this line under the VUname class:

            try:
                a = Account._by_name(user_name, True)
                return self.error(errors.USERNAME_TAKEN)
            except NotFound:
                return user_name

I changed that to:

            try:
                a = Account._by_name(user_name, True)
                return self.error(errors.USERNAME_TAKEN_DEL)
            except NotFound:
                return user_name

and added this in errors.py:

     ('USERNAME_TAKEN_DEL', _('that username is taken by a user who has deleted')),

and added this in login.html:

     ${error_field(""USERNAME_TAKEN_DEL"", ""user"", kind=""span"")}

svc -t etc and then try to register with an existing username and I get

&gt;an error occurred while posting (status: 500)

Watching the logs gives me
&gt;Debug at: http://reddit.local/_debug/view/1303852567

So I bring that up and find

    File '/home/reddit/reddit/r2/r2/controllers/validator/validator.py', line 164 in newfn
      simple_vals, param_vals, *a, **kw)
    File '/home/reddit/reddit/r2/r2/controllers/validator/validator.py', line 202 in validatedForm
      val = self_method(self, form, responder, *a, **kw)
    File '/home/reddit/reddit/r2/r2/controllers/api.py', line 401 in POST_register
      user = register(name, password)
    File '/home/reddit/reddit/r2/r2/models/account.py', line 477 in register
      a = Account._by_name(name)
    File '/home/reddit/reddit/r2/r2/models/account.py', line 217 in _by_name
      uid = cls._by_name_cache(name.lower(), allow_deleted, _update = _update)
    AttributeError: 'NoneType' object has no attribute 'lower'

Fairly used to traces at least in gdb, so I understand that name seems to be null or the sort. But why? Account._by_name is called before my new var is even called.

Ah no wait, it's a different call to _by_name in account.py, hm. I'm going to guess it's failing to return the new text error then and continuing to attempt to create the account?

[EDIT] There we go, found it. I missed the line in api.py where it makes the form actually return the error.

Okay so am I debugging the right way? Is there an actual log file that gets written to on errors like the 500? Or do I just have to watch service/app/log/main/current?",,False,,t5_2qizd,True,,,True,t3_hzoz6,http://www.reddit.com/r/redditdev/comments/hzoz6/debugging/,
1301108211.0,3,self.redditdev,gbq48,Is there any way to retrieve more than the last 1000 entries from a subreddit?,5,2,10,http://www.reddit.com/r/redditdev/comments/gbq48/is_there_any_way_to_retrieve_more_than_the_last/,"There used to be a search option that could do this, but it was removed. Any ideas?",,False,,t5_2qizd,False,,,True,t3_gbq48,http://www.reddit.com/r/redditdev/comments/gbq48/is_there_any_way_to_retrieve_more_than_the_last/,
1298180415.0,4,self.redditdev,fov54,Trying to use reddit API from C#,8,4,4,http://www.reddit.com/r/redditdev/comments/fov54/trying_to_use_reddit_api_from_c/,"I wanted to create a simple system tray program to check my inbox but I'm doing something horribly wrong.

[Code](http://codepaste.net/47djv6). 

The problem I'm having is that it keeps telling me that I'm ""trying to do that too often"". I forgot the exact json error but that's basically it. I have waited out the timeout (it initially told me to wait 1 minute.. then 2.. then 30) but it just keeps rising, even if I wait it out completely. I'm currently on a 5 hour lockout :| (posting this from laptop where I'm logged in). I tried this on a different IP with a different username and STILL got the ""trying to do that too often"" error on first try!

I have tried with and without keepalive, useragent and api_type.. Not sure what to try next.. This all seems very correct :| I'm wondering if I screwed myself over somehow when I was trying this initially and got myself blacklisted. That doesn't explain why I get same behavior from different ip/username though.",,False,,t5_2qizd,False,,,True,t3_fov54,http://www.reddit.com/r/redditdev/comments/fov54/trying_to_use_reddit_api_from_c/,
1297217345.0,3,self.redditdev,fhvxy,"Where can I find a basic, easy to understand, description of the API content attributes such as ""created"", ""created_utc"" etc...?",4,1,7,http://www.reddit.com/r/redditdev/comments/fhvxy/where_can_i_find_a_basic_easy_to_understand/,,,False,,t5_2qizd,False,,,True,t3_fhvxy,http://www.reddit.com/r/redditdev/comments/fhvxy/where_can_i_find_a_basic_easy_to_understand/,
1287446987.0,4,self.redditdev,dt2i2,How would Reddit feel about an image repost identifier using hashes?,6,2,10,http://www.reddit.com/r/redditdev/comments/dt2i2/how_would_reddit_feel_about_an_image_repost/,"Obviously it wouldn't be able to catch everything, but here's what I'm thinking.

User posts an image to any subreddit (/r/pics for example), in the ""check for repost"" algorithm, it not only checks to see if the URL is unique, but if a hash of the image already exists, if found it brings you to the usual ""this may be a repost"" page, with the option to post anyway (if the original post is old enough maybe?).

Anyway, I'm downloading the Developer VM right now, and don't think it would be that hard to implement (I've been a software engineer for 5 years), so I probably will either way, but what are the chances of getting it merged into the official branch?",,False,,t5_2qizd,False,,,True,t3_dt2i2,http://www.reddit.com/r/redditdev/comments/dt2i2/how_would_reddit_feel_about_an_image_repost/,
1285565321.0,2,self.redditdev,djcht,Just launched a new Reddit client -- www.catchthecloud.com.  Thanks for the API!,9,7,8,http://www.reddit.com/r/redditdev/comments/djcht/just_launched_a_new_reddit_client/,"Hey!

I just wanted to say thanks for making an easy API to implement.  It's been easier to get Reddit working on our app than any other one I've worked with so far.  Rock!


Check it out -- http://www.catchthecloud.com/assets/movies/Reddit_Basics.htm",,False,,t5_2qizd,False,,,True,t3_djcht,http://www.reddit.com/r/redditdev/comments/djcht/just_launched_a_new_reddit_client/,
1280239939.0,4,self.redditdev,cu88w,"Although I've created subreddits on my reddit, I am unable to see them in the reddit title bar, or search for them. What can I do?",5,1,3,http://www.reddit.com/r/redditdev/comments/cu88w/although_ive_created_subreddits_on_my_reddit_i_am/,"To be specific, all three reddits have the option to be viewed as public and to be shown in the default set. While entries into those reddits do appear on the main page, the titles do not display in the title bar at the top, nor can I search for them. 

What can I look at, to get a better understanding of what's going on?",,False,,t5_2qizd,False,,,True,t3_cu88w,http://www.reddit.com/r/redditdev/comments/cu88w/although_ive_created_subreddits_on_my_reddit_i_am/,
1275274575.0,5,self.redditdev,c9rt4,"So what's the status on Reddit for the Blackberry?  I remember people saying they were going to make it a few months ago, but I never heard a followup.",7,2,6,http://www.reddit.com/r/redditdev/comments/c9rt4/so_whats_the_status_on_reddit_for_the_blackberry/,"There is m.reddit.com, but I can't submit or vote on anything with that, which takes out half the fun of using reddit.",,False,,t5_2qizd,False,,,True,t3_c9rt4,http://www.reddit.com/r/redditdev/comments/c9rt4/so_whats_the_status_on_reddit_for_the_blackberry/,
1272236268.0,4,self.redditdev,bvzah,How do I submit a post (specifically a self post) with the API?,4,0,6,http://www.reddit.com/r/redditdev/comments/bvzah/how_do_i_submit_a_post_specifically_a_self_post/,"Also, how can I tell if a submission is a self post? checking the domain for `'self.' + submisssion['data']['subreddit']` isn't perfect as `self.reddit.com`, etc. are valid domains and `submission['data']['url'] == 'http://www.reddit.com' + submission['data']['permalink']` doesn't work because I am using multiple subreddits (the URL will have `/r/firstsubreddit+secondsubreddit+...` and the permalink won't).

**Edit**: Title problem solved. New problem: Why does my bot have to complete a CAPTCHA to submit a post to a subreddit it's a moderator of? Also how do I handle said CAPTCHA? The JSON response contains a bunch of numbers and a string random characters, but I have no idea what any of it means.

**Edit Edit**: Problem solved. KeyserSosa just now implemented an `is_self` property for submissions in the JSON. Also, ketralnis helped me resolve the CAPTCHA problem.",,False,,t5_2qizd,True,,,True,t3_bvzah,http://www.reddit.com/r/redditdev/comments/bvzah/how_do_i_submit_a_post_specifically_a_self_post/,
1272163985.0,4,self.redditdev,bvpu5,getting error in SQLAlchemy for reddit ubuntu setup,5,1,6,http://www.reddit.com/r/redditdev/comments/bvpu5/getting_error_in_sqlalchemy_for_reddit_ubuntu/,"I followed the [ubuntu setup](http://code.reddit.com/wiki/RedditStartToFinishIntrepid) and when I try to run the following command:

paster shell example.ini

I get this error:

file ""/usr/local/lib/python2.6/dist-packages/SQLAlchemy-0.5.3-py2.6.egg/sqlalchemy/engine/base.py"", line 931, in _handle_dbapi_exception
raise exc.DBAPIError.instance(statement, parameters, e, connection_invalidated=is_disconnect)
sqlalchemy.exc.ProgrammingError: (ProgrammingError) function ip_network(character varying) does not exist
LINE 1: ...p_network_reddit_data_award on reddit_data_award (ip_network...
                                                         ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
""create index idx_ip_network_reddit_data_award on reddit_data_award (ip_network(value)) where key = 'ip'"" {}

Am I doing something wrong?

Thanks.",,False,,t5_2qizd,False,,,True,t3_bvpu5,http://www.reddit.com/r/redditdev/comments/bvpu5/getting_error_in_sqlalchemy_for_reddit_ubuntu/,
1269979196.0,4,self.redditdev,bkclh,Are there any formal/informal start to finish instructions for hosting a reddit on the Amazon cloud?,6,2,1,http://www.reddit.com/r/redditdev/comments/bkclh/are_there_any_formalinformal_start_to_finish/,,,False,,t5_2qizd,False,,,True,t3_bkclh,http://www.reddit.com/r/redditdev/comments/bkclh/are_there_any_formalinformal_start_to_finish/,
1267581035.0,4,self.redditdev,b8hqa,Sending messages with in reddit via php / java from another site.,5,1,0,http://www.reddit.com/r/redditdev/comments/b8hqa/sending_messages_with_in_reddit_via_php_java_from/,"Is this possible? I was able to find a way to check if a user exists and obtain their time on reddit and karma. But our little project would greatly benefit from being able to send messages to the users through reddit.

I have no Python experience, but was hoping there is something already out there that I could copy paste and hack it into what I need it to do.",,False,,t5_2qizd,False,,,True,t3_b8hqa,http://www.reddit.com/r/redditdev/comments/b8hqa/sending_messages_with_in_reddit_via_php_java_from/,
1264123772.0,3,self.redditdev,asmwy,"Does anybody check the ""translation errors"" bugs",4,1,0,http://www.reddit.com/r/redditdev/comments/asmwy/does_anybody_check_the_translation_errors_bugs/,"I've submitted two tickets on code.reddit.com (#120 and #657). I'd actually completely forgotten about the first ticket it was so long ago. Anyways they've just been sitting there and nothing has happened to them, there isn't even anyone assigned.",,False,,t5_2qizd,False,,,True,t3_asmwy,http://www.reddit.com/r/redditdev/comments/asmwy/does_anybody_check_the_translation_errors_bugs/,
1257305262.0,3,self.redditdev,a0pzx,"New Here, will somebody point to what i need to know about in the code base?",5,2,3,http://www.reddit.com/r/redditdev/comments/a0pzx/new_here_will_somebody_point_to_what_i_need_to/,"I'll be the first to admit it, I'm no rockstar programmer. I can hack my way around things, but true mastery eludes me. I'm looking for a good jumping off place to really dive into how this reddit thing works. I understand things best if I can see a path, so a beginning to end would be helpful. My background is primarily front-end but I am grasping at lofty reticulated pythons here. 

From my initial glimpses, I think it's some of the most beautiful code I've ever seen, because all the cogs I work with are digital representations of my own malformed inane dribble. 

Thanks for your input.",,False,,t5_2qizd,False,,,True,t3_a0pzx,http://www.reddit.com/r/redditdev/comments/a0pzx/new_here_will_somebody_point_to_what_i_need_to/,
1252960217.0,5,reddit.com,9kifw,I think I found a bug in some URLs,5,0,4,http://www.reddit.com/r/redditdev/comments/9kifw/i_think_i_found_a_bug_in_some_urls/,,,False,,t5_2qizd,False,,,False,t3_9kifw,http://www.reddit.com/r/technology/comments/7rg8k/how_come_laptops_are_vastly_more_powerful_and/c07atgh,
1250929721.0,5,self.redditdev,9d1o7,Getting info about an URL with the API doesn't work with .json or .rss or .xml,5,0,3,http://www.reddit.com/r/redditdev/comments/9d1o7/getting_info_about_an_url_with_the_api_doesnt/,"Hi all,
I read [in the wiki](http://code.reddit.com/wiki/API) that you can do something like:

    http://www.reddit.com/api/info?url=URL.EXTENSION

Where EXTENSION is one of .json, .rss, .xml.

That doesn't seem to work, as, I guess, the dot and the extension are parsed as part of the URL.

I'm interested in using the API to find the number of ups, downs and comments on a story, knowing the URL. If there are multiple submissions, I only care about the first one (chronologically).

I noticed that I can do that if I know the id of the story, but how can I know that using the API?

Thanks!",,False,,t5_2qizd,False,,,True,t3_9d1o7,http://www.reddit.com/r/redditdev/comments/9d1o7/getting_info_about_an_url_with_the_api_doesnt/,
1250289645.0,4,self.redditdev,9aru7,"Hey Redditdev, I want to be able to get an RSS feed for something more than the first 100 items on my user overview page (all of them) - is that possible?",7,3,2,http://www.reddit.com/r/redditdev/comments/9aru7/hey_redditdev_i_want_to_be_able_to_get_an_rss/,,,False,,t5_2qizd,False,,,True,t3_9aru7,http://www.reddit.com/r/redditdev/comments/9aru7/hey_redditdev_i_want_to_be_able_to_get_an_rss/,
1246956714.0,4,self.redditdev,8yvzs,KeyError in memcache.py,4,0,3,http://www.reddit.com/r/redditdev/comments/8yvzs/keyerror_in_memcachepy/,"Hello, in my reddit installation, I get [this error](http://reddit.pastebin.com/f18fd17a) fairly often (especially when I submit a link, and then get redirected to the new page where I can comment). Has anybody seen it?

",,False,,t5_2qizd,False,,,True,t3_8yvzs,http://www.reddit.com/r/redditdev/comments/8yvzs/keyerror_in_memcachepy/,
1246351998.0,4,self.redditdev,8wwka,Move a post to a different reddit. Possible?,5,1,2,http://www.reddit.com/r/redditdev/comments/8wwka/move_a_post_to_a_different_reddit_possible/,"Hi. I was thinking, can a moderator move a post to a different reddit, because it's not appropriate where it was posted? Thanks.",,False,,t5_2qizd,False,,,True,t3_8wwka,http://www.reddit.com/r/redditdev/comments/8wwka/move_a_post_to_a_different_reddit_possible/,
1246061720.0,2,self.redditdev,8w2dg,"Right-clicking on a link adds it to the ""Recently viewed links"" list.",4,2,0,http://www.reddit.com/r/redditdev/comments/8w2dg/rightclicking_on_a_link_adds_it_to_the_recently/,"Whether I actually view the link or not, right-clicking on it is enough to be added to the ""Recently viewed links"" list. Sometimes this is confusing, especially if I want to actually go to the last viewed link. Even canceling a click (clicking then dragging or something) will add the link to the recently viewed list.",,False,,t5_2qizd,True,,,True,t3_8w2dg,http://www.reddit.com/r/redditdev/comments/8w2dg/rightclicking_on_a_link_adds_it_to_the_recently/,
1236539853.0,3,self.redditdev,831tv,Occasionally thumbnail scraped is inappropriate; Nice feature would be optionally disable it per-submission,6,3,4,http://www.reddit.com/r/redditdev/comments/831tv/occasionally_thumbnail_scraped_is_inappropriate/,,,False,,t5_2qizd,False,,,True,t3_831tv,http://www.reddit.com/r/redditdev/comments/831tv/occasionally_thumbnail_scraped_is_inappropriate/,
1231820950.0,4,self.redditdev,7paks,you are trying to submit too fast,10,6,19,http://www.reddit.com/r/redditdev/comments/7paks/you_are_trying_to_submit_too_fast/,,,False,,t5_2qizd,False,,,True,t3_7paks,http://www.reddit.com/r/redditdev/comments/7paks/you_are_trying_to_submit_too_fast/,
1230165062.0,4,self.redditdev,7ll11,The ad on the right hand side of the page sometimes appears on the bottom right UPSIDE DOWN!,4,0,5,http://www.reddit.com/r/redditdev/comments/7ll11/the_ad_on_the_right_hand_side_of_the_page/,,,False,,t5_2qizd,False,,,True,t3_7ll11,http://www.reddit.com/r/redditdev/comments/7ll11/the_ad_on_the_right_hand_side_of_the_page/,
1217691598.0,6,self.redditdev,6ulq1,Submitting to non-subscribed subreddits,10,4,7,http://www.reddit.com/r/redditdev/comments/6ulq1/submitting_to_nonsubscribed_subreddits/,,,False,,t5_2qizd,False,,,True,t3_6ulq1,http://www.reddit.com/r/redditdev/comments/6ulq1/submitting_to_nonsubscribed_subreddits/,
1376488270.0,3,self.redditdev,1kciqy,Willing to learn more about the Parent&gt;Child ordering of comments,3,0,3,http://www.reddit.com/r/redditdev/comments/1kciqy/willing_to_learn_more_about_the_parentchild/,"Hi!

I'm learning more about the way Reddit is built and how it works. I however cannot find anything about the ordering of comments. Especially with the parent&gt;child ordering. Can anyone point me to this code or explain it to me using some examples or external references?

Thanks in advance!",,False,,t5_2qizd,False,,,True,t3_1kciqy,http://www.reddit.com/r/redditdev/comments/1kciqy/willing_to_learn_more_about_the_parentchild/,
1376276886.0,3,self.redditdev,1k6lz5,How can I build comment permalink from ids?,3,0,5,http://www.reddit.com/r/redditdev/comments/1k6lz5/how_can_i_build_comment_permalink_from_ids/,I can query the API for comments but I'm unsure of how to build the permalink to that comment.  Probably a stupid question but can someone point me in the right direction?,,False,,t5_2qizd,False,,,True,t3_1k6lz5,http://www.reddit.com/r/redditdev/comments/1k6lz5/how_can_i_build_comment_permalink_from_ids/,
1376241102.0,4,self.redditdev,1k5h9v,reddit should offer serious API developers a price tier plan that allows more API calls or custom API calls -- a win/win for the developer and reddit.,8,4,1,http://www.reddit.com/r/redditdev/comments/1k5h9v/reddit_should_offer_serious_api_developers_a/,"I am the creator and developer for redditanalytics.com (currently down as I'm moving things to a better server).  

I really think that Reddit could draw in more income by setting up a price-tier for developers who wish to provide value-adds for reddit.  It's a win/win for reddit and developers.

As a serious developer with some large projects on the horizon, I would happily pay $20+ a month if I had more access to the reddit api -- including an increase in the rate-limit for API access.  

Is this a thought that the reddit admins are entertaining?  I have a list of API enhancements that would really make my (our) job as a developer much easier.",,False,,t5_2qizd,False,,,True,t3_1k5h9v,http://www.reddit.com/r/redditdev/comments/1k5h9v/reddit_should_offer_serious_api_developers_a/,
1375549936.0,3,self.redditdev,1jmtw1,How to use the new multireddit APIs?,3,0,5,http://www.reddit.com/r/redditdev/comments/1jmtw1/how_to_use_the_new_multireddit_apis/,"Im currently unable to get them to work, Ive looked up the API documentation at http://www.reddit.com/dev/api#GET_api_multi_my_multis but whatever I try I get an HTTP error back, whether I try to retreive the subscribed multireddits via my_multis or when I try to subscribe to a new multireddit.

Is there a working sample call out there?",,False,,t5_2qizd,False,,,True,t3_1jmtw1,http://www.reddit.com/r/redditdev/comments/1jmtw1/how_to_use_the_new_multireddit_apis/,
1375460604.0,3,self.redditdev,1jklbu,[PRAW] Is there a function for getting unread mod mail?,3,0,1,http://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/,"Does `get_unread()` also get unread mod mail (it doesn't seem like it should, because it's a part of `praw.__init__.PrivateMessagesMixin(*args, **kwargs)`)? Is there a way to only get unread mod mail through `get_mod_mail()`?

Edit: Thinking about it further, I think there are some apps out there that provide notifications for new mod mail, so surely this must be possible somehow (?)",,False,,t5_2qizd,False,,,True,t3_1jklbu,http://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/,
1375329489.0,2,self.redditdev,1jgzrc,Is there a way to retrieve information from a private subreddit?,6,4,1,http://www.reddit.com/r/redditdev/comments/1jgzrc/is_there_a_way_to_retrieve_information_from_a/,I know some subreddits are private. Is there a way to get information these subs?,,False,,t5_2qizd,False,,,True,t3_1jgzrc,http://www.reddit.com/r/redditdev/comments/1jgzrc/is_there_a_way_to_retrieve_information_from_a/,
1375282038.0,3,self.redditdev,1jf95w,Empty api response,4,1,5,http://www.reddit.com/r/redditdev/comments/1jf95w/empty_api_response/,"Hello. Very very basic question: I'm playing with a script and can't get the reddit api to talk to me, it sends back an empty response with http status 302. If I paste the same url into a browser, I get a response. Clues?  
  
I thought it might be the user agent, which I wasn't reporting originally, but now I'm sending ""nodejsfun/1.0 by younan"".  
  
I'm just trying to fetch /user/&lt;user name&gt;/about.json. Any help would be appreciated, and sorry for being so new and ignorant. ",,False,,t5_2qizd,False,,,True,t3_1jf95w,http://www.reddit.com/r/redditdev/comments/1jf95w/empty_api_response/,
1374775886.0,3,self.redditdev,1j1j6q,[PRAW] RedirectException when searching for URL if there's only one result.,4,1,2,http://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/,"Example: Searching for http://www.livememe.com/vg972qp in /r/AdviceAnimals.

Is it possible to get the submission object in such a case?

    RedirectException: Unexpected redirect from http://www.reddit.com/r/all/search/.json?q=http%3A%2F%2Fwww.livememe.com%2Fvg972qp&amp;sort=new to http://www.reddit.com/submit.json?url=http%3A%2F%2Fwww.livememe.com%2Fvg972qp

P.S.: I'm still looking for an answer to [this question](http://www.reddit.com/r/redditdev/comments/1hvgjf/praw_actually_getting_the_top_200_submissions/) as well. It was caught by the spam filter and unfortunately to this day, the mods didn't find the time to approve it.",,False,,t5_2qizd,1374784926.0,,,True,t3_1j1j6q,http://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/,
1374722908.0,3,self.redditdev,1j067o,Reddit OAuthProvider redirect_uri troubles,3,0,4,http://www.reddit.com/r/redditdev/comments/1j067o/reddit_oauthprovider_redirect_uri_troubles/,"i can't seem to get the redirect_uri to work. Weird thing is, it worked **once**, but not since. 

example of the uri i'm using:

https://ssl.reddit.com/api/v1/authorize?client_id=gCChBhMHHo525Q&amp;redirect_uri=http%3a%2f%2fmarvelchampions.com%2fAccount%2fExternalLoginCallback%3f__provider__%3dReddit%26__sid__%3d9d396c1146784a08bc5386d97804a4dd&amp;state=cbfdb659-834b-4171-a15b-d78b06e7b06d&amp;scope=identity&amp;response_type=code&amp;duration=permanent


in my app settings, i have tried the following:

* http://marvelchampions.com/Account/ExternalLoginCallback
* http://marvelchampions.com/
* http://marvelchampions.com
* http://marvelchampions.com/Account/ExternalLoginCallback?

none of which seem to work.

i always get the same error:

     invalid redirect_uri parameter.

what am i doing wrong? i'm new to oath and have no idea ",,False,,t5_2qizd,1374724344.0,,,True,t3_1j067o,http://www.reddit.com/r/redditdev/comments/1j067o/reddit_oauthprovider_redirect_uri_troubles/,
1374616490.0,3,self.redditdev,1iwwds,whats a better way to grab random pictures from a subreddit,4,1,1,http://www.reddit.com/r/redditdev/comments/1iwwds/whats_a_better_way_to_grab_random_pictures_from_a/,"I currently do this
  
    rand = random.randint(0,99)
    r = requests.get('http://www.reddit.com/r/{subreddit}.json?  
                           limit=100'.format(subreddit=srr))
    pic = j['data']['children'][rand]['data']['url']

I keep getting the same images, so I tried increasing it to 1000 but it says index out of range",,False,,t5_2qizd,False,,,True,t3_1iwwds,http://www.reddit.com/r/redditdev/comments/1iwwds/whats_a_better_way_to_grab_random_pictures_from_a/,
1373546519.0,3,self.redditdev,1i2rd8,Fetching liked threads / comments,3,0,0,http://www.reddit.com/r/redditdev/comments/1i2rd8/fetching_liked_threads_comments/,"Hi,

Is there a way to fetch liked threads &amp; comments from an user using OAuth ? I've checked [the API](http://www.reddit.com/dev/api/oauth) but didn't see anything, which seems strange (how works unofficial clients ? [/user/*/liked](http://www.reddit.com/dev/api#GET_user_{username}_liked) only works for logged in users).
",,False,,t5_2qizd,False,,,True,t3_1i2rd8,http://www.reddit.com/r/redditdev/comments/1i2rd8/fetching_liked_threads_comments/,
1372951718.0,4,self.redditdev,1hmyl0,Has anyone implemented Reddit OAuth on android? I've some doubts regarding the redirect_uri.,7,3,2,http://www.reddit.com/r/redditdev/comments/1hmyl0/has_anyone_implemented_reddit_oauth_on_android/,"I'm getting an error such as this: Invalid redirect_uri parameter. 

Hence, I'm a bit confused how this works. I've used this as my redirect_uri: reddit:authorizationFinished
With ""reddit"" being in the Android:scheme in the manifest.xml of the app so that I can receive the redirect uri. But I keep getting forbidden request from reddit. 
Help?",,False,,t5_2qizd,False,,,True,t3_1hmyl0,http://www.reddit.com/r/redditdev/comments/1hmyl0/has_anyone_implemented_reddit_oauth_on_android/,
1372604589.0,3,self.redditdev,1hd7kx,API: How to get 20 most recent posts in a sub reddit that contain a key word,4,1,3,http://www.reddit.com/r/redditdev/comments/1hd7kx/api_how_to_get_20_most_recent_posts_in_a_sub/,"I'm looking to monitor a particular sub reddit for posts that touch on a particular subject.

Example: The most recent 20 submissions to r/pics where the title includes the word 'kitten'

I have no interest in kittens, but you get the idea. I often miss checking reddit for a few days, and with posts being down voted or dropping off the front page, it's easy to miss something.  Any help would be appreciated.",,False,,t5_2qizd,False,,,True,t3_1hd7kx,http://www.reddit.com/r/redditdev/comments/1hd7kx/api_how_to_get_20_most_recent_posts_in_a_sub/,
1372579021.0,3,self.redditdev,1hctu5,Installing PRAW properly,4,1,4,http://www.reddit.com/r/redditdev/comments/1hctu5/installing_praw_properly/,"Hey,

I am trying to install PRAW and use it, and have been following the documentation closely. I am following this tutorial: https://github.com/praw-dev/praw/blob/master/docs/pages/writing_a_bot.rst

 If I go into IDLE, and type ""import praw"", it works, but if I make a filename.py, it gives me this: 

Traceback (most recent call last):
  File ""C:\Users\Nat\Desktop\praw.py"", line 10, in &lt;module&gt;
    import praw
  File ""C:\Users\Nat\Desktop\praw.py"", line 12, in &lt;module&gt;
    r = praw.Reddit('NSA related-question monitor by /u/natzim v 1.0.')
AttributeError: 'module' object has no attribute 'Reddit'

Thanks for the help",,False,,t5_2qizd,False,,,True,t3_1hctu5,http://www.reddit.com/r/redditdev/comments/1hctu5/installing_praw_properly/,
1371883761.0,3,self.redditdev,1gufrm,PRAW: logic to examine comment score?,3,0,2,http://www.reddit.com/r/redditdev/comments/1gufrm/praw_logic_to_examine_comment_score/,"Nooby here. Basically what I want to do is retrieve reddit.com/comments, look at the comment scores and take and action according to the score value. When "" r.get_all_comments() "" is used, only 25 comments load(I think) each time that the function is called. This and the fact that so many comments are submitted on reddit each second means there is no time for anyone to upvote/downvote a comment that is retrieved with this function. All the scores will be at ""1"".

Anyone have an idea of a way to remedy this? Maybe a way to update the retrieved comments' information after a certain amount of time? Any info is much appreciated! 
",,False,,t5_2qizd,False,,,True,t3_1gufrm,http://www.reddit.com/r/redditdev/comments/1gufrm/praw_logic_to_examine_comment_score/,
1371738809.0,3,self.redditdev,1gq8pz,"Using RedditAPI, how do you list specific subreddits?",3,0,6,http://www.reddit.com/r/redditdev/comments/1gq8pz/using_redditapi_how_do_you_list_specific/,"Hello again redditdevs!

I'm just wondering if there's a way to use the reddit api in order to display information of specific subreddits?

For example, I'd access:

api.reddit.com/r/example**?filter=askReddit,technology,funny**

and the server would respond with a collection containing the subreddits **askReddit, technology** and **funny**.

Any help would be much appreciated!",,False,,t5_2qizd,False,,,True,t3_1gq8pz,http://www.reddit.com/r/redditdev/comments/1gq8pz/using_redditapi_how_do_you_list_specific/,
1371150460.0,2,alienstream.com,1gabm8,"Meet AlienStream, An online audioplayer for reddit, with mobile compatibility.",5,3,10,http://www.reddit.com/r/redditdev/comments/1gabm8/meet_alienstream_an_online_audioplayer_for_reddit/,,,False,,t5_2qizd,False,,,False,t3_1gabm8,http://alienstream.com,
1371085890.0,3,self.redditdev,1g8jwy,"Suggested new keys for comment objects -- author_karma, author_created_utc, etc.",6,3,10,http://www.reddit.com/r/redditdev/comments/1g8jwy/suggested_new_keys_for_comment_objects_author/,"Here are some suggestions for new keys in the comment JSON objects.  I feel that these additional keys would help filter quality content for developers by allowing the developer the choice to ingest comments that meet a certain criteria based on the author's previous history.


**author_karma** -- (signed int) This key would show the approximate total karma for the user.  This value would not need to be constantly updated, but could be cached.  This would work in conjunction with ...

**author_total_posts**  (unsigned int) This key would show the approximate total posts made by author and could be cached.  From the previous two keys, a developer could calculate an author's average karma per post and choose an appropriate threshold for filtering comments based on these keys.

**author_created_utc** (timestamp) The date/time the user account was created.  Also would help with filtering by allowing the developer to filter based on the age of the account.

The first two keys could be condensed to a **author_avg_karma** key.  

Thoughts?
",,False,,t5_2qizd,1371086091.0,,,True,t3_1g8jwy,http://www.reddit.com/r/redditdev/comments/1g8jwy/suggested_new_keys_for_comment_objects_author/,
1370730153.0,3,self.redditdev,1fy6pr,API to get titles for a given list of link ids?,3,0,4,http://www.reddit.com/r/redditdev/comments/1fy6pr/api_to_get_titles_for_a_given_list_of_link_ids/,"Does anyone know if there is an API that returns the titles for a given list of link ids? 

For my Win8 app ReddHub, I store link_ids of links read, and want to show people the corresponding titles. I could do this naively and request the title for each, which would be slow, or I could save the titles - but an API that did this would make life easy :)",,False,,t5_2qizd,False,,,True,t3_1fy6pr,http://www.reddit.com/r/redditdev/comments/1fy6pr/api_to_get_titles_for_a_given_list_of_link_ids/,
1370541310.0,3,self.redditdev,1fsz2m,Storing PRAW objects?,4,1,5,http://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/,"Hi,

Is there a way to store an instance of a Reddit object? I've tried pickle, but when I load the pickled data I get a 'maximum recursion depth exceeded' exception:

    File ""praw/objects.py"", line 78, in __getattr__
        if not self._populated:
    RuntimeError: maximum recursion depth exceeded while calling a Python object

Is there another way to store and reuse a logged in session?

Thanks

Edit: Writing out the question got me to think about it, and solve it. Sorry for jumping the gun. Does this look right?

    def init():
        global r
        handler = MultiprocessHandler('127.0.0.1', 6000)
        r = praw.Reddit(user_agent=conf.Get('useragent'), site_name='reddit_nossl', handler=handler)
        
    def login_store():
        init()
        r.login(conf.Get('username'), conf.Get('password'))
        store = {'http':r.http, 'modhash':r.modhash}
        pickleFile = open(pickleFileName, 'wb')
        pickle.dump(store, pickleFile, pickle.HIGHEST_PROTOCOL)
        pickleFile.close()    
        
    def read_load():
        pickleFile = open(pickleFileName, 'rb')
        data = pickle.load(pickleFile)
        pickleFile.close()
        
        init()
        r.http = data['http']
        r.modhash = data['modhash']
        r._authentication = True
    
        msgs = r.get_inbox()
        for msg in msgs:
            print msg.body  ",,False,,t5_2qizd,1370542200.0,,,True,t3_1fsz2m,http://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/,
1369952173.0,3,self.redditdev,1fd6ky,How do I get more than 25 comments at a time?,4,1,4,http://www.reddit.com/r/redditdev/comments/1fd6ky/how_do_i_get_more_than_25_comments_at_a_time/,"I'm doing a simple this to get them. 

` subreddit_comments = subreddit.get_comments()`

I guess I'm just confused on how to get more of them. Thanks. 
",,False,,t5_2qizd,False,,,True,t3_1fd6ky,http://www.reddit.com/r/redditdev/comments/1fd6ky/how_do_i_get_more_than_25_comments_at_a_time/,
1369620817.0,3,self.redditdev,1f44f6,Developing a bot?,3,0,4,http://www.reddit.com/r/redditdev/comments/1f44f6/developing_a_bot/,"Sorry if I'm posting in the wrong place, but this seemed like the best place to ask about that.

I want to develop a bot that will work within a specific subreddit and it must be able to do the following:

* Check the subreddit for posts tagged with a certain string.
* Send and receive messages to and from the members.
* Comment on posts.

I don't know where to start, I am a programmer, but don't know how to use the API. I'd like to make it as a Java Application that I could keep running on my machine, monitoring the subreddit and the bot's inbox, but I can work with other languages if necessary.

Thanks",,False,,t5_2qizd,False,,,True,t3_1f44f6,http://www.reddit.com/r/redditdev/comments/1f44f6/developing_a_bot/,
1369587226.0,3,self.redditdev,1f36b0,Is there a way to detect if a comment has been edited?,4,1,6,http://www.reddit.com/r/redditdev/comments/1f36b0/is_there_a_way_to_detect_if_a_comment_has_been/,"I checked the API and it doesn't look like there is but I was wondering if it is supported directly by the API, perhaps even as a hidden feature. I would even settle for checking the timestamp of when it was modified but it appears that isn't supported either. Does anyone have a solution to tell if the comment has been edited?",,False,,t5_2qizd,False,,,True,t3_1f36b0,http://www.reddit.com/r/redditdev/comments/1f36b0/is_there_a_way_to_detect_if_a_comment_has_been/,
1369526469.0,3,self.redditdev,1f20l5,[plugin / feature request] In-page alert when browsing a non-reddit page which has a reddit post about it,3,0,1,http://www.reddit.com/r/redditdev/comments/1f20l5/plugin_feature_request_inpage_alert_when_browsing/,"**Does this already exist?**  
  

**Explanation of feature**  

When browsing any site online, ie. a youtube page or bbc article, you are given an alert if a current reddit post has linked to the article/video/etc. This would operate as an in-browser plugin, and not necessarily a part of RES. *Though I don't know which other plugin developers this post should be seen by.
  
**Why I want this feature**    
I am relatively new to reddit and find it hard to reconcile my new found interest in up-vote based discussions with the time required to simply browse articles from the sites I like. I would love to be able to browse my usual haunts outside of reddit, but to know near-immediately (and automatically) when a reddit discussion based on the very same weblink is waiting for me back on reddit.
  

**Issues**    
Problem: Would the plugin need to check against some sort of library file? If so, this might take up to much resources
Solution: Limit the library to only logging reddit posts which contain a certain minimum amount of comments (ie 1, 5, or 10). Also, limit the library to only retaining records of posts which were created in the last x amount of months (ie 1)
    
**Disclaimers**    
a. This is my 2nd post ever. I spent time trying to find the most appropriate place for this post and /r/Enhancement seemed right. I'm not sure if there is a general plugin developers subreddit somewhere.  
b. I have basically no technical knowledge to know whether this idea is even possible. But i sure as hell couldn't make it.    
**Peace.**",,False,,t5_2qizd,False,,,True,t3_1f20l5,http://www.reddit.com/r/redditdev/comments/1f20l5/plugin_feature_request_inpage_alert_when_browsing/,
1369429389.0,3,self.redditdev,1ezssi,Error with r.login(),6,3,6,http://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/,"When I use r.login('username','password') in the python shell, it works fine and logs me in. However, in a script it gives an error.

This is the only code in my script. Help?
   
    import praw 
    
    r = praw.Reddit(user_agent = 'An automated reddit project - Meeshu')
    r.login('Meeshu','password')",,False,,t5_2qizd,False,,,True,t3_1ezssi,http://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/,
1369346970.0,3,self.redditdev,1exklr,PRAW + Accessing Comments - some questions,3,0,4,http://www.reddit.com/r/redditdev/comments/1exklr/praw_accessing_comments_some_questions/,"1. is the method _insert_comment() called for every comment added to a submission? Are there circumstances where a comment will be added but this is not called?


",,False,,t5_2qizd,False,,,True,t3_1exklr,http://www.reddit.com/r/redditdev/comments/1exklr/praw_accessing_comments_some_questions/,
1369331302.0,3,self.redditdev,1ewzf7,Reddit API returns 411 with length header,3,0,4,http://www.reddit.com/r/redditdev/comments/1ewzf7/reddit_api_returns_411_with_length_header/,"I have been working on a bot for reddit and any time it tries to post a comment it runs into error 411: length required, even though I am setting the request property, I tried the code in the reddit API console and it returns 200 OK.  
The link to the code is [here.](https://github.com/UnacceptableUse/Demobilizerbot/blob/master/src/com/unacceptableuse/demobilizer/Demobilizer.java)
You want the part in *public Boolean comment(...)*  
Also, I know the bot is kinda broken in other ways too, it doesn't filter thing properly and that's fine for testing right now.  
Any help would be greatly appreciated, thanks.",,False,,t5_2qizd,False,,,True,t3_1ewzf7,http://www.reddit.com/r/redditdev/comments/1ewzf7/reddit_api_returns_411_with_length_header/,
1369220535.0,3,self.redditdev,1etsnn,Help on retrieving links beyond the first page,3,0,4,http://www.reddit.com/r/redditdev/comments/1etsnn/help_on_retrieving_links_beyond_the_first_page/,"Hello! 
I know this question have been asked before ([here](http://www.reddit.com/r/redditdev/comments/12j2c6/problem_with_getting_json_from_subreddits_beyond/) and [here](http://www.reddit.com/r/redditdev/comments/d7egb/how_to_get_more_json_results_i_get_only_30/) but i still cannot manage to make my code work.
I'm trying to get links from various subreddit using json.
Here's my code:
    
    song_list = [];
    $.getJSON(
        ""http://www.reddit.com/r/dub.json?jsonp=?"",
        function foo(data){
            song_list.push(data.data);
        }
    )

It's nice, I get an array with all the links inside an object but I the title says, I would like to get more than 25 submissions.
However, when i write

    ""http://www.reddit.com/r/dub&amp;limit=100.json?jsonp=?""

I still get only 25 submissions...
Any help would be gladly appreciated.

",,False,,t5_2qizd,False,,,True,t3_1etsnn,http://www.reddit.com/r/redditdev/comments/1etsnn/help_on_retrieving_links_beyond_the_first_page/,
1369155532.0,3,self.redditdev,1erup9,Why are hot scores computed in memory?,3,0,3,http://www.reddit.com/r/redditdev/comments/1erup9/why_are_hot_scores_computed_in_memory/,"In browsing the reddit code I was surprised to learn that the hot scores for each link are computed and then sorted in memory to obtain the final list. Granted that the results are cached, depending on the value of 'HOT_PAGE_AGE' in app_globals, and of course the subreddit, this could still be a very large number of things. Can anyone explain why the hotness algorithm was architected this way, as opposed to say a database query/view or even a cron job?

SOURCE: https://github.com/reddit/reddit/blob/master/r2/r2/lib/_normalized_hot.pyx

P.S. I was also curious to see the hot function in sql/functions.sql mirrors the one used by the 'get_hot' function in _sorts.pyx, but is not used. Anyone know why?",,False,,t5_2qizd,False,,,True,t3_1erup9,http://www.reddit.com/r/redditdev/comments/1erup9/why_are_hot_scores_computed_in_memory/,
1369072114.0,3,self.redditdev,1epfrj,"Can't sort this one out: trying to log in using PRAW, keep getting ""AttributeError: 'MockRequest' object has no attribute 'origin_req_host'""",4,1,5,http://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/,"I'm just beginning to learn how to use PRAW and python, so this may be something very obvious that I'm missing, but I keep getting this when I try to log in using PRAW:

    C:\Python33\lib\http\client.py:1172: DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers.
      source_address)
    Traceback (most recent call last):
      File ""C:\Python33\Files\Test5.py"", line 11, in &lt;module&gt;
        r.login(USERNAME,PASSWORD) # necessary if your bot will talk to people
      File ""C:\Python33\lib\site-packages\praw\__init__.py"", line 1120, in login
        self.request_json(self.config['login'], data=data)
      File ""C:\Python33\lib\site-packages\praw\decorators.py"", line 95, in wrapped
        return_value = function(reddit_session, *args, **kwargs)
      File ""C:\Python33\lib\site-packages\praw\__init__.py"", line 469, in request_json
        response = self._request(url, params, data)
      File ""C:\Python33\lib\site-packages\praw\__init__.py"", line 342, in _request
        response = handle_redirect()
      File ""C:\Python33\lib\site-packages\praw\__init__.py"", line 315, in handle_redirect
        timeout=timeout, **kwargs)
      File ""C:\Python33\lib\site-packages\praw\handlers.py"", line 135, in wrapped
        result = function(cls, **kwargs)
      File ""C:\Python33\lib\site-packages\praw\handlers.py"", line 54, in wrapped
        return function(cls, **kwargs)
      File ""C:\Python33\lib\site-packages\praw\handlers.py"", line 90, in request
        allow_redirects=False)
      File ""C:\Python33\lib\site-packages\requests\sessions.py"", line 460, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Python33\lib\site-packages\requests\adapters.py"", line 256, in send
        r = self.build_response(request, resp)
      File ""C:\Python33\lib\site-packages\requests\adapters.py"", line 125, in build_response
        extract_cookies_to_jar(response.cookies, req, resp)
      File ""C:\Python33\lib\site-packages\requests\cookies.py"", line 105, in     extract_cookies_to_jar
        jar.extract_cookies(res, req)
      File ""C:\Python33\lib\http\cookiejar.py"", line 1647, in extract_cookies
        if self._policy.set_ok(cookie, request):
      File ""C:\Python33\lib\http\cookiejar.py"", line 931, in set_ok
        if not fn(cookie, request):
      File ""C:\Python33\lib\http\cookiejar.py"", line 952, in set_ok_verifiability
        if request.unverifiable and is_third_party(request):
      File ""C:\Python33\lib\http\cookiejar.py"", line 707, in is_third_party
        if not domain_match(req_host, reach(request.origin_req_host)):
    AttributeError: 'MockRequest' object has no attribute 'origin_req_host'

I did some searching, and the best I could come up with is a problem with requests, but I've installed it and all its requirements and I'm just at a loss. ",,False,,t5_2qizd,False,,,True,t3_1epfrj,http://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/,
1368995351.0,3,self.redditdev,1enepj,RateLimitExceeded problem,3,0,3,http://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/,"I'm trying to comment on a submission, then comment on a comment inside the submission. I made the bot sleep 9 minutes in between both comments, but I still can't seem to shake the RateLimitExceeded exception. So what exactly causes the RateLimitExceeded exception? And how can I avoid it? I was trying to do this on /r/test if it matters at all.

btw I'm using praw.",,False,,t5_2qizd,False,,,True,t3_1enepj,http://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/,
1368979531.0,3,self.redditdev,1emvsg,There is a /listing request and a /hot request in the API that seem to pull the same data. What are the differences?,3,0,6,http://www.reddit.com/r/redditdev/comments/1emvsg/there_is_a_listing_request_and_a_hot_request_in/,Can't find the answer to this anywhere.,,False,,t5_2qizd,False,,,True,t3_1emvsg,http://www.reddit.com/r/redditdev/comments/1emvsg/there_is_a_listing_request_and_a_hot_request_in/,
1368201440.0,3,self.redditdev,1e2ozt,"Is the iReddit app repo abandoned, or just really old?",5,2,1,http://www.reddit.com/r/redditdev/comments/1e2ozt/is_the_ireddit_app_repo_abandoned_or_just_really/,"I wanted to add a simple copy URL feature to the iReddit app which is hosted [here] (http://GitHub.com/Reddit/iReddit) and I was curious as to the state of the repo. Is it old and just hasn't seen any activity in over a year, or did it relocate to somewhere else?

I just don't want to submit my pull request to the wrong place.

Thanks guys,
Evin. ",,False,,t5_2qizd,False,,,True,t3_1e2ozt,http://www.reddit.com/r/redditdev/comments/1e2ozt/is_the_ireddit_app_repo_abandoned_or_just_really/,
1367094746.0,3,self.redditdev,1d8fko,"Clarification on the ""2 seconds per request"" API limit",3,0,0,http://www.reddit.com/r/redditdev/comments/1d8fko/clarification_on_the_2_seconds_per_request_api/,"I searched for an answer but did not see anything relevant.

I'm wondering if the response time should be included in the request time.

## Examples

\#1 Consider response time as part of the 'request':

    reddit.get('/r/all/comments') # synchronous call, waits for response
    sleep(2)
    reddit.get_next() # request more from the server

\#2 Ignore response time

    time_at_request = time.time()
    reddit.get('/r/all/comments') # synchronous call, waits for response
    time_spent_in_request = time.time() - time_at_request
    if time_spent_in_request &lt; 2:
        sleep(2 - time_spent_in_request)
    reddit.get_next() # request more from the server

The first example does not take the response time into consideration. If the server takes 10 seconds to respond, the script will wait an extra 2 seconds before sending another request. Total time between requests is 12 seconds.

The second example calculates the time it takes for the server to respond. If the request takes less than 2 seconds (e.g. 1.5 seconds), the bot will wait for the remainder of the 2 seconds (e.g. 0.5 seconds) before making another request.

I'm inclined to think the second example is OK (within ""2 requests per second"") but I don't want my bot to get banned for rate-limiting.",,False,,t5_2qizd,1367095845.0,,,True,t3_1d8fko,http://www.reddit.com/r/redditdev/comments/1d8fko/clarification_on_the_2_seconds_per_request_api/,
1366873369.0,3,self.redditdev,1d2ino,"Is it possible to fetch a Comment using only its ID, not knowing its submission ID?",3,0,4,http://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/,"I'm using PRAW, but if the only way to do this is by bypassing PRAW, then that'll work too.

For example, suppose we're given a comment ID ""c9m894d"" and we need to figure out some more information about it: its content, author, score, etc. However, we don't know the ID of its submission, so we can't do the `praw.objects.Submission.get_info(r, ""http://www.reddit.com/r/all/comments/""+submission_id+""/_/""+comment_id).comments[0]` thing. Comment IDs are unique, right? So is there a way to retrieve a comment using only its ID?",,False,,t5_2qizd,False,,,True,t3_1d2ino,http://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/,
1365311334.0,4,self.redditdev,1bu7ak,How do i run PRAW on google appengine?,6,2,3,http://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/,"PRAW being a third-party module makes it hard to use it on Google App engine. 

There seem to be two options to be able to use it on app engine - install it in the local directory or use buildout. 

I'm really really confused about how to use buildout for this on app engine and there dont seem to be informative tutorials for this online yet. 

I extracted the egg files and copied them onto my app's directory, but there are persistent problems with dependencies. I've done this method for a lot of other third-party libraries, but i can't seem to get this going with PRAW. 

Has anyone done either of these things? Any tips on what to try next? 

Update: I did manage to get this going. I unzipped all the egg files of the praw installation and put them in the local directory of my app. There is some trouble with the update_checker dependency, and i got rid of it completely. There's an additional problem with obtaining the platform of google app engine while constructing the user agent string. I got rid of that bit as well. Now i have it working fine.

",,False,,t5_2qizd,1365358783.0,,,True,t3_1bu7ak,http://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/,
1364109411.0,4,self.redditdev,1awkvu,Permissions issue when trying to easy-install praw on a hosted web server,5,1,2,http://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/,"I'm trying to run some tests on a shared web server and but I get a permissions error when I try to get praw on the server.

\# easy_install praw

&gt;[Errno 13] Permission denied: '/usr/lib/python2.6/site-packages/test-easy-install-20367.write-test'
&gt;The installation directory you specified (via --install-dir, --prefix, or
the distutils default setting) was:

 &gt;   /usr/lib/python2.6/site-packages/

&gt;Perhaps your account does not have write access to this directory?  If the
installation directory is a system-owned directory, you may need to sign in
as the administrator or ""root"" account.  If you do not have administrative
access to this machine, you may wish to choose a different installation
directory, preferably one that is listed in your PYTHONPATH environment
variable.

I assume this is because I'm on a shared server and don't have permissions into the communal Python directory. How would I go about getting around this issue?",,False,,t5_2qizd,False,,,True,t3_1awkvu,http://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/,
1364092860.0,3,self.redditdev,1aw7jc,PRAW get_info(): not working?,5,2,3,http://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/,"I'm trying to use PRAW's get_info() method (a wrapper for the /api/info API method) to get posts for some URLs. For some reason the behavior seems to be inconsistent. I've been testing some URLs while learning just to see the format of responses and whatnot, but some URLs (that I got from actual posts) give responses indicating that there are no posts for that URL.

For example, the URL: http://i.imgur.com/jOG9a.jpg is used in [this post](http://www.reddit.com/r/gaming/comments/10no3v/the_pokemon_holy_grail/). If I fire up Python / PRAW in my terminal, this is the result:

    &gt;&gt;&gt; import praw
    &gt;&gt;&gt; r = praw.Reddit(user_agent='testing /u/StealsTopComments')
    &gt;&gt;&gt; r.get_info(url='http://i.imgur.com/jOG9a.jpg')
    []

Can anyone shed some light on this? Am I misunderstanding what /info is for? ",,False,,t5_2qizd,False,,,True,t3_1aw7jc,http://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/,
1362193593.0,3,self.redditdev,19i4w2,Applescript to pull random links from specific reddits [xpost from r/reddithelp] ,4,1,1,http://www.reddit.com/r/redditdev/comments/19i4w2/applescript_to_pull_random_links_from_specific/,"Hey I figured it's worth a go asking this randome question here.

I'm working on a script that pulls links from reddit to test a computers ability to browse reddit.

I'm currently using http://www.reddit.com/r/aww+earthporn+historyporn+itookaphoto+quotesporn/random

This is kinda on the right track but won't provide a proper random link at any given call, in fact it can take 60 seconds +. And it also goes to the comment instead of the link making each page load less distinctive.

What I'd much rather have happen is the script pull the images/links from this multi subreddit http://www.reddit.com/r/aww+earthporn+historyporn+itookaphoto+quotesporn and bring that through, but I'm not sure how to pull that out.

Currently it's all done in apple script but could easily be worked with some other code.
Code for any ones curiosity is this

    set URLHistory to {}
    set screenshotCounter to 0
    
    tell application ""Safari""
    	close every window
    end tell
    tell application ""Finder""
    	set p to path to desktop -- Or whatever path you want
    	try
    		make new folder at p with properties {name:""Browse_the_Web""}
    	end try
    end tell
    repeat
    	do shell script ""open -a \""Safari\"" \"""" &amp; ""http://reddit.com/r/aww+earthporn+historyporn/random"" &amp; ""\""""
    	delay 60
    	tell application ""Safari""
    		tell front tab of front window
    			set thisURL to URL
    		end tell
    	end tell
    	if (URLHistory does not contain thisURL) then
    		set URLHistory to URLHistory &amp; thisURL as item
    		set screenshotCounter to screenshotCounter + 1
    		do shell script ""screencapture  ~/Desktop/Browse_the_web/browsetheweb"" &amp; screenshotCounter &amp; "".png""
    		delay 1
    	end if
    	
    	tell application ""Safari""
    		close every window
    	end tell
    end repeat
    ",,False,,t5_2qizd,False,,,True,t3_19i4w2,http://www.reddit.com/r/redditdev/comments/19i4w2/applescript_to_pull_random_links_from_specific/,
1361943598.0,3,self.redditdev,19b8mp,"PRAW get_inbox(), printing whole message from inbox",4,1,2,http://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/,"Hi! I'm new at using PRAW and When using the below code, I can print my messages, but if a message is long enough, some of the message text is cut off. Am I using the generator wrong or is there a way to read the whole message? 
Thanks!  

    import praw
 
    r = praw.Reddit(""new bot by /u/johnflim"")
    r.login()

    inbox = r.get_inbox(11)
    for message in inbox:
        print message 

",,False,,t5_2qizd,False,,,True,t3_19b8mp,http://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/,
1361390148.0,3,self.redditdev,18wkqv,Two-factor authentication password incorrect,3,0,8,http://www.reddit.com/r/redditdev/comments/18wkqv/twofactor_authentication_password_incorrect/,"When setting up for two factor auth, I used Google Authenticator on my Droid, took a snapshot of the QR code, and wrote the code given by the app to the one-time password below the QR, but im getting incorrect password. How do i fix this?",,False,,t5_2qizd,False,,,True,t3_18wkqv,http://www.reddit.com/r/redditdev/comments/18wkqv/twofactor_authentication_password_incorrect/,
1361321800.0,3,self.redditdev,18us63,Default subreddits,3,0,4,http://www.reddit.com/r/redditdev/comments/18us63/default_subreddits/,"Hey, (I apologize in advance for a lot of my recent submissions)

This is, again, regarding the instance of this clone of mine. I have covered most of the things, and now there one more thing: setting of the default subreddits. As I see it, there are two things there:
* subreddits on the front page for users that are not logged in
* subreddits that a new user is automatically subscribed to when registering

I found the setting in the ini, ""automatic_reddits"", https://github.com/reddit/reddit/blob/master/r2/example.ini#L454 but I can't really seem to figure the effect of this thing. I have tried entering 10 subreddits there, and now tried 15, but the new users get 20 of them when they sign up, and I don't know where did my clone got that number. 

num_default_reddits = 10 
This line was also interesting, as I though it was to decide how many of those will be showed to not logged in users, and how many of them will the new user subscribe too, and I tried setting this number to 15, but that didn't really work as expected.

I spent some time at https://github.com/reddit/reddit/blob/master/r2/r2/models/subreddit.py, examinig default_subreddits(), top_lang_srs() and subscribe_defaults(), and I see that there's some sort of relation between the setting of number of reddits, language of the site and popularity, as well as that list of reddits in the automatic_reddits line, but couldn't find exactly how is reddit doing these stuff.

So, I am wondering, could someone explain a bit what is affecting the showing of reddits to anonymous users, which ones will be auto-subiscribed-to for newly registered users, what role does the language setting of the site (in ini, site_lang, or lang_overide??)  has (and how that relates to the languages of those specific subreddits) and, at the end, how could I make this to work as intended, provide a list of reddits for the users to be autosubsribed to, which would ideally be the same thing which anonymous users see?

Thanks!",,False,,t5_2qizd,False,,,True,t3_18us63,http://www.reddit.com/r/redditdev/comments/18us63/default_subreddits/,
1361199337.0,3,self.redditdev,18r34w,Broken links in top bar,3,0,5,http://www.reddit.com/r/redditdev/comments/18r34w/broken_links_in_top_bar/,"Hey, I asked on chat this but noone responded, and since I gotta run, I wanted to ask this here so you can see it :)

""Its me with my issue again :( The top bar is adding port, :8080 to the links, and that makes most of the links unusable. It's interesting that when logged in, there's no port added. Does anyone know what might be causing this?

The weird thing is that in iOS, Safari, links are OK. In Chrome and IE they're not. In Firefox, links are OK. I tried starting incognito sessions, clearing cache and so on, but without luck :(""

Thanks in advance for your time!",,False,,t5_2qizd,False,,,True,t3_18r34w,http://www.reddit.com/r/redditdev/comments/18r34w/broken_links_in_top_bar/,
1360387247.0,1,self.redditdev,186ggu,Is it possible to run Reddit's code on Google App Engine? ,7,6,4,http://www.reddit.com/r/redditdev/comments/186ggu/is_it_possible_to_run_reddits_code_on_google_app/,"Can Reddit's code run on Google App Engine. I know most people use Amazon AWS, but what about app engine? ",,False,,t5_2qizd,False,,,True,t3_186ggu,http://www.reddit.com/r/redditdev/comments/186ggu/is_it_possible_to_run_reddits_code_on_google_app/,
1359380807.0,3,self.redditdev,17feq6,Can I make a request? Anyone keen on making a bookmarklet to show a Reddit score for any link on the current page? (crosspost /r/bookmarklets),7,4,2,http://www.reddit.com/r/redditdev/comments/17feq6/can_i_make_a_request_anyone_keen_on_making_a/,"A humble suggestion, in case someone just happens to be interested: Sometimes I see a list of links on a site and I wonder which, if any of them, have been posted already to Reddit. I might want to submit some, or I might wonder what redditors had to say about those links, and bulk checking if links have been submitted to Reddit would be very nice. 

I imagined the most simple way to do this would be very similar to site owners adding reddit rank or submission widgets to their pages (similar to Facebook and Twitter button/widgets), but done through a bookmarklet instead. Ideally, to limit the links checked, the bookmarklet would prompt me for text to use in finding matching links. So, for example, the bookmarklet might only show scores for links on the page which contain the text ""vimeo"" if that is what I typed. 

Not sure if this is even possible but thought that if it is, then it may be quite simple for someone here. ",,False,,t5_2qizd,False,,,True,t3_17feq6,http://www.reddit.com/r/redditdev/comments/17feq6/can_i_make_a_request_anyone_keen_on_making_a/,
1359355216.0,3,self.redditdev,17f0c6,A question about URLs,5,2,4,http://www.reddit.com/r/redditdev/comments/17f0c6/a_question_about_urls/,"Using [this post](http://www.reddit.com/r/technology/comments/17d832/the_pc_isnt_ready_to_die_its_ready_for_a_rebirth/) for example I know that the reddit URL is 
    www.reddit.com/r/technology/comments/*thingID*/*????* 

I know that the ??? is constructed by the title escaped for link, but when the title is too long, how does reddit decide what to call the link",,False,,t5_2qizd,1359355542.0,,,True,t3_17f0c6,http://www.reddit.com/r/redditdev/comments/17f0c6/a_question_about_urls/,
1359241501.0,4,self.redditdev,17c7kt,503 on read_message,6,2,4,http://www.reddit.com/r/redditdev/comments/17c7kt/503_on_read_message/,"Hello! I'm POSTing as follows:

&gt;POST /api/read_message
&gt;
&gt;id=t4_...&amp;uh=...

And I'm getting this back:

    &lt;HTML&gt;&lt;HEAD&gt;
    &lt;TITLE&gt;Service Unavailable - Fail to connect&lt;/TITLE&gt;
    &lt;/HEAD&gt;&lt;BODY&gt;
    &lt;H1&gt;Service Unavailable&lt;/H1&gt;
    The server is temporarily unable to service your request.  Please try again
    later.&lt;P&gt;
    Reference&amp;#32;&amp;#35;6&amp;#46;1f9102cc&amp;#46;1359241360&amp;#46;559cc9c
    &lt;/BODY&gt;&lt;/HTML&gt;

With a 503 error to boot. It only occurs on this one API request, everything else works fine.

EDIT: This has been ""temporarily unavailable"" for more than 14 hours.

**EDIT**: The 503 went away and became a 400 with a 404 page delivered as HTML. I eventually tracked down the reason why and the problem is no more.",,False,,t5_2qizd,1359267658.0,,,True,t3_17c7kt,http://www.reddit.com/r/redditdev/comments/17c7kt/503_on_read_message/,
1356571488.0,3,self.redditdev,15hu08,Working on a reddit api program...issues with request limit.,4,1,9,http://www.reddit.com/r/redditdev/comments/15hu08/working_on_a_reddit_api_programissues_with/,"I'm working on a program to make my life easier (I'm writing a program which displays reddit posts in a sub I moderate on my tablet).

The program I'm writing properly throttles requests to Reddit under normal circumstances, but it doesn't throttle correctly when I am debugging the program (closing the application, making a change, starting again under the debugger).  So I end up hitting the request limit and my requests start getting throttled.

How do you guys work around this in development?  A local copy of reddit to develop against?",,False,,t5_2qizd,False,,,True,t3_15hu08,http://www.reddit.com/r/redditdev/comments/15hu08/working_on_a_reddit_api_programissues_with/,
1356103901.0,3,self.redditdev,158ai9,newbie questions about admin accounts and deleting subreddits.,7,4,4,http://www.reddit.com/r/redditdev/comments/158ai9/newbie_questions_about_admin_accounts_and/,"I just set up a reddit and it's working smoothly except for the fact that I can't figure out how to 1)delete subreddits entirely and 2)use the ""one-time password"" when turning admin on. The first is rather self explanatory but when I attempt to turn on admin (in this case with the reddit account created by the build-demo-data module), it asks for ""my papers"" and prompts for a password and a one-time password and I can't figure out where that password comes from, when it fails it prompts of me to ""enable two-factor authentication"".

Any help would be greatly appreciated.",,False,,t5_2qizd,False,,,True,t3_158ai9,http://www.reddit.com/r/redditdev/comments/158ai9/newbie_questions_about_admin_accounts_and/,
1355858772.0,3,self.redditdev,152d9c,Assistance with /moderationlog API call,3,0,3,http://www.reddit.com/r/redditdev/comments/152d9c/assistance_with_moderationlog_api_call/,"Hey all. I've been having some issues getting the /moderationlog api call to work. I've authenticated successfully and cookies are working correctly (as well as other api calls). However, I can't seem to get the modlog to form correctly.

I know the API docs are a bit incomplete, and I think that's what my issue is with [the call](http://www.reddit.com/dev/api#GET_moderationlog).

I am trying to get the modlog for /r/hate with thing ID **t5_2qmez**.

After login, I am submitting a GET request (not POST) to: *http://www.reddit.com/api/moderationlog*

My cookie is sent, as well as the following post fields: *api_type=json&amp;target=t5_2qmez* - but I end up with a 404.

Any assistance to point me in the right direction with what I'm missing is appreciated.",,False,,t5_2qizd,False,,,True,t3_152d9c,http://www.reddit.com/r/redditdev/comments/152d9c/assistance_with_moderationlog_api_call/,
1355471499.0,3,self.redditdev,14u1cg,Having trouble with parameters of /about/log.json,3,0,8,http://www.reddit.com/r/redditdev/comments/14u1cg/having_trouble_with_parameters_of_aboutlogjson/,"Hey guys, First of all, I love working with the reddit API, it's been lots of fun!  
However, I'm kinda stuck.  
/r/subreddit/about/log.json returns mod\_id36 and sr\_id36, Both are undocumented (or I haven't googled hard enough) and I'm not sure how to get these (I think mod\_id36 is the user id).  
Yes, I've tried /by\_id/t2_&lt;mod_id36&gt; but that doesn't seem to work.",,False,,t5_2qizd,False,,,True,t3_14u1cg,http://www.reddit.com/r/redditdev/comments/14u1cg/having_trouble_with_parameters_of_aboutlogjson/,
1353472804.0,3,self.redditdev,13jtqq,batch hide command,3,0,4,http://www.reddit.com/r/redditdev/comments/13jtqq/batch_hide_command/,Is there a way to hide multiple links in one hide request? I'm trying to implement a hide read feature that also hides stories on Reddit so you don't see them again.,,False,,t5_2qizd,False,,,True,t3_13jtqq,http://www.reddit.com/r/redditdev/comments/13jtqq/batch_hide_command/,
1350556696.0,3,self.redditdev,11olv5,Getting an error when trying to access /r/random on my clone.,3,0,3,http://www.reddit.com/r/redditdev/comments/11olv5/getting_an_error_when_trying_to_access_rrandom_on/,[PasteBin](http://pastebin.com/kaUZME16),,False,,t5_2qizd,False,,,True,t3_11olv5,http://www.reddit.com/r/redditdev/comments/11olv5/getting_an_error_when_trying_to_access_rrandom_on/,
1350000456.0,3,self.redditdev,11c7gu,Reddit Data Dump for Research,3,0,6,http://www.reddit.com/r/redditdev/comments/11c7gu/reddit_data_dump_for_research/,"I am trying to collect a huge dump of reddit data from certain subreddits like politics, USPolitics etc for my research (opinion mining). I am particularly interested in the posts and its comments and votes in the past 1 year. What is the best way to get such data? I am writing a small script with the Reddit API. But if there is an easier way to obtain this list, it will be great!",,False,,t5_2qizd,False,,,True,t3_11c7gu,http://www.reddit.com/r/redditdev/comments/11c7gu/reddit_data_dump_for_research/,
1349684003.0,3,self.redditdev,114o4x,"Is there any way to use ID, subreddit name in utf8?",4,1,1,http://www.reddit.com/r/redditdev/comments/114o4x/is_there_any_way_to_use_id_subreddit_name_in_utf8/,"As title says, can I make Id and subreddit name in utf8 by just detouring  regenx checking? Or is there any reason not to make id and subreddit name in utf8?",,False,,t5_2qizd,False,,,True,t3_114o4x,http://www.reddit.com/r/redditdev/comments/114o4x/is_there_any_way_to_use_id_subreddit_name_in_utf8/,
1348457271.0,3,self.redditdev,10dizi,banned_by and num_reports fields,3,0,4,http://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/,"Just looking over some data for a project. I've processed over 2.5 million comments and banned_by and num_reports is always blank. Although the project isn't using these fields I could see how they could be of use. 

Surely, of all those comments one of these would have something (maybe). Is this mod only territory? Anything I'm missing or think my code is flawed? Does someone have a comment/submission handy for me to verify?

http://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/
https://github.com/reddit/reddit/issues/429

PS I'm using praw.

Thanks",,False,,t5_2qizd,1348457600.0,,,True,t3_10dizi,http://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/,
1347882414.0,3,self.redditdev,100o1k,Possible to have API for 'read' links,3,0,1,http://www.reddit.com/r/redditdev/comments/100o1k/possible_to_have_api_for_read_links/,"One of the downsides of using an iphone/android app to browse reddit is that when you quit your app and load up reddit.com on a browser all the links are blue. Is it possible to prevent this?

One way i can think of is to save (https://github.com/reddit/reddit/wiki/API%3A-save) every post that was read, and then have a chrome or firefox plugin to sync the saved posts as visited links in your browser history. Does redditdev have any plans to implement something like this?

This is a bit hacky, but I'm open to suggestions on how this could be implemented. Thanks.

",,False,,t5_2qizd,1374816091.0,,,True,t3_100o1k,http://www.reddit.com/r/redditdev/comments/100o1k/possible_to_have_api_for_read_links/,
1346605476.0,3,self.redditdev,z8i84,Best way to filter a user's submissions for a list of subreddits?,5,2,4,http://www.reddit.com/r/redditdev/comments/z8i84/best_way_to_filter_a_users_submissions_for_a_list/,"I'm working on a Chrome extension that will identify if users have posted to various subreddits when you see their name. Currently, I hit http://www.reddit.com/user/{username}/submitted/.json, but this returns a paginated list of all posts. Is there some way to filter against a list of subreddits so that I don't have to make dozens of hits for each user I want to check? ",,False,,t5_2qizd,False,,,True,t3_z8i84,http://www.reddit.com/r/redditdev/comments/z8i84/best_way_to_filter_a_users_submissions_for_a_list/,
1346159540.0,2,self.redditdev,yyjqa,"What is the best practice to utilize the API from the same server as a reddit source? (Python, or PHP)",4,2,5,http://www.reddit.com/r/redditdev/comments/yyjqa/what_is_the_best_practice_to_utilize_the_api_from/,"The API appears to be very useful for my system's requirements, but I feel I am probably not using it in the best possible way. Currently I use a PHP script (I am a PHP native, only just learning Python now) on a separate server to make API calls with cURL/file requests, essentially calling my reddit source server from my secondary server.

1.  It seems much more suitable to somehow poll the API directly, rather than through a URL request. Is this possible?

2. Also, the reason I use my script on a separate server is because I cannot figure out a publicly accessible path that I could place the script on the reddit source server. Is there such paths, or does the service override non-reddit requests?

Guidance in either of these issues would surely help my network speeds, and would be most appreciated.",,False,,t5_2qizd,False,,,True,t3_yyjqa,http://www.reddit.com/r/redditdev/comments/yyjqa/what_is_the_best_practice_to_utilize_the_api_from/,
1345169573.0,2,self.redditdev,ycsk7,Should I make 'Powered by Reddit' mark be shown on every page in my clone site? ,5,3,3,http://www.reddit.com/r/redditdev/comments/ycsk7/should_i_make_powered_by_reddit_mark_be_shown_on/,"I just want to make some listing site using reddit engine.
However I'm not familiar with CPAL license. Should I make 'Powered by Reddit' mark be shown on every page in my clone site? or just make it shown on like 'About page'?

Thank you for reply in advance.",,False,,t5_2qizd,1345171301.0,,,True,t3_ycsk7,http://www.reddit.com/r/redditdev/comments/ycsk7/should_i_make_powered_by_reddit_mark_be_shown_on/,
1344928570.0,3,self.redditdev,y6xi7,Error converting Pyrex file to C when making python module,3,0,4,http://www.reddit.com/r/redditdev/comments/y6xi7/error_converting_pyrex_file_to_c_when_making/,"I've installed Reddit's dependencies, cloned the Git repository, and I'm trying to install reddit:

	laofmoonster@deb64:~/reddit/reddit/r2$ make pyx
	[+] including definitions from Makefile.py
	cython r2/lib/mr_tools/_mr_tools.pyx

	Error converting Pyrex file to C:
	------------------------------------------------------------
	...
		def __getattr__(self, attr):
			return self[attr]

	def valiter(grouper):
		key, group = grouper
		return key, imap(lambda x: x[1:], group)
						^
	------------------------------------------------------------

	/home/laofmoonster/reddit/reddit/r2/r2/lib/mr_tools/_mr_tools.pyx:71:21: Expected an identifier or literal
	make: *** [r2/lib/mr_tools/_mr_tools.c] Error 1

running Debian 6 (""Squeeze"") 64 bit",,False,,t5_2qizd,False,,,True,t3_y6xi7,http://www.reddit.com/r/redditdev/comments/y6xi7/error_converting_pyrex_file_to_c_when_making/,
1344878236.0,3,self.redditdev,y5ilz,Is there a way to get the points of a comment with the api?,3,0,2,http://www.reddit.com/r/redditdev/comments/y5ilz/is_there_a_way_to_get_the_points_of_a_comment/,"Looking through the json data for a comment (through a submission), I don't see a way to get the score of a comment, only ups and downs. Ups-downs is not always accurate compared to the number of points displayed on the site. Is there a way to get this?",,False,,t5_2qizd,False,,,True,t3_y5ilz,http://www.reddit.com/r/redditdev/comments/y5ilz/is_there_a_way_to_get_the_points_of_a_comment/,
1342828276.0,4,self.redditdev,wwfaj,How to not receive selftext_html?,6,2,0,http://www.reddit.com/r/redditdev/comments/wwfaj/how_to_not_receive_selftext_html/,"I'm making a wrapper for the Reddit API with libcurl, and currently on the phase of fetching subreddit posts by getting their main json file 

ex: http://reddit.com/r/rosehulman.json

It gets the requests, but the character buffer is huge because it also includes the self post text. Is there any variable I can pass to the json file so it doesn't include the self text and decreases the overall length of characters?",,False,,t5_2qizd,False,,,True,t3_wwfaj,http://www.reddit.com/r/redditdev/comments/wwfaj/how_to_not_receive_selftext_html/,
1341637471.0,3,self.redditdev,w60cs,Using morechildren without PRAW?,5,2,11,http://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/,"There have been several answers over the last two years, (all by /u/bboe) in regards to this question, but I'm just not able to figure it out for myself.

http://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/

http://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/

http://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/

Basically I should know what I'm doing but the official docs are a bit iffy, and looking at the PRAW code isn't helping.

Apparently I need to send the following parameters:

        params = {""link_id"": 't3_w5iwp',
              ""children"": 'c5afy83',
              ""depth"": '',
              ""id"": 'c5ag775',
              ""pv_hex"":"""",
              ""r"":'gaming',
              ""renderstyle"":"""",
              ""api_type"":'json'}

Then I send it like this:

    url = r'http://reddit.com/api/morechildren'
    requests.get(url,params=params) #it's actually a requests.session object, logged in.

But I'm getting a `not found` error page. Even copying the url from the Apigee page doesn't seem to be working",,False,,t5_2qizd,1341637742.0,,,True,t3_w60cs,http://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/,
1338859620.0,3,self.redditdev,ula8k,"API call to return ordered list of ""things"" from all of my subscribed subreddits?",3,0,3,http://www.reddit.com/r/redditdev/comments/ula8k/api_call_to_return_ordered_list_of_things_from/,"I've already discovered http://www.reddit.com/reddits/mine/subscriber.json, which returns the list of subreddits to which I'm subscribed.  Is there any API call that will return the actual items from these subreddits, like the list of items returned when you view the reddit homepage while signed in?  Or would we have to fetch the items from each subreddit returned by mine/subscriber.json and order them by karma?

Thanks in advance for any help!",,False,,t5_2qizd,False,,,True,t3_ula8k,http://www.reddit.com/r/redditdev/comments/ula8k/api_call_to_return_ordered_list_of_things_from/,
1338623245.0,3,self.redditdev,ugy05,Thumbnails not showing up,3,0,4,http://www.reddit.com/r/redditdev/comments/ugy05/thumbnails_not_showing_up/,"My (incomplete) reddit site is installed  [here](http://fiamthu.com) . I'm using Amazon AWS for the thumbnails but the thumbnail images are not uploaded to the S3 repository. The community images are [uploaded correctly](http://fiamthu.com/r/Funny/) but the thumbnails just won't show up for the stories/posts (I've enabled thumbnails in the community settings too). 

What am I doing wrong here?
",,False,,t5_2qizd,False,,,True,t3_ugy05,http://www.reddit.com/r/redditdev/comments/ugy05/thumbnails_not_showing_up/,
1338391903.0,3,self.redditdev,uc7kt,Weird Caching Issues?,3,0,4,http://www.reddit.com/r/redditdev/comments/uc7kt/weird_caching_issues/,"I noticed when I first installed reddit on a VM there were some weird caching issues, I'm pretty sure with even built in fields.  For example, when a user would post a comment it would appear (through a JS update), but when they hit reload it would be gone.  However, if another logged in user visited the page then it would show up.  It seems that this problem basically fixed itself over time, as if the caching engine ""learned"" the fields and updated them appropriately.

Anyway this issue is now occurring with more than just comments; in particular, [I added more fields to text submissions](/r/redditdev/comments/sxcig/).  I made a new design flow where an author would submit a text post, then a new user type ""reviewer"" would review it, then the author would be able to add a rebuttal.  Each of these are separate usertext objects stored in the link.  I am not sure if this is pertinent, but we just got to the rebuttal stage and are having the same strange caching issues.  The submitted rebuttal will show up immediately (through a JS update), but then will not reappear for the author, or even for non-logged in users.  Other logged in users will be able to see the rebuttal though, so I know it is being saved.

I mention this since the rebuttal field has been mostly unused up until this point, which is why I think the cache isn't recognizing it.  I should also mentioned I just rebooted the server (in case that makes a difference).  Notably, I am not noticing this issue with the other fields I added (such as the ""review"").

Is this a caching issue as I suspect?  If so, is there a way to force the cache to update?  Otherwise is there a way to just disable it? (I don't have enough traffic to merit the caching system anyway...)

",,False,,t5_2qizd,False,,,True,t3_uc7kt,http://www.reddit.com/r/redditdev/comments/uc7kt/weird_caching_issues/,
1336951305.0,3,self.redditdev,tllvv,would it be possible to use the reddit source code to implement live streaming ama?,5,2,5,http://www.reddit.com/r/redditdev/comments/tllvv/would_it_be_possible_to_use_the_reddit_source/,"I envision taking AMA to a new level.  I would like to construct a site similar to AMA but allow for live streaming within the submitted AMA post.

[EXAMPLE](http://i.imgur.com/AxLFG.jpg)

If anyone knows is this is possible, please let me know!",,False,,t5_2qizd,False,,,True,t3_tllvv,http://www.reddit.com/r/redditdev/comments/tllvv/would_it_be_possible_to_use_the_reddit_source/,
1335903833.0,5,self.redditdev,t21bh,Moderate all comments?,6,1,12,http://www.reddit.com/r/redditdev/comments/t21bh/moderate_all_comments/,"Is there a way to force moderation of all comments?  That is, I want a new comment to be hidden until a moderator has approved it.

How would I go about making this feature?  Ideally I would like it to be an option in the community settings.",,False,,t5_2qizd,False,,,True,t3_t21bh,http://www.reddit.com/r/redditdev/comments/t21bh/moderate_all_comments/,
1335469655.0,4,self.redditdev,su1rm,as.reddit.com? Alternate stylesheet?,5,1,9,http://www.reddit.com/r/redditdev/comments/su1rm/asredditcom_alternate_stylesheet/,What is this and how does it work? I noticed [/r/texasrangers](/r/texasrangers) seems to be able to select from an alternate stylesheet.,,False,,t5_2qizd,False,,,True,t3_su1rm,http://www.reddit.com/r/redditdev/comments/su1rm/asredditcom_alternate_stylesheet/,
1335412814.0,4,self.redditdev,st18j,Reddit python api wrapper example required. ,6,2,1,http://www.reddit.com/r/redditdev/comments/st18j/reddit_python_api_wrapper_example_required/,"Can someone please provide an example program, using the reddit api wrapper for python, that will log into an account define submission as this post and upvote it?

I cannot figure out how to define submissions for voting. ",,False,,t5_2qizd,False,,,True,t3_st18j,http://www.reddit.com/r/redditdev/comments/st18j/reddit_python_api_wrapper_example_required/,
1335400878.0,3,self.redditdev,ssq9t,Using the mellort/reddit api wrapper. How do you find the item for the voting function ?,4,1,3,http://www.reddit.com/r/redditdev/comments/ssq9t/using_the_mellortreddit_api_wrapper_how_do_you/,"item.upvote()
How do I find the item for a post? Do I declare this an item variable?
A brief example will make me very happy. 

Sincerely,

           Rocetsauce9
",,False,,t5_2qizd,False,,,True,t3_ssq9t,http://www.reddit.com/r/redditdev/comments/ssq9t/using_the_mellortreddit_api_wrapper_how_do_you/,
1334257557.0,5,self.redditdev,s6m12,Get a single comment by ID,7,2,4,http://www.reddit.com/r/redditdev/comments/s6m12/get_a_single_comment_by_id/,using mellort python reddit API how can I get(so i can reply) a comment using the url to the comment or the comment ID?,,False,,t5_2qizd,False,,,True,t3_s6m12,http://www.reddit.com/r/redditdev/comments/s6m12/get_a_single_comment_by_id/,
1334136253.0,3,self.redditdev,s43mm,"Not sure where else to put this, but are mod tools part of the API?",4,1,3,http://www.reddit.com/r/redditdev/comments/s43mm/not_sure_where_else_to_put_this_but_are_mod_tools/,"I have several reddit apps on my android phone. Seriously, I have seven reddit apps. But none of them have support for delete/remove/spam/ham/markNSFW etc. Is this just not part of the API or am I expecting too much out of developers?",,False,,t5_2qizd,False,,,True,t3_s43mm,http://www.reddit.com/r/redditdev/comments/s43mm/not_sure_where_else_to_put_this_but_are_mod_tools/,
1332223991.0,3,self.redditdev,r4qz4,I'm new to this. Where do I start?,3,0,4,http://www.reddit.com/r/redditdev/comments/r4qz4/im_new_to_this_where_do_i_start/,"Hey there!

So I'm interested in getting started using the Reddit API. The trouble is, I have no idea where to start. Does anybody have any tips on how to get started accessing Reddit's data? I'm not even sure if I have to use a certain language to access it or not; this is very new territory for me but I'm excited to get started. I realize this is a very open-ended question, but I'd appreciate any tips on where to start reading! I'll be going over the Github repository to learn as much as I can, but sometimes it's nice to have some guidance. Thanks!",,False,,t5_2qizd,False,,,True,t3_r4qz4,http://www.reddit.com/r/redditdev/comments/r4qz4/im_new_to_this_where_do_i_start/,
1331335279.0,3,self.redditdev,qpksb,Question about creating a reddit clone...,3,0,0,http://www.reddit.com/r/redditdev/comments/qpksb/question_about_creating_a_reddit_clone/,"1.) How much is thee source code ""plug n' play? By that I mean, if I just want to change the name, the subreddits, the font and the colors, how much programming needs to be done?

2.) Do the basic code admin accounts include the ability to ban users, delete comments, delete submissions and ""ghost ban?""

3.) Can I insert Google ads to the home page and each subreddit's main page? I will need to offset hosting fees some how!

4.) With the source, is it easy to limit the ""create a new subreddit"" button to admin use only?

5.) I plan on hiring somebody to set this up for me. How much should it cost?",,False,,t5_2qizd,False,,,True,t3_qpksb,http://www.reddit.com/r/redditdev/comments/qpksb/question_about_creating_a_reddit_clone/,
1331254655.0,3,self.redditdev,qo5xd,How should I access the list of links in the sidebar of a given subreddit?,5,2,2,http://www.reddit.com/r/redditdev/comments/qo5xd/how_should_i_access_the_list_of_links_in_the/,"I once tried to use GraphViz to make a network graph of which subreddits link to which other subreddits in their sidebars. Unfortunately, I collected the data manually, which took me a long time and left it inevitably incomplete and sprinkled with a few errors.

I'm interested in automating this, so that I could just get a list of which subs link to which other subs in a file format that GraphViz would understand. This would be easy except for the fact that I don't know how to obtain the list of links contained on a subreddit's sidebar.

I assume this is the best place to ask that question, but I have one more: I have never used an API before, and I don't know how. What is the basic way I get a program I wrote on my computer (probably in Python or Java) to communicate with Reddit's API? Is there a guide I should read somewhere?

TL;DR: I would like to generate a list of links from the sidebar of a given subreddit.

Thank you for your time!",,False,,t5_2qizd,False,,,True,t3_qo5xd,http://www.reddit.com/r/redditdev/comments/qo5xd/how_should_i_access_the_list_of_links_in_the/,
1330629555.0,2,self.redditdev,qdahl,Why can't I reply to a comment using the python reddit api wrapper? (code inside),4,2,3,http://www.reddit.com/r/redditdev/comments/qdahl/why_cant_i_reply_to_a_comment_using_the_python/,"This is my code, and imho everything checks out (I looked at examples, used Google for answers, etc). For some reason I can't post a reply. 

    import reddit
    import urllib

    r = reddit.Reddit(user_agent=""imauser"")
    r.login(user=""imauser"",password=""hunter2"")

    stories=r.get_subreddit('fitness').get_hot(limit=1)

    for x in stories:
        x.comments[5].reply(""This is a test."")
",,False,,t5_2qizd,False,,,True,t3_qdahl,http://www.reddit.com/r/redditdev/comments/qdahl/why_cant_i_reply_to_a_comment_using_the_python/,
1329309650.0,4,self.redditdev,pqmi3,Few questions on installation and development,6,2,9,http://www.reddit.com/r/redditdev/comments/pqmi3/few_questions_on_installation_and_development/,"I'm trying to install a reddit fork  lesswrong code (https://github.com/tricycle/lesswrong/) for lesswrong.ru and got few questions regarding that.

1. First of all, how can I properly update translation (.po) files (for actually translating them)?  There seem to be some additions in the code that are not in the translation files.

2. When using `uncompressedJS = false`, javascript mostly stops working.  Where should I look to find out why?

3. Is it *plausibly* possible to determine and limit all URLs reddit uses and run it directly alongside MediaWiki (behind nginx)?

4. What is the preferable way to run reddit behind nginx at all (on a relatively low-load server)? FastCGI?

I'm pretty sure there will be more questions, and what will remain important and unanswered I will go ask the fork's developers, but for now the most important question is #1.",,False,,t5_2qizd,False,,,True,t3_pqmi3,http://www.reddit.com/r/redditdev/comments/pqmi3/few_questions_on_installation_and_development/,
1328465951.0,2,self.redditdev,pc1hu,Is there a way to limit the number of stories returned by a subreddit?,5,3,3,http://www.reddit.com/r/redditdev/comments/pc1hu/is_there_a_way_to_limit_the_number_of_stories/,"I saw that the next page does this, 

reddit.com/r/test/?count=25

but that doesn't work if you don't have the after parameter also. Is there any ways to get only the top one or two stories from the subreddit? Thanks :)",,False,,t5_2qizd,False,,,True,t3_pc1hu,http://www.reddit.com/r/redditdev/comments/pc1hu/is_there_a_way_to_limit_the_number_of_stories/,
1321890488.0,4,self.redditdev,mk88o,Images not updating,7,3,5,http://www.reddit.com/r/redditdev/comments/mk88o/images_not_updating/,"Hello, I've installed reddit and updated some image files in the /r2/r2/public/static folder.

I restart my service with 

svc -d reddit-app01

svc -u reddit-app01

and ""make"" in my r2 folder

My images still stay the same even though I see the updated ones on the server. I'm sure it's a simple command that I'm forgetting here...

Any help is appreciated! thanks!
",,False,,t5_2qizd,False,,,True,t3_mk88o,http://www.reddit.com/r/redditdev/comments/mk88o/images_not_updating/,
1321022681.0,3,self.redditdev,m8pfg,[API] Can't submit a self post without captcha,7,4,4,http://www.reddit.com/r/redditdev/comments/m8pfg/api_cant_submit_a_self_post_without_captcha/,"(Crossposted from the mailing list, as it seems to get very little traffic)

 I'm currently writing a bot (the account that's currently posting this) to try to help me keep a personal
subreddit in order.  Its job is to post a self post, and then put
'next' links in the previous entries, so they're all linked together.
Like the subject says, I can't post a story via the API.  I can log in
okay, but when I attempt to POST to /api/submit with all the required
information I don't get a reply that looks like any of the replies
documented on the API page.  Instead, I get:

    {""jquery"":[
    [0, 1, ""call"", [""body""]],
    [1, 2, ""attr"", ""find""],
    [2, 3, ""call"", ["".status""]],
    [3, 4, ""attr"", ""hide""],
    [4, 5, ""call"", []],
    [5, 6, ""attr"", ""html""],
    [6, 7, ""call"", [""""]],
    [7, 8, ""attr"", ""end""],
    [8, 9, ""call"", []],
    [1, 10, ""attr"", ""captcha""],
    [10, 11, ""call"", [""IXHVetHj5nUk4JCbPkZOfHZZxvt0hbPJ""]],
    [1, 12, ""attr"", ""find""],
    [12, 13, ""call"", ["".error.BAD_CAPTCHA.field-captcha""]],
    [13, 14, ""attr"", ""show""],
    [14, 15, ""call"", []],
    [15, 16, ""attr"", ""text""],
    [16, 17, ""call"", [""care to try these again?""]],
    [17, 18, ""attr"", ""end""],
    [18, 19, ""call"", []]
    ]}

I'm guessing from this it's expecting a CAPTCHA and, as that's
designed explicitly to catch bots, that's exactly what it did.  I've
done everything I can think of to try to convince reddit that the
bot's allowed to post in the subreddit - I put it on the list of
'approved submitters', I verified its e-mail, I even made it a mod,
but I keep getting this response.  Is there any way to fix this?

Perhaps offtopic:  I recognize the format of the return message as
JSON, but what the heck is it trying to tell me?  Those fields mean
nothing to me, unlike (for example) the kind of JSON response I get
after logging in, which is very straightforward.

Thanks!",,False,,t5_2qizd,False,,,True,t3_m8pfg,http://www.reddit.com/r/redditdev/comments/m8pfg/api_cant_submit_a_self_post_without_captcha/,
1320477627.0,2,self.redditdev,m18cs,/r/reseph will track my reddit code.,6,4,0,http://www.reddit.com/r/redditdev/comments/m18cs/rreseph_will_track_my_reddit_code/,I created http://www.reddit.com/r/reseph/ to organize my patches and the sort. Any upcoming plans or thoughts I have for code I can submit to reddit will be there. Sometimes it takes a bit for me to find a good idea to patch in that fits my views of an acceptable feature and something not too large of a project.,,False,,t5_2qizd,False,,,True,t3_m18cs,http://www.reddit.com/r/redditdev/comments/m18cs/rreseph_will_track_my_reddit_code/,
1320004113.0,2,self.redditdev,luc0v,Problem with Comment API,4,2,6,http://www.reddit.com/r/redditdev/comments/luc0v/problem_with_comment_api/,"Hello All,

I see I'm not the first one to have trouble with the [Comment API](http://www.reddit.com/r/redditdev/search?q=api+comment&amp;restrict_sr=on&amp;sort=relevance).

I too am getting the ""You Broke reddit Error.""  My question is this, when I submit a story, at jquery[12][3][0] I receive a link to the comments page.  i.e.:

    http://www.reddit.com/r/shareCoding/comments/lu91p/another_test_1/

as I understand it, the `lu91p` represents the thing ID.  

So, to submit a comment I would call the comment API with the following attributes:

    http://www.reddit.com/api/comment
    {
        thing_id  =&gt; 't4_lu91p',
        text  =&gt; ""Hey!\n\n this is a comment"",
        uh    =&gt; $user_hash,
    }

This is a POST request.

Am I missing something?  I've already logged in and set the cookie previously in the code.  

For context, my project is [here](http://cpansearch.perl.org/src/JON/Reddit-0.11/lib/Reddit.pm), and here is the [documentation](http://search.cpan.org/~jon/Reddit-0.11/lib/Reddit.pm).  [This](http://pastebin.com/SypUX7Mb) is the subroutine I am trying to debug.

Thanks for the assist.

EDIT:

To translate the pastebin post,

I am calling the Perl Library LWP::UserAgent that handles all of the semantics of POST requests, so I don't have to roll-my-own.  Effectively the call is

    LWP::UserAgent-&gt;post( $post, %attribute_hash)

where  ` { key =&gt; $value } ` represents a hash, and keys are attributes of the POST request.",,False,,t5_2qizd,True,,,True,t3_luc0v,http://www.reddit.com/r/redditdev/comments/luc0v/problem_with_comment_api/,
1315851949.0,4,self.redditdev,kdbb7,How do I get this code to retrieve the full comment?,6,2,9,http://www.reddit.com/r/redditdev/comments/kdbb7/how_do_i_get_this_code_to_retrieve_the_full/,"This code was written in python and uses mellort's reddit api on github. This code (just a sample) retrieves comments, but for some reason, those longer than about 80 characters are truncated [example: And what about all of the very real physical evidence that fits with the evoluti...] How do I get the full text for each comment using this module (which, of the ones I've found, is the simplest)?

    import reddit
    import urllib

    r = reddit.Reddit(user_agent=""ma"")
    r.login(user=""ma"",password=""mia"")

    stories=r.get_all_comments(limit=10);

    for x in stories:
            print x",,False,,t5_2qizd,False,,,True,t3_kdbb7,http://www.reddit.com/r/redditdev/comments/kdbb7/how_do_i_get_this_code_to_retrieve_the_full/,
1315762347.0,3,sourceforge.net,kc6yq,Reddit API library in Python 3 (git repository),3,0,1,http://www.reddit.com/r/redditdev/comments/kc6yq/reddit_api_library_in_python_3_git_repository/,,,False,,t5_2qizd,False,,,False,t3_kc6yq,http://sourceforge.net/p/redditlibpy3/,
1313454333.0,2,self.redditdev,jjxb2,Data dump of public Reddit content (i.e. &gt; 1000 links),6,4,5,http://www.reddit.com/r/redditdev/comments/jjxb2/data_dump_of_public_reddit_content_ie_1000_links/,"I'd like to experiment with some visualizations of Reddit's submissions.  The problem is, I can only crawl the last 1000 links.

Is there any way I can download a snap shot, even if it's a little old?

I'm thinking the same info as http://www.reddit.com/.json, just going back more than 1000 links.

Thanks, 

Josh
",,False,,t5_2qizd,False,,,True,t3_jjxb2,http://www.reddit.com/r/redditdev/comments/jjxb2/data_dump_of_public_reddit_content_ie_1000_links/,
1312695071.0,3,self.redditdev,jbcxv,How do I get a Submission object given a post's title/url/id using the Python wrapper?,4,1,0,http://www.reddit.com/r/redditdev/comments/jbcxv/how_do_i_get_a_submission_object_given_a_posts/,I'm using this https://github.com/mellort/reddit_api but I can't find the method to get a specific post. Any suggestions?,,False,,t5_2qizd,False,,,True,t3_jbcxv,http://www.reddit.com/r/redditdev/comments/jbcxv/how_do_i_get_a_submission_object_given_a_posts/,
1311273693.0,3,self.redditdev,iw2b2,Way to get the default subscribed subreddits in JSON?,4,1,3,http://www.reddit.com/r/redditdev/comments/iw2b2/way_to_get_the_default_subscribed_subreddits_in/,"I'm currently building a reddit app for Windows Phone 7, and I'm trying to get a good experience for lurkers as well as logged in users. I know I can use mine.json to get a list of subreddits that the user is currently logged into, but how can I get a list of the default not-logged-in subreddits on the front page?

EDIT: Answered in IRC. Top 10 subreddits in reddits.json! :D",,False,,t5_2qizd,True,,,True,t3_iw2b2,http://www.reddit.com/r/redditdev/comments/iw2b2/way_to_get_the_default_subscribed_subreddits_in/,
1309447866.0,2,self.redditdev,idblp,How do I know if a user entered a wrong username?,5,3,21,http://www.reddit.com/r/redditdev/comments/idblp/how_do_i_know_if_a_user_entered_a_wrong_username/,"I've successfully logged in to Reddit with my C# app and am now just trying to catch all the exceptions. I know how to get the WRONG_PASSWORD and RATELIMIT errors, but it seems to me that both the Set-Cookie and JSON results are the same when either the password or the username is wrong. Is there a way of differentiating between the two?",,False,,t5_2qizd,False,,,True,t3_idblp,http://www.reddit.com/r/redditdev/comments/idblp/how_do_i_know_if_a_user_entered_a_wrong_username/,
1309135478.0,2,self.redditdev,i9wfr,"Fetching a domain's new submissions as JSON works for me in the browser, but not with cURL",4,2,2,http://www.reddit.com/r/redditdev/comments/i9wfr/fetching_a_domains_new_submissions_as_json_works/,"I'm trying to detect when a new submission is made from a particular domain. The web location 

    http://www.reddit.com/domain/blog.reddit.com/new

works, as does this when viewed in a browser:

    http://www.reddit.com/domain/blog.reddit.com/new.json

However, when I use cURL in PHP to grab the latter URL, it returns *weird* results. For instance, an AskReddit post with blog.reddit.com inserted in the URL:
    
    http://www.reddit.com/domain/blog.reddit.com/comments/i9w6b/westboro_baptist_church_is_going_to_be_at_my_high/

What's strangest is I can cURL this JSON accurately:

    http://www.reddit.com/domain/blog.reddit.com.json

It just seems to be a problem when cURL fetching new domain results. Any ideas what's going on here?",,False,,t5_2qizd,False,,,True,t3_i9wfr,http://www.reddit.com/r/redditdev/comments/i9wfr/fetching_a_domains_new_submissions_as_json_works/,
1305003716.0,2,self.redditdev,h7wr5,"Just setup a reddit on a newly installed machine, can get index and login to load fine but searching results in pylons throwing ""&lt;type 'exceptions.TypeError'&gt;: unsupported operand type(s) for +: 'NoneType' and 'str' """,4,2,6,http://www.reddit.com/r/redditdev/comments/h7wr5/just_setup_a_reddit_on_a_newly_installed_machine/,"The line giving the error is ""netloc = splits.hostname + (':%s' % splits.port if splits.port else '')""  

I'm guessing this has something to do with the domain being just the IP address (what are IP addresses stored as? string? int? ) But any idea on how to fix it? I've little experience with python and none with pylons :(  

Also searching for subreddits results in ""&lt;type 'exceptions.NameError'&gt;: global name 'e' is not defined "" I'm not sure if it's the same problem causing this",,False,,t5_2qizd,False,,,True,t3_h7wr5,http://www.reddit.com/r/redditdev/comments/h7wr5/just_setup_a_reddit_on_a_newly_installed_machine/,
1296027613.0,3,self.redditdev,f98sf,"Mods, may (sub)reddit subscription requests be requested more frequently than 2 seconds by a userscript?",4,1,1,http://www.reddit.com/r/redditdev/comments/f98sf/mods_may_subreddit_subscription_requests_be/,"I have created a userscript, [manage reddit subscriptions](http://userscripts.org/scripts/show/94558), which will process multiple sub/unsubscription requests. It currently requests one/2 seconds. As the normal subscribe/unsubscribe buttons would allow multiple rapid requests, is it permissible that I decrease the 2 second delay between requests sub/unsubscribe requests?

The script also requests pages of subscribed reddits (2 seconds apart). Is there are single request that I can make to receive all subscribed reddits?",,False,,t5_2qizd,False,,,True,t3_f98sf,http://www.reddit.com/r/redditdev/comments/f98sf/mods_may_subreddit_subscription_requests_be/,
1292822953.0,2,self.redditdev,eom44,Getting the user hash for voting calls?,4,2,1,http://www.reddit.com/r/redditdev/comments/eom44/getting_the_user_hash_for_voting_calls/,"It appears that when submitting a vote, both a vote hash (vh) and user hash (uh) is sent in the request body. I understand the vh can be obtained in the body of any reddit page, but how do I obtain the user hash?",,False,,t5_2qizd,False,,,True,t3_eom44,http://www.reddit.com/r/redditdev/comments/eom44/getting_the_user_hash_for_voting_calls/,
1287522710.0,3,self.redditdev,dtiij,Token-based auth for the API,4,1,5,http://www.reddit.com/r/redditdev/comments/dtiij/tokenbased_auth_for_the_api/,"I was playing with the idea of doing some push notification stuff, when I realised that the user credentials would have to be stored on the server  not a good thing.

Maybe we could think about ways to implement a token-based system like Twitters xAuth, where a user authenticates once, locally, after which the application is issued an application-specific token that it can store and use for further requests. Optionally the user could de-authorise apps again.",,False,,t5_2qizd,False,,,True,t3_dtiij,http://www.reddit.com/r/redditdev/comments/dtiij/tokenbased_auth_for_the_api/,
1284751494.0,1,self.redditdev,dfce5,How to get all the comments with the api,7,6,9,http://www.reddit.com/r/redditdev/comments/dfce5/how_to_get_all_the_comments_with_the_api/,"Hi,

I am trying to get all the comments for a given story through the api. I do an initial:

    wget www.reddit.com/comments/df3gp.json

And get 1 x t3 and 200 x t1 (I know about ?limit=, but it just move from 200 to 500, so I run into the same problem with a story with &gt; 500 comments).

So now I want to get the rest of the comments, reading the [API guide](http://code.reddit.com/wiki/API), I go:

    wget www.reddit.com/by_id/c0zqmr4.json

But get a 404. Same with:
    wget www.reddit.com/comments/t1_c0zqmr4.json
    wget www.reddit.com/comments/c0zqmr4.json
    wget www.reddit.com/api/comment/t1_c0zqmr4.json

I am not able to get any of  the ""kind: more"" comments through the api.


Is there a way to get those through the API?

Or even better, is there a way to get all the comments to a story in one call?


Thanks.

PS: and yes, I understand that I need to wait 2 seconds between calls.",,False,,t5_2qizd,False,,,True,t3_dfce5,http://www.reddit.com/r/redditdev/comments/dfce5/how_to_get_all_the_comments_with_the_api/,
1284579642.0,3,self.redditdev,dec11,I think /r/random can work better. Can someone point me at the code?,4,1,8,http://www.reddit.com/r/redditdev/comments/dec11/i_think_rrandom_can_work_better_can_someone_point/,"Right now /r/random is errr... well completely random. Sometimes it produces interesting results but more often than not, it drops you on a subreddit where no one has posted in months or even years.

Think there's a simple solution: ignore subreddits where the last post was older than X where X might be one month. That should help promote young and active subreddits while making /r/random an interesting feature.

Can someone point me at the implementation and I'll give it a try.

Can haz Python, lol skaling.",,False,,t5_2qizd,False,,,True,t3_dec11,http://www.reddit.com/r/redditdev/comments/dec11/i_think_rrandom_can_work_better_can_someone_point/,
1284174304.0,2,self.redditdev,dcd39,Maximum string lengths in reddit,7,5,6,http://www.reddit.com/r/redditdev/comments/dcd39/maximum_string_lengths_in_reddit/,"I just went through some trial and error to work out the various maximum string lengths around reddit but I thought I'd ask here in case I've got anything wrong or someone else wanted to know.

* Max subreddit name length = 20
* Max username length = 20
* Max link name length = 300
* Max permalink url length = 90 (not including http://reddit.com)
* Max domain name length = 256 (maybe a few chars less)
* Max link url length = unlimited (I stopped testing after about 2100 chars)

Does this sound about right?",,False,,t5_2qizd,False,,,True,t3_dcd39,http://www.reddit.com/r/redditdev/comments/dcd39/maximum_string_lengths_in_reddit/,
1282726648.0,3,self.redditdev,d570p,reddit widget help,4,1,2,http://www.reddit.com/r/redditdev/comments/d570p/reddit_widget_help/,"I'd like to use this on a site I'm developing for my students. I'd prefer to use reddit over something like http://slinkset.com/ as I'd like them to have the option of being involved in the wider reddit community.

However similar to [the issue noted in this thread](http://www.reddit.com/r/redditdev/comments/cepjr/bug_in_reddit_widget_link_generator_selfsubreddit/) when I click on any of the comment thread titles in the widget it fails to load. All the other links in the widget are fine (voting arrows, subreddit title etc). 

I don't know much about programming or html but I assume it's something like a href=\""/r/subreddit/comments/ instead of a href=\""http://www.reddit.com/r/subreddit/comments 

Is there any way for me to download the javascript file (so I can fix it and then host it on our server). Or is it something a reddit developer could fix easily?

Also, in a semi-related request is there any way to have a reddit button that displays the number of comments in the thread rather than the points?

(Not sure if this is clear but I'd like to be able post a link to the course subreddit under each online lesson so Ss can discuss the materials. I'd like them to be able to see which lessons/materials are generating a lot of comments so they don't have to click on each link to see if other Ss have started a discussion or not.) 

Any help/suggestions appreciated!

",,False,,t5_2qizd,False,,,True,t3_d570p,http://www.reddit.com/r/redditdev/comments/d570p/reddit_widget_help/,
1275751405.0,4,self.redditdev,cbs3k,Possible usability and flow bug affecting new members and their subreddit subscriptions.,5,1,1,http://www.reddit.com/r/redditdev/comments/cbs3k/possible_usability_and_flow_bug_affecting_new/,"When a new member registers, he may add or edit his list of subreddits by going here: http://www.reddit.com/reddits/mine/ . Additionally, he can go here as well to see the list http://www.reddit.com/reddits/mine/subscriber .

Both pages list subreddits that seem to be automatically added by Reddit, because they are shown on the right side panel, and look like this when rendered as text:

# - frontpage+ announcements not approved
# - frontpage+ frontpageAskReddit
# - frontpage+ frontpageblog not approved
# - frontpage+ frontpagefunny
# - frontpage+ frontpagegaming
# - frontpage+ frontpagepics
# - frontpage+ frontpagepolitics
# - frontpage+ frontpageprogramming
# - frontpage+ frontpagereddit.com
# - frontpage+ frontpagescience
# - frontpage+ frontpageworldnews
# - frontpage+ frontpageWTF

Here is the bug in my point of view: when a new member goes into one of the above pages he sees nothing in his 'myreddits' listing or 'subscriber' listing and none of the .json or .xml feeds will return anything. But, as soon as he presses a button to unsubscribe, on the right side panel , let's say for ""r/pics"" , he will see the listing and the feeds contain all the remaining subreddit subscriptions.",,False,,t5_2qizd,False,,,True,t3_cbs3k,http://www.reddit.com/r/redditdev/comments/cbs3k/possible_usability_and_flow_bug_affecting_new/,
1275459171.0,3,self.redditdev,caioc,API - Comments - Retrieving comment perma-link,3,0,2,http://www.reddit.com/r/redditdev/comments/caioc/api_comments_retrieving_comment_permalink/,"What is the recommended way for constructing a perma-link to a comment?
  
If we take a JSON comment result like the following:

    { ""data"" : { ""author"" : ""BauerUK"",
          ""body"" : ""Wow, this looks like perfect 'ERROR'D' 
          material. You should 
          [submit it.](http://thedailywtf.com/Contact.aspx)"",
          
          ""body_html"" : ""&lt;div class=\""md\""&gt;&lt;p&gt;Wow, this looks like perfect 
          'ERROR'D' material. You should 
          &lt;a href=\""http://thedailywtf.com/Contact.aspx\"" &gt;submit it.
          &lt;/a&gt;&lt;/p&gt;&lt;/div&gt;"",
          
          ""created"" : 1275394413.0,
          ""created_utc"" : 1275394413.0,
          ""downs"" : 1,
          ""id"" : ""c0r6fsy"",
          ""likes"" : null,
          ""link_id"" : ""t3_ca6pu"",
          ""link_title"" : ""Dear Oracle: WTF?"",
          ""name"" : ""t1_c0r6fsy"",
          ""parent_id"" : ""t3_ca6pu"",
          ""replies"" : """",
          ""subreddit"" : ""geek"",
          ""subreddit_id"" : ""t5_2qh17"",
          ""ups"" : 5
        },
      ""kind"" : ""t1""
    }
    
I can see the `subreddit`, `link_id` and the comment `id`, however, that isn't enough to construct a perma-link of the comment. What is missing is the ""safe title"":

    http://www.reddit.com/r/geek/comments/ca6pu/dear_oracle_wtf/c0r6fsy
                           |___|         |_____|...............|_______|
                             \                \            \         \
                        `subreddit`           `link_id`  missing    `id`

I'm probably missing some simple/undocumented way to link to a comment. 

If not, is it safe to replicate the ""safe title"" creation process? I assume there's a possibility that this could change in the future.",,False,,t5_2qizd,False,,,True,t3_caioc,http://www.reddit.com/r/redditdev/comments/caioc/api_comments_retrieving_comment_permalink/,
1273326474.0,3,self.redditdev,c1gdb,CSV dump of user data?,3,0,9,http://www.reddit.com/r/redditdev/comments/c1gdb/csv_dump_of_user_data/,Would a CSV dump of data on users be available?,,False,,t5_2qizd,False,,,True,t3_c1gdb,http://www.reddit.com/r/redditdev/comments/c1gdb/csv_dump_of_user_data/,
1273149321.0,3,self.redditdev,c0kgh,Making a reddit comment from the command line.,3,0,1,http://www.reddit.com/r/redditdev/comments/c0kgh/making_a_reddit_comment_from_the_command_line/,"I'm trying to post a reddit comment from the command line with curl.  I've looked through the api docs and while this is meant to be possible, the information on how to do it hasn't been filled in.
Does anyone know how to do this?",,False,,t5_2qizd,False,,,True,t3_c0kgh,http://www.reddit.com/r/redditdev/comments/c0kgh/making_a_reddit_comment_from_the_command_line/,
1272456606.0,2,self.redditdev,bx7qh,I'm stuck at the paster line,4,2,10,http://www.reddit.com/r/redditdev/comments/bx7qh/im_stuck_at_the_paster_line/,"On FreeBSD7. I'm completely new to python so I apologize if this is super obvious. But in any case, please help!

    Traceback (most recent call last):
  File ""/usr/local/bin/paster"", line 8, in &lt;module&gt;
    load_entry_point('PasteScript==1.7.3', 'console_scripts', 'paster')()
  File ""/usr/local/lib/python2.6/site-packages/PasteScript-1.7.3-py2.6.egg/paste/script/command.py"", line 83, in run
    command = commands[command_name].load()
  File ""build/bdist.linux-i686/egg/pkg_resources.py"", line 1954, in load
  File ""/usr/local/lib/python2.6/site-packages/Pylons-0.9.6.2-py2.6.egg/pylons/__init__.py"", line 4, in &lt;module&gt;
    from pylons.config import config
  File ""/usr/local/lib/python2.6/site-packages/Pylons-0.9.6.2-py2.6.egg/pylons/config.py"", line 2, in &lt;module&gt;
    from pylons.configuration import *
  File ""/usr/local/lib/python2.6/site-packages/Pylons-0.9.6.2-py2.6.egg/pylons/configuration.py"", line 16, in &lt;module&gt;
    import pylons.legacy
  File ""/usr/local/lib/python2.6/site-packages/Pylons-0.9.6.2-py2.6.egg/pylons/legacy.py"", line 7, in &lt;module&gt;
    from paste.wsgiwrappers import WSGIResponse
  File ""/usr/local/lib/python2.6/site-packages/Paste-1.7.3.1-py2.6.egg/paste/wsgiwrappers.py"", line 19, in &lt;module&gt;
    from paste.util.mimeparse import desired_matches
ImportError: cannot import name desired_matches
",,False,,t5_2qizd,False,,,True,t3_bx7qh,http://www.reddit.com/r/redditdev/comments/bx7qh/im_stuck_at_the_paster_line/,
1267591920.0,3,self.redditdev,b8jnx,Is there a canonical list of 'wanted features'?,3,0,2,http://www.reddit.com/r/redditdev/comments/b8jnx/is_there_a_canonical_list_of_wanted_features/,"If one wanted to help with the reddit project where would one start?  I am running a small reddit site with a few dozen users right now (great fun trying to get it going on Ubuntu 9.10 :&gt;) and would love to get a bit more into the actual guts and give back to the community.  
",,False,,t5_2qizd,False,,,True,t3_b8jnx,http://www.reddit.com/r/redditdev/comments/b8jnx/is_there_a_canonical_list_of_wanted_features/,
1263906779.0,3,self.redditdev,arfqz,"Could we have a Reddit option that shows us the links that have more than 500, 1000 points that are not in the front page reddits...? Is that difficult?",4,1,7,http://www.reddit.com/r/redditdev/comments/arfqz/could_we_have_a_reddit_option_that_shows_us_the/,,,False,,t5_2qizd,False,,,True,t3_arfqz,http://www.reddit.com/r/redditdev/comments/arfqz/could_we_have_a_reddit_option_that_shows_us_the/,
1262864536.0,3,self.redditdev,amne7,problems with mobile mode,3,0,1,http://www.reddit.com/r/redditdev/comments/amne7/problems_with_mobile_mode/,"I've recently started browsing reddit in mobile mode. There are however a few problems. Apparently I don't have enough reddit history to submit tickets, so here goes:

Clicking on a self post takes me to the ""standard"" version. the actual comment link works fine though.

Another problem is that the actual text belonging to the self post is not displayed.

I'd love to see the [s],[m]  and [a] tags in the mobile view as well.

Hope this attracts some devs' attention.

EDIT: apparently #2 is already mentioned on trac: http://code.reddit.com/ticket/388",,False,,t5_2qizd,True,,,True,t3_amne7,http://www.reddit.com/r/redditdev/comments/amne7/problems_with_mobile_mode/,
1252537425.0,5,reddit.com,9izbu,Problem with link titles on Opera,6,1,2,http://www.reddit.com/r/redditdev/comments/9izbu/problem_with_link_titles_on_opera/,,,False,,t5_2qizd,False,,,False,t3_9izbu,http://www.reddit.com/r/self/comments/8w0bw/did_reddits_page_layout_just_change_again/c0amu63,
1251984167.0,3,self.redditdev,9gxiz,How can I modify controllers/error.py or config/middleware.py to retry on KeyError?,3,0,3,http://www.reddit.com/r/redditdev/comments/9gxiz/how_can_i_modify_controllerserrorpy_or/,"Hi,
my reddit installation [often gets a KeyError exception](http://www.reddit.com/r/redditdev/comments/8yvzs/keyerror_in_memcachepy/), despite having 128MB for memcached and only some 200 pageviews/day on average.

Since refreshing the page always works, when I get that error, I was thinking if it would be possible to modify the code to do that automatically.

Any help?
Thanks in advance!",,False,,t5_2qizd,False,,,True,t3_9gxiz,http://www.reddit.com/r/redditdev/comments/9gxiz/how_can_i_modify_controllerserrorpy_or/,
1250756302.0,3,self.redditdev,9cd2m,How do I get the API working?,3,0,5,http://www.reddit.com/r/redditdev/comments/9cd2m/how_do_i_get_the_api_working/,"Hi,
I read [here](http://code.reddit.com/wiki/API) that you can go to an URL like http://www.reddit.com/api/info?url=http://www.google.com/ and get directly to the reddit link or links.

But, in my reddit  installation this doesn't work: e.g. http://talk.geekherocomic.com/api/info?url=http://www.geekherocomic.com/2009/08/19/a-coding-paradox/

Is there something I need to enable in my installation to do this?

Also, is there an API to go directly to a submission and show its comments?

Thanks!",,False,,t5_2qizd,False,,,True,t3_9cd2m,http://www.reddit.com/r/redditdev/comments/9cd2m/how_do_i_get_the_api_working/,
1250368624.0,3,self.redditdev,9azge,Really odd problem: can't find my old links anymore,3,0,22,http://www.reddit.com/r/redditdev/comments/9azge/really_odd_problem_cant_find_my_old_links_anymore/,"Hi all, this problem will require a bit of an explanation: I run my own reddit (lots of modifications in my own git branch, but merged master in moments ago) at [Talk@GeekHeroComic](http://talk.geekherocomic.com/). I run it as a comment system for the comic strips: each strip has a button called Comment This, that has an url like this:
    talk.geekherocomic.com/goto?url=url-of-the-comic-strip

Now, that *goto* you see there is an action I have added to the *front* controller, and the code is the following:

    @validate(url = VRequired('url', None))
    def GET_goto(self, url):
        """"""Go to submission.""""""
        if url:
            # check to see if the url has already been submitted
            links = link_from_url(url)
            if links and len(links) == 1:
                return self.redirect(links[0].already_submitted_link)
            elif links:
                builder = IDBuilder([link._fullname for link in links])
                listing = LinkListing(builder, nextprev=False).listing()
                infotext = (strings.multiple_submitted
                            % links[0].resubmit_link())
                res = BoringPage(_(""seen it""),
                                 content = listing,
                                 infotext = infotext).render()
                return res
            else:
                captcha = Captcha() if c.user.needs_captcha() else None
                sr_names = (Subreddit.submit_sr_names(c.user) or
                            Subreddit.submit_sr_names(None))


                return FormPage(_(""submit""),
                                content=NewLink(url=url or '',
                                                title='',
                                                subreddits = sr_names,
                                                captcha=captcha,
                                                then = 'comments')).render()

What it does is: it figures out if we have the link already, and if not, redirects to the submit page. Of course if we have it, it redirects you there. It used to work like a charm until yesterday, until today I rebooted my server and it stopped working. Let me describe how:

It looks like it doesn't find the link with the *link_from_url* function, so it always redirects me to the submit page. The funny thing is that if I submit it again, my *goto* function will find it from now on.

This is pretty bad for me because it means that now the people can't comment anymore on the old strips.

Can you please help?",,False,,t5_2qizd,False,,,True,t3_9azge,http://www.reddit.com/r/redditdev/comments/9azge/really_odd_problem_cant_find_my_old_links_anymore/,
1246651790.0,3,self.redditdev,8y0uj,Can I serve reddit with nginx?,3,0,11,http://www.reddit.com/r/redditdev/comments/8y0uj/can_i_serve_reddit_with_nginx/,"I've read in the docs that it's possible to serve reddit from apache and lighttpd using mod_scgi, and I managed to get it working with apache. I have then decided to migrate my other sites to nginx, and I'm loving the better performances. Can I serve reddit with nginx?
Thanks!",,False,,t5_2qizd,False,,,True,t3_8y0uj,http://www.reddit.com/r/redditdev/comments/8y0uj/can_i_serve_reddit_with_nginx/,
1246352813.0,3,self.redditdev,8wwod,I keep getting this error in my reddit installation. Please help!,3,0,14,http://www.reddit.com/r/redditdev/comments/8wwod/i_keep_getting_this_error_in_my_reddit/,"This happens randomly and in lots of different use cases. Can you please help? Thanks!

Error - &lt;type 'exceptions.KeyError'&gt;: '7bc8fe1e3c65077c2ab69c5b30898d27'
URL: http://talk.geekherocomic.com/
File '/usr/lib/python2.5/site-packages/Paste-1.7.2-py2.5.egg/paste/exceptions/errormiddleware.py', line 144 in __call__
  app_iter = self.application(environ, sr_checker)
File '/root/src/reddit/r2/r2/config/middleware.py', line 263 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 331 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 280 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 295 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 98 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 98 in __call__
  return self.app(environ, start_response)
File '/root/src/reddit/r2/r2/config/middleware.py', line 400 in __call__
  return self.app(environ, start_response)
File '/usr/lib/python2.5/site-packages/Pylons-0.9.6.2-py2.5.egg/pylons/wsgiapp.py', line 314 in __call__
  return self.app(environ, start_response)
File '/usr/lib/python2.5/site-packages/Beaker-1.3.1-py2.5.egg/beaker/middleware.py', line 70 in __call__
  return self.app(environ, start_response)
File '/usr/lib/python2.5/site-packages/Beaker-1.3.1-py2.5.egg/beaker/middleware.py', line 149 in __call__
  return self.wrap_app(environ, session_start_response)
File '/usr/lib/python2.5/site-packages/Routes-1.8-py2.5.egg/routes/middleware.py', line 99 in __call__
  response = self.app(environ, start_response)
File '/usr/lib/python2.5/site-packages/Pylons-0.9.6.2-py2.5.egg/pylons/wsgiapp.py', line 95 in __call__
  response = self.dispatch(controller, environ, start_response)
File '/usr/lib/python2.5/site-packages/Pylons-0.9.6.2-py2.5.egg/pylons/wsgiapp.py', line 236 in dispatch
  return controller(environ, start_response)
File '/root/src/reddit/r2/r2/lib/base.py', line 94 in __call__
  res = WSGIController.__call__(self, environ, start_response)
File '/usr/lib/python2.5/site-packages/Pylons-0.9.6.2-py2.5.egg/pylons/controllers/core.py', line 160 in __call__
  response = self._inspect_call(self.__before__)
File '/usr/lib/python2.5/site-packages/Pylons-0.9.6.2-py2.5.egg/pylons/controllers/core.py', line 79 in _inspect_call
  result = func(**args)
File '/root/src/reddit/r2/r2/lib/base.py', line 47 in __before__
  self.pre()
File '/root/src/reddit/r2/r2/controllers/reddit_base.py', line 459 in pre
  set_subreddit()
File '/root/src/reddit/r2/r2/controllers/reddit_base.py', line 249 in set_subreddit
  c.site = Subreddit._by_name(sr_name)
File '/root/src/reddit/r2/r2/models/subreddit.py', line 102 in _by_name
  sr_id = cls._by_name_cache(name, _update = _update)
File '/root/src/reddit/r2/r2/lib/memoize.py', line 45 in new_fn
  res = fn(*a, **kw)
File '/root/src/reddit/r2/r2/models/subreddit.py', line 88 in _by_name_cache
  l = list(q)
File '/root/src/reddit/r2/r2/lib/db/thing.py', line 785 in __iter__
  lst = self._cursor().fetchall()
File '/root/src/reddit/r2/r2/lib/utils/utils.py', line 219 in fetchall
  return self._fetch(self.rp.fetchall())
File '/root/src/reddit/r2/r2/lib/utils/utils.py', line 214 in _fetch
  return self.fn(res)
File '/root/src/reddit/r2/r2/lib/db/thing.py', line 839 in row_fn
  return self._kind._byID(_ids, self._data, False, extra_props)
File '/root/src/reddit/r2/r2/lib/db/thing.py', line 267 in _byID
  bases = sgm(cache, ids, items_db, prefix)
File '/root/src/reddit/r2/r2/lib/cache.py', line 200 in sgm
  r = cache.get_multi(s_keys.keys(), prefix)
File '/root/src/reddit/r2/r2/lib/cache.py', line 45 in get_multi
  r = self.simple_get_multi(key_map.keys())
File '/root/src/reddit/r2/r2/lib/cache.py', line 184 in simple_get_multi
  r = c.simple_get_multi(need)
File '/root/src/reddit/r2/r2/lib/contrib/memcache.py', line 763 in get_multi
  retvals[prefixed_to_orig_key[rkey]] = val   # un-prefix returned key.
KeyError: '7bc8fe1e3c65077c2ab69c5b30898d27'
",,False,,t5_2qizd,False,,,True,t3_8wwod,http://www.reddit.com/r/redditdev/comments/8wwod/i_keep_getting_this_error_in_my_reddit/,
1242267550.0,3,self.redditdev,8kb77,"I'm suddenly getting 10x the number of articles in my RSS feed, has something changed?",3,0,0,http://www.reddit.com/r/redditdev/comments/8kb77/im_suddenly_getting_10x_the_number_of_articles_in/,,,False,,t5_2qizd,False,,,True,t3_8kb77,http://www.reddit.com/r/redditdev/comments/8kb77/im_suddenly_getting_10x_the_number_of_articles_in/,
1230761216.0,5,self.redditdev,7mpn9,"AskRedDev: If I mod and then unmod a post, is there a record of that action?",6,1,1,http://www.reddit.com/r/redditdev/comments/7mpn9/askreddev_if_i_mod_and_then_unmod_a_post_is_there/,,,False,,t5_2qizd,False,,,True,t3_7mpn9,http://www.reddit.com/r/redditdev/comments/7mpn9/askreddev_if_i_mod_and_then_unmod_a_post_is_there/,
1230660280.0,3,self.redditdev,7mh1i,A way to flag posts with common complaints instead of spamming the comments each and every time,5,2,3,http://www.reddit.com/r/redditdev/comments/7mh1i/a_way_to_flag_posts_with_common_complaints/,,,False,,t5_2qizd,False,,,True,t3_7mh1i,http://www.reddit.com/r/redditdev/comments/7mh1i/a_way_to_flag_posts_with_common_complaints/,
1225342002.0,3,reddit.com,7a80p,Is there a reason it's difficult to see what subreddits a user moderates??,3,0,1,http://www.reddit.com/r/redditdev/comments/7a80p/is_there_a_reason_its_difficult_to_see_what/,,,False,,t5_2qizd,False,,,False,t3_7a80p,http://www.reddit.com/r/reddit.com/comments/79yn8/subreddit_squatting_have_you_ever_wanted_to_see/,
1213970188.0,2,self.redditdev,6o8bn,Feature Request - Responses:  Enable users to post responses to headlines and sort them like comments,6,4,13,http://www.reddit.com/r/redditdev/comments/6o8bn/feature_request_responses_enable_users_to_post/,,,False,,t5_2qizd,False,,,True,t3_6o8bn,http://www.reddit.com/r/redditdev/comments/6o8bn/feature_request_responses_enable_users_to_post/,
1376680457.0,2,self.redditdev,1ki7v4,I want to create a python script to scrape some reddit data. Some non-programmer type questions.,2,0,8,http://www.reddit.com/r/redditdev/comments/1ki7v4/i_want_to_create_a_python_script_to_scrape_some/,"Technical : I want to track some up/down voting patterns, so I need to repeatedly 'ping' the server (actually I want to retrieve json data). How often is too often to 'ping' the server? I dont want to 'piss off the servers'.

Legal : The data gathered is for personal use (education, satisfy curiosity), not for commercial or any other uses. Is this against any reddiquette or TOS or illegal in any way? (Basically datamining reddit) I dont want to 'piss off the alien'.

",,False,,t5_2qizd,False,,,True,t3_1ki7v4,http://www.reddit.com/r/redditdev/comments/1ki7v4/i_want_to_create_a_python_script_to_scrape_some/,
1376624441.0,3,self.redditdev,1kgsy4,A Reddit API wrapper for Java using Jersey,6,3,0,http://www.reddit.com/r/redditdev/comments/1kgsy4/a_reddit_api_wrapper_for_java_using_jersey/,"Hey everyone,

Currently working on simple-as-dirt Reddit API wrapper in Java. Any comments/criticism would be greatly appreciated. Still very much a work in progress. Planning on doing some neat stuff with Neo4j once I figure out the Reddit API a little better.

https://github.com/corydissinger/reddit-jersey-client",,False,,t5_2qizd,False,,,True,t3_1kgsy4,http://www.reddit.com/r/redditdev/comments/1kgsy4/a_reddit_api_wrapper_for_java_using_jersey/,
1376532631.0,2,self.redditdev,1ke588,Entry point of reddit server image,3,1,0,http://www.reddit.com/r/redditdev/comments/1ke588/entry_point_of_reddit_server_image/,Noobie at reading reddit code. I'd like to start reading code from the entry point. How can I find the entry point for a user who is first registering in reddit?,,False,,t5_2qizd,False,,,True,t3_1ke588,http://www.reddit.com/r/redditdev/comments/1ke588/entry_point_of_reddit_server_image/,
1376504492.0,2,self.redditdev,1kd3s3,Is there a way to get all past instances of when someone mentioned your username?,2,0,1,http://www.reddit.com/r/redditdev/comments/1kd3s3/is_there_a_way_to_get_all_past_instances_of_when/,"I had asked this question in a thread discussing Reddit Gold but all the responses were silly ""nice try NSA"" responses. This would be an interesting feature to implement on a per username basis so only that particular username could perform the search. 

",,False,,t5_2qizd,False,,,True,t3_1kd3s3,http://www.reddit.com/r/redditdev/comments/1kd3s3/is_there_a_way_to_get_all_past_instances_of_when/,
1376319062.0,2,self.redditdev,1k7jeb,Help getting subscribed subreddits?,2,0,5,http://www.reddit.com/r/redditdev/comments/1k7jeb/help_getting_subscribed_subreddits/,"I'm working on a site that will allow users to give me their reddit credentials and the script will record their username and all of the subreddits they're subscribed to.

I doubt this will actually take off or anything, but it's a response to an interesting thread on /r/CrazyIdeas that I thought would be nice to actually implement.

As I don't know anything about the reddit API, or Google app scripts, I'm having a bit of trouble. **How exactly would I get the user's subscribed subreddits? Do I need their password or is that accessible in the first place?** (I'm sure people would feel a lot more comfortable if they didn't have to give their password).

After I get back their subreddits I've just got to figure out how to use Google Scripts to record the data in a Google Spreadsheet and later access it for graphs and shit. (honestly, I'm wondering if it's even worth not doing this in dreamweaver at this point). 

Any help will be greatly appreciated.

Edit: thanks for the help. Reddit is blocked from my workplace, but I generally access it on slow days using my universities VPN (like I'm doing now). SSL doesn't work on the VPN (gmail works, but not much else) and it didn't occur to me until now that this may also be an issue.",,False,,t5_2qizd,1376322364.0,,,True,t3_1k7jeb,http://www.reddit.com/r/redditdev/comments/1k7jeb/help_getting_subscribed_subreddits/,
1375829167.0,2,self.redditdev,1jue0c,"/api/morechildren call working for post's comments, but not for comment's comments (replies)",2,0,0,http://www.reddit.com/r/redditdev/comments/1jue0c/apimorechildren_call_working_for_posts_comments/,"I'm trying to call /api/morechildren, and it's working perfectly fine with getting more comments to a post (sending a post request to ""http://api.reddit.com/api/morechildren"" even returns them in complete json format, after some struggling I had with getting them as HTML objects of some sort).

However, trying to get more replies to a comment gives me a hard time, resulting in an ""Internal Server Error"" from the server, for the same request (with different parameters obviously).

I thought that maybe reddit was just under temporary load, but I tried several times over the course of an hour or so, and every time it worked perfectly for posts's comments, and resulting in the same error for comment's comments (replies).

Please advise on the issues.",,False,,t5_2qizd,False,,,True,t3_1jue0c,http://www.reddit.com/r/redditdev/comments/1jue0c/apimorechildren_call_working_for_posts_comments/,
1375801933.0,2,self.redditdev,1jtcm3,Reddit API - Getting the logged in user's email.,2,0,10,http://www.reddit.com/r/redditdev/comments/1jtcm3/reddit_api_getting_the_logged_in_users_email/,"I'm trying to use the reddit API to get the logged in user's email address as it appears on reddit, but didn't find any API call to get it, not do I can it in the json after logging in.

Did I miss something?


P.S

I want to get it in order to use the /api/update API call, that requires an email to change the user's password (if the email isn't set it'll just be put as a blank email in reddit instead of the existing one).",,False,,t5_2qizd,False,,,True,t3_1jtcm3,http://www.reddit.com/r/redditdev/comments/1jtcm3/reddit_api_getting_the_logged_in_users_email/,
1375785886.0,2,self.redditdev,1jsycq,Resolving domain for installation on new personal server,2,0,2,http://www.reddit.com/r/redditdev/comments/1jsycq/resolving_domain_for_installation_on_new_personal/,"Hi people, I had been running the VM on my machine to test on the clone for quite a few weeks, but yesterday I got a new machine at work and I'd like to use that to host it. On my machine I wrote ""0.0.0.0 reddit.local"" in etc/hosts to access the instance with the browser, but I can't understand how to set this on my server. I'm running Apache and this is what I did so far:

* Changed reddit.local to 123.123.123.123 (let's say this is my ip) in the install script
* Installed the clone

If I write 123.123.123.123 in the browser i still get the standard welcome page of Apache.
So what to I add to etc/hosts for the machine's IP to resolve to the instance of reddit?

I'm quite inexperienced on this stuff, but I'm learning python and writing some bot and I'd like to test them on something more powerful than my laptop :/

*Edit:* Maybe I need to set apache to redirect the IP to 0.0.0.0:80? ",,False,,t5_2qizd,1375786589.0,,,True,t3_1jsycq,http://www.reddit.com/r/redditdev/comments/1jsycq/resolving_domain_for_installation_on_new_personal/,
1375308566.0,2,self.redditdev,1jg9yw,Understanding the before/after parameters in GET /new,4,2,2,http://www.reddit.com/r/redditdev/comments/1jg9yw/understanding_the_beforeafter_parameters_in_get/,"Hello, I was screwing around with clojure and trying to build a simple reddit client.  I thought I could start by just polling a subreddit every ~5 minutes for new submissions.  I start with a call to

http://www.reddit.com/r/clojure/new.json?limit=1

I then grab the name (right now t3_1jg35w) and then try to call before with the belief that if anything is newer than t3_1jg35w I'll get it back.
http://www.reddit.com/r/clojure/new.json?limit=1?before=t3_1jg35w

Instead I seem to return a large number of results with t3_1jg35w still being the first.  

I thought maybe after was the parameter I should be using instead, but that appears to also return a set of results with t3_1jg35w first..

http://www.reddit.com/r/clojure/new.json?limit=1?after=t3_1jg35w

Am I misusing this API?  or put another way, is there a way to accomplish what I'm trying to do here?",,False,,t5_2qizd,False,,,True,t3_1jg9yw,http://www.reddit.com/r/redditdev/comments/1jg9yw/understanding_the_beforeafter_parameters_in_get/,
1375280170.0,2,self.redditdev,1jf6xz,"Thumbnail data not showing in jsonp request, but can view the data in json",3,1,4,http://www.reddit.com/r/redditdev/comments/1jf6xz/thumbnail_data_not_showing_in_jsonp_request_but/,"Hi, I'm having a confusing problem when trying to pull a thumbnail URL from a particular subreddit.

I'm using the get getJson function which assigns the parsed json to an object. I'm able to pull all the relevant information from &lt;object&gt;.data.children[i].data such as url, title, score, etc.. All other than thumbnail. 

I'm able to access the thumbnail value on other subreddits without issue.

Are certain subreddits able to lock out thumbnail data, or is this a bug?

An example of this would be viewing 

http://reddit.com/r/NetflixBestOf.json in http://jsonviewer.stack.hu/ (json viewer) you can see the thumbnail url is present

but when you try and parse it in code

http://jsfiddle.net/jYsJn/ - the value is simply empty. If you change the subreddit retrieved in this example to ""videos"" for example it pulls them fine.


Sorry if this is obvious, I'm still getting the hang of the whole thing. Thanks.
",,False,,t5_2qizd,False,,,True,t3_1jf6xz,http://www.reddit.com/r/redditdev/comments/1jf6xz/thumbnail_data_not_showing_in_jsonp_request_but/,
1375233054.0,2,self.redditdev,1je1uk,any way to get the total page count of a user's comments?,3,1,9,http://www.reddit.com/r/redditdev/comments/1je1uk/any_way_to_get_the_total_page_count_of_a_users/,"I know how to grab the JSON of a page of a user's comments, and how to minimize page requests by specifying the max limit of 100 comments.

**Is there a way to get the total count of comment pages, *before* fetching them?**  Right now, I'm just counting them as I fetch them, but I'd like to be able to display something like ""grabbing page X / [total]"" while my script runs, so the user has some indication of how long it will take until it is finished.  *(I realize that this count would vary depending on how many comments were on each page.)*",,False,,t5_2qizd,False,,,True,t3_1je1uk,http://www.reddit.com/r/redditdev/comments/1je1uk/any_way_to_get_the_total_page_count_of_a_users/,
1375226894.0,2,self.redditdev,1jdugq,[PRAW] Script to notify me via text/e-mail when a new post has been submitted to a specific sub,3,1,6,http://www.reddit.com/r/redditdev/comments/1jdugq/praw_script_to_notify_me_via_textemail_when_a_new/,SSIA.  Anyone  have any examples of such a script.  I basically want to poll a subreddit and get a notification sent to me whenever a new post is submitted.   Thanks in advance,,False,,t5_2qizd,False,,,True,t3_1jdugq,http://www.reddit.com/r/redditdev/comments/1jdugq/praw_script_to_notify_me_via_textemail_when_a_new/,
1375226850.0,2,self.redditdev,1jduep,confused about morechildren,2,0,0,http://www.reddit.com/r/redditdev/comments/1jduep/confused_about_morechildren/,"I want to go to a subreddit, get all the comments, and view the 'more comments' section. If the id is 't3_1jce6p', what do i put for children? it says put in the children id's delimited by a comma, but I am not supposed to actually put in children 1 by 1?  there are hundreds of them. 

this post helped a bit but I am still lost, http://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/

I want to return json of hidden comments, so what is the correct way to do this? and what if there are more comments after clicking 'more comments'

here is the non working code I have come up with

    data = { 'link_id' : 't3_1jce6p',
         'children' : 'cbdcft5', #not sure what to put here, but i think its a list of the children id's
         'api_type': 'json'
         }
         
    client = requests.session()

    r = client.post('http://www.reddit.com/api/morechildren', data=data)
    j = json.loads(r.content)

    print j['data']['children']

right now I have a working program to type in a user name and return all their posts, but I want to go a step further and give the post as well as the question it answers(not even sure how to do this yet)",,False,,t5_2qizd,False,,,True,t3_1jduep,http://www.reddit.com/r/redditdev/comments/1jduep/confused_about_morechildren/,
1375059806.0,2,self.redditdev,1j8wfe,"[PRAW] download images in subs, if you upvote them?",2,0,9,http://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/,"There are a list of subs, (ex: /r/earthporn , /r/spaceporn/ ) that if I upvote an image I want to download it using praw.

Do I have to 

1. Grab the list like normal
    
    submissions = r.get_subreddit('EarthPorn').get_hot(limit=10)

2. Another query [LoggedInRedditor.get_liked()](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.LoggedInRedditor.get_liked) and check for collisions?

Or am I missing a function somewhere on submissions?",,False,,t5_2qizd,False,,,True,t3_1j8wfe,http://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/,
1374521783.0,1,self.redditdev,1itzck,"Since reddit is open source, couldn't users submit documentation to be added?",6,5,6,http://www.reddit.com/r/redditdev/comments/1itzck/since_reddit_is_open_source_couldnt_users_submit/,Are the current admins as anti documentation to the source code as the original guys? ,,False,,t5_2qizd,False,,,True,t3_1itzck,http://www.reddit.com/r/redditdev/comments/1itzck/since_reddit_is_open_source_couldnt_users_submit/,
1374445912.0,2,self.redditdev,1irw0w,API question,4,2,1,http://www.reddit.com/r/redditdev/comments/1irw0w/api_question/,"Hey guys, I'm working on some bots right now for reddit.  I'm currently using a curl request to grab comments on reddit.com/comments and page through them, then start at the new current list when I reach the end.  However, I've tried adding a comment to a thread, then checking reddit.com/comments and haven't found the comment.  Does reddit.com/comments not actually show a stream of ALL new comments???  If not, what are the rules of what is shown there?  Where can I get a stream of all new comments?",,False,,t5_2qizd,False,,,True,t3_1irw0w,http://www.reddit.com/r/redditdev/comments/1irw0w/api_question/,
1374367412.0,2,self.redditdev,1iq3s1,Reddit API Logout?,4,2,3,http://www.reddit.com/r/redditdev/comments/1iq3s1/reddit_api_logout/,How do you full logout of Reddit -- like can I destroy the current session? /logout doesn't do anything. Thanks!,,False,,t5_2qizd,False,,,True,t3_1iq3s1,http://www.reddit.com/r/redditdev/comments/1iq3s1/reddit_api_logout/,
1374118831.0,2,self.redditdev,1ijb3m,Error when running a PRAW script,3,1,2,http://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/,"So I've been trying to run a script and I keep getting this error:

    Traceback (most recent call last):
      File ""__main__.py"", line 14, in &lt;module&gt;
        main()
      File ""__main__.py"", line 11, in main
        bot.go()
      File ""c:\path\to\my\script.py"", line 381, in go
        before_id = self.scan(before_id)
      File ""c:\path\to\my\script.py"", line 161, in scan
        if self.do_stuff(comment):
      File ""c:\path\to\my\script.py"", line 254, in do_stuff
        comments = self.get_thread_comments(orig_comment)
      File ""c:\path\to\my\script.py"", line 315, in get_thread_comments
        new_comments = reply.comments()
      File ""C:\Program Files\Python27\lib\site-packages\praw\objects.py"", line 583, in comments
        not in self.submission._comments_by_id]
    AttributeError: 'NoneType' object has no attribute '_comments_by_id'

The lines in question are as follows:

            for reply in item.replies:
                if type(reply) is praw.objects.MoreComments:
                    new_comments = reply.comments() #Crashes here
                    for comment in new_comments:
                        stack.append(comment)
                stack.append(reply)

The call to comments() is dying for some reason that's a complete mystery to me. What's going on?",,False,,t5_2qizd,False,,,True,t3_1ijb3m,http://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/,
1374103351.0,2,self.redditdev,1iirrw,Just installed PRAW. Keep getting error when trying to running a script.,4,2,2,http://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/,"I'm new to Python and PRAW. I just installed PRAW today and was following some example code I found online. I keep getting an error when it reaches the line r.login().

    Traceback (most recent call last):
      File ""C:/Users/Sam/Documents/learning.py"", line 7, in &lt;module&gt;
        r.login()
      File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 1120, in login
        self.request_json(self.config['login'], data=data)
      File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 95, in wrapped
        return_value = function(reddit_session, *args, **kwargs)
      File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 469, in request_json
        response = self._request(url, params, data)
      File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 342, in _request
        response = handle_redirect()
      File ""C:\Python27\lib\site-packages\praw\__init__.py"", line 313, in handle_redirect
        response = self.handler.request(request=request.prepare(),
    AttributeError: 'Request' object has no attribute 'prepare'

I took out the r.login() part and it ran until it got to:

    for submission in subreddit.get_hot(limit=10):
Then it gave me the same AttributeError: 'Request' object has no attribute 'prepare'. I'm sure this is something simple, but I'm a complete beginner and clueless as to what it is.",,False,,t5_2qizd,False,,,True,t3_1iirrw,http://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/,
1374100533.0,2,self.redditdev,1iio7g,How to search and pull url of a subreddit logo.,3,1,9,http://www.reddit.com/r/redditdev/comments/1iio7g/how_to_search_and_pull_url_of_a_subreddit_logo/,"I'm building a web app that uses the reddit api and would like to be able to access the direct thumbnail url of a subreddit's logo. I would already have captured an article's unique ID and/or it's respective subreddit.

Each logo has it's own unique page/url but I don't seem to find a correlation between it and the subreddit id.

Anyone know how?",,False,,t5_2qizd,False,,,True,t3_1iio7g,http://www.reddit.com/r/redditdev/comments/1iio7g/how_to_search_and_pull_url_of_a_subreddit_logo/,
1373921909.0,2,self.redditdev,1id5y0,reddit wiki,3,1,1,http://www.reddit.com/r/redditdev/comments/1id5y0/reddit_wiki/,"Hi, my wiki instance does not show the wiki.

It outputs:

""no tienes permiso para hacer eso
 wiki_disabled.""

But wiki is not disabled on run.ini config file:

disable_wiki = false

Does anyone know if theres any other place to enable it?
J.M.",,False,,t5_2qizd,False,,,True,t3_1id5y0,http://www.reddit.com/r/redditdev/comments/1id5y0/reddit_wiki/,
1373446198.0,2,self.redditdev,1hzwgn,[PRAW] Get all submissions for a specific time,2,0,2,http://www.reddit.com/r/redditdev/comments/1hzwgn/praw_get_all_submissions_for_a_specific_time/,"Hi,

I recently started to crawl all submissions in Reddit. However, I stopped the crawling process and now I am missing many submissions in the past I now want to get. Is this somehow possible? For getting all submissions I used the following code:

&gt;r.get_subreddit('all').get_new(limit=None)

I saw that the Reddit API has both a ""before"" and ""after"" parameter which seem to do exactly what I want. Can someone tell me if and how I can change my code accordingly?

Philipp",,False,,t5_2qizd,False,,,True,t3_1hzwgn,http://www.reddit.com/r/redditdev/comments/1hzwgn/praw_get_all_submissions_for_a_specific_time/,
1373370046.0,2,self.redditdev,1hxgmy,Evented message receiving in API?,2,0,3,http://www.reddit.com/r/redditdev/comments/1hxgmy/evented_message_receiving_in_api/,"In API, is it possible to get instant notifications of new unread messages (/message/unread) _without periodic polling_?

Or not necessarily instant, e.g. with any delay under 5 minutes.

And for other possible events as well (if there's any).",,False,,t5_2qizd,False,,,True,t3_1hxgmy,http://www.reddit.com/r/redditdev/comments/1hxgmy/evented_message_receiving_in_api/,
1372741321.0,2,self.redditdev,1hh8q6,Startup Error,2,0,9,http://www.reddit.com/r/redditdev/comments/1hh8q6/startup_error/,"With this code:

    import praw
    user_agent = ""Test Bot by RTLShadow""
    r = praw.Reddit(user_agent = user_agent)

I get this error:
    AttributeError: 'NoneType' object has no attribute 'write'

Any idea what the problem is?",,False,,t5_2qizd,False,,,True,t3_1hh8q6,http://www.reddit.com/r/redditdev/comments/1hh8q6/startup_error/,
1372679822.0,2,self.redditdev,1hf6fi,"[PRAW] Can it change the sidebar (description) text? Also, PRAW keeps asking for a captcha when I test the basic script.",3,1,4,http://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/,"I am a noob at both PRAW, python and CSS, so please bear with me...

I am writing a script to post several weekly community threads. I am also working on a CSS mod that generates a menu at the top of the page, so I can link to the the latest version of each of the three weekly community threads, effectively making a 'sticky' post at the top of the page. The CSS menu takes the menu content (names and links to each post) from the text in the sidebar.

Is there anyway to get PRAW to replace the link in the sidebar text for last weeks thread with the new one?

I have tested the basic posting script in a test subreddit and it works OK. I created a new account to use to post the new community threads from, but each time I run the script I get asked to type in a captcha. How would this work when I run it from a cron job?

Many thanks :)",,False,,t5_2qizd,False,,,True,t3_1hf6fi,http://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/,
1372311200.0,2,self.redditdev,1h5u3a,[PRAW] Creating multi-line comments,4,2,8,http://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/,"I'm trying to write a comment with PRAW,  but I've realized that I don't know how to make comments multi-line. I want to use a variable in it too. Something like this:
    
    username = ""cgillett""
    submission.add_comment(""Username:  "" + username + ""\n hey"")

Thanks for your help!

",,False,,t5_2qizd,False,,,True,t3_1h5u3a,http://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/,
1372105363.0,2,self.redditdev,1gzol9,Could someone ELI5 me how to login programatically to reddit?,4,2,5,http://www.reddit.com/r/redditdev/comments/1gzol9/could_someone_eli5_me_how_to_login/,"Context: I am writing a toy API wrapper in Haskell(because I haven't found any in that language) as a personal project, hopefully being able to rewrite my r/no_sob_story's bot in it. I can already get content from reddit, and I am trying to post: but I am a bit of a noob when it comes to HTTP and can't understand how logging in works. My questions are:

- What URI should I use? Is `http://www.reddit.com/api/login/{u}?username={u}%password={p}%api_type=json` the correct one?

-  How should the POST request look like? Any special headers or something?

- What data should I keep for the login to last?

- Anything else I should know? (Notice that I am writing this from scratch, so no wrapper is handling anything from me).


I am using haskell's builtin [HTTP library](http://hackage.haskell.org/package/HTTP), specifically the `Network.Browser` functionality, if that helps.


Edit: Thank you all, I logged in successfully.",,False,,t5_2qizd,1372180823.0,,,True,t3_1gzol9,http://www.reddit.com/r/redditdev/comments/1gzol9/could_someone_eli5_me_how_to_login/,
1371570104.0,2,self.redditdev,1gl8lg,Changes to the reddit PHP SDK to update link flair and to allow listings by search.,3,1,0,http://www.reddit.com/r/redditdev/comments/1gl8lg/changes_to_the_reddit_php_sdk_to_update_link/,"This probably belongs on github, but hopefully anyone who needs it will also check here.

* Search listings

An update to the getListing() function.  Just looks for a ? in the $sr parameter and splits it into the URL accordingly.

        public function getListing($sr, $limit = 5){
            $limit = (isset($limit)) ? ""?limit="".$limit : """";
            if($sr == 'home' || $sr == 'reddit' || !isset($sr)){
                $urlListing = ""http://www.reddit.com/.json{$limit}"";
            } else if (strpos($sr, ""?"") !== false) {
                $qPos = strpos($sr, ""?"");
                $urlListing = ""http://www.reddit.com/r/"" . substr($sr, 0, $qPos) . "".json{$limit}&amp;"" . substr($sr, $qPos + 1);
            } else {
                $urlListing = ""http://www.reddit.com/r/{$sr}/.json{$limit}"";
            }

            return $this-&gt;runCurl($urlListing);
        }

* Link flair update

Requires that you already know the flair_template_id value of the flair you want to use.

        public function setLinkFlair($subreddit, $post, $flair_template_id){
            $urlFlair = ""{$this-&gt;apiHost}/selectflair"";
            $postData = sprintf(""r=%s&amp;flair_template_id=%s&amp;link=%s&amp;uh=%s""
                ,$subreddit
                ,$flair_template_id 
                ,$post
                ,$this-&gt;modHash
            );
            $response = $this-&gt;runCurl($urlFlair, $postData);
            return $response;
        }
",,False,,t5_2qizd,False,,,True,t3_1gl8lg,http://www.reddit.com/r/redditdev/comments/1gl8lg/changes_to_the_reddit_php_sdk_to_update_link/,
1371558802.0,2,self.redditdev,1gkwrz,Having difficulty fetching a user's subreddit subscriptions using Oauth,3,1,0,http://www.reddit.com/r/redditdev/comments/1gkwrz/having_difficulty_fetching_a_users_subreddit/,"I have been following various php examples (https://github.com/jariz/RedditOAuth https://github.com/reddit/reddit/wiki/OAuth2-PHP-Example)
for using Oauth and the reddit API to allow a reddit user to authorise an app to use some of their information.

Using the examples above as they stand (substituting my keys) works find, but as soon as I change the scope to include ""mysubreddits"" (see http://www.reddit.com/dev/api/oauth#GET_subreddits_mine_{where}) and attempt to use the authorisation key given to fetch the /subreddits/mine/subscriber.json data, the page never loads. There is no error message and I have left it to run for about 30mins to see if it is just my network being slow.

Long story short, I have not been able to find a single example of code that uses something other than /api/v1/me.json, am I missing something in terms of parameters that must be included?",,False,,t5_2qizd,False,,,True,t3_1gkwrz,http://www.reddit.com/r/redditdev/comments/1gkwrz/having_difficulty_fetching_a_users_subreddit/,
1371369445.0,2,self.redditdev,1gg1ja,Any way yet to retrieve more than 1000 comments for a user?,7,5,7,http://www.reddit.com/r/redditdev/comments/1gg1ja/any_way_yet_to_retrieve_more_than_1000_comments/,,,False,,t5_2qizd,False,,,True,t3_1gg1ja,http://www.reddit.com/r/redditdev/comments/1gg1ja/any_way_yet_to_retrieve_more_than_1000_comments/,
1370927934.0,2,self.redditdev,1g3p8y,Getting only new submissions since last request.,3,1,6,http://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/,"Does this exist in the API? using praw

I would like to have something like this.

    submissions = r.get_subreddit('askreddit').get_new(since=date/last request thing)

Thanks
",,False,,t5_2qizd,False,,,True,t3_1g3p8y,http://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/,
1370695032.0,2,self.redditdev,1fx8ws,How can I keep a bot running continuously when reddit.com crashes?,5,3,5,http://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/,"I have a simple bot written in Python (PRAW) that automatically flairs posts on a subreddit. However, the bot crashes when my internet drops out, reddit.com crashes, or I have a blackout for a few hours.

How can I make the bot account for these and survive the problem?",,False,,t5_2qizd,False,,,True,t3_1fx8ws,http://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/,
1370421887.0,2,self.redditdev,1fplrx,"How do you use PRAW's ""set_stylesheet""?",6,4,10,http://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/,"Assume please that I have called ""get_stylesheet"" on a sub that I moderate, and I have that object. It's a dict, and an element of the dict is the CSS for the sub, and it's correct, so I know the ""get_stylesheet"" call worked properly.

What can I pass to ""set_stylesheet"" that will not result in this error:

&gt; praw.errors.APIException: (BAD_CSS) `invalid css` on field `stylesheet_contents`

I have tried passing the dictionary, and dictionary[""stylesheet""], and both produce this error.

    rh = praw.Reddit(""whatever"")
    &lt;login&gt;
    sh = rh.get_subreddit(sub)
    ss = sh.get_stylesheet()

    sh.set_stylesheet(&lt;what goes here?&gt;)

ss, stylesheet = ss, ss[""stylesheet""], stylesheet=ss[""stylesheet""], all of these result in the above error.

I am logged on and a moderator of the sub.

I have also tried:

    ss = rh.get_stylesheet(sub)
    rh.set_stylesheet(sub, &lt;whatever&gt;)

... with the same results.

I would like to eventually be able to make changes to the stylesheet, but for now I'd settle for a no-op.
",,False,,t5_2qizd,False,,,True,t3_1fplrx,http://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/,
1370381009.0,2,self.redditdev,1fofdk,"Is it allowed to scrape thumbnails from Reddit? I am currently getting an ""access denied"" from redditmedia when trying to access them directly.",3,1,4,http://www.reddit.com/r/redditdev/comments/1fofdk/is_it_allowed_to_scrape_thumbnails_from_reddit_i/,,,False,,t5_2qizd,False,,,True,t3_1fofdk,http://www.reddit.com/r/redditdev/comments/1fofdk/is_it_allowed_to_scrape_thumbnails_from_reddit_i/,
1370125281.0,2,self.redditdev,1fhjbt,PRAW error when using flat_comments,3,1,2,http://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/,"I am getting the following error when attempting to follow the parsing comment example on the praw wiki:

    Traceback (most recent call last):
      File ""C:\Users\Jason\Desktop\utahbot.py"", line 6, in &lt;module&gt;
        flat_comments = praw.helpers.flatten_tree(submission.comments_flat)
      File ""C:\Python27\lib\site-packages\praw\objects.py"", line 82, in __getattr__
        attr))
    AttributeError: '&lt;class 'praw.objects.Submission'&gt;' has no attribute 'comments_flat'


The following is my code:

    import time
    import praw
    r = praw.Reddit('Test Bot by /u/jcannon98188')
    r.login('BOTUSERNAME','BOTPASSWORD')
    submission = r.get_submission(submission_id='1fhhxs')
    flat_comments = praw.helpers.flatten_tree(submission.comments_flat)
    already_done = []
    for comment in flat_comments:
        if comment.body == ""Utah"" and comment.id not in already_done:
            comment.reply(' world!')
            already_done.append(comment.id)
    



Any ideas on what is happening?",,False,,t5_2qizd,False,,,True,t3_1fhjbt,http://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/,
1369990651.0,2,self.redditdev,1fe7pj,Does PRAW support Wiki stuff?,5,3,4,http://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/,"In particular I want to maintain state via a Reddit Wiki page.

I'm sorry if this something obvious but when I google ""praw wiki"" the signal to noise ratio is zero.

If doing this via a Wiki is insane, is there another obvious way to maintain state? I could do it via a thread in a subreddit, but there are length limits on self-posts and comments, and stuff gets archived, etc.

Thank you,
",,False,,t5_2qizd,False,,,True,t3_1fe7pj,http://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/,
1369643025.0,2,self.redditdev,1f4mtc,Where do I find my praw.ini in Ubuntu 12.04?,2,0,4,http://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/,"I want to configure proxy settings in PRAW. Documentation says praw.ini should be in ""/home/foobar/.config/praw.ini"" but there is no foobar folder in my home directory. There is a .config folder but it doesn't contain praw.ini.",,False,,t5_2qizd,False,,,True,t3_1f4mtc,http://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/,
1369356357.0,2,self.redditdev,1exvqb,"After weeks of trial and error I still can't catch Reddit downtime exceptions (HTTP 429, 502, etc). Help?",2,0,6,http://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/,"So the exception usually looks something like this:

	ERROR 2013-05-24 00:08:14,296 _check_inbox(): couldn't mark message as read: 502 Server Error: Bad Gateway
	ERROR 2013-05-24 00:08:14,297 Caught exception in main() loop: 502 Server Error: Bad Gateway
	Traceback (most recent call last):
	  File ""/home/dv/git/cointipbot/src/cointipbot.py"", line 457, in main
	    self._check_inbox()
	  File ""/home/dv/git/cointipbot/src/cointipbot.py"", line 346, in _check_inbox
	    m.mark_as_read()
	  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 313, in mark_as_read
	    return self.reddit_session.user.mark_as_read(self)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 663, in mark_as_read
	    retval = self.reddit_session._mark_as_read(ids, unread=unread)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/decorators.py"", line 262, in wrapped
	    return function(cls, *args, **kwargs)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 1721, in _mark_as_read
	    response = self.request_json(self.config[key], data=data)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/decorators.py"", line 95, in wrapped
	    return_value = function(reddit_session, *args, **kwargs)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 469, in request_json
	    response = self._request(url, params, data)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 343, in _request
	    _raise_response_exceptions(response)
	  File ""/usr/local/lib/python2.7/dist-packages/praw/internal.py"", line 182, in _raise_response_exceptions
	    response.raise_for_status()
	  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 689, in raise_for_status
	    raise HTTPError(http_error_msg, response=self)
	HTTPError: 502 Server Error: Bad Gateway

The code I have in place to catch it is:

             # Mark message as read
             while True:
                 try:
                     m.mark_as_read()
                     break
                 except urllib2.HTTPError, e:
                     if e.code in [429, 500, 502, 503, 504]:
                         lg.warning(""_check_inbox(): Reddit is down (error %s), sleeping..."", e.code)
                         time.sleep(60)
                         pass
                     else:
                         raise
                 except Exception, e:
                     lg.error(""_check_inbox(): couldn't mark message as read: %s"", str(e))
                     raise

Am I not catching the right HTTPError? From stack trace, I've found exception definition to be at __/usr/local/lib/python2.7/dist-packages/requests/exceptions.py__:

	class HTTPError(RequestException):
	    """"""An HTTP error occurred.""""""

	    def __init__(self, *args, **kwargs):
		"""""" Initializes HTTPError with optional `response` object. """"""
		self.response = kwargs.pop('response', None)
		super(HTTPError, self).__init__(*args, **kwargs)

No mention of urllib2 in that file. So, am I catching the wrong type of HTTPError?",,False,,t5_2qizd,False,,,True,t3_1exvqb,http://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/,
1369304067.0,2,stackoverflow.com,1ew8yl,Google App Engine Go - Reddit API returns error 429 - StackOverflow,2,0,0,http://www.reddit.com/r/redditdev/comments/1ew8yl/google_app_engine_go_reddit_api_returns_error_429/,,,False,,t5_2qizd,False,,,False,t3_1ew8yl,http://stackoverflow.com/questions/16711070/google-app-engine-go-reddit-api-returns-error-429,
1369291932.0,2,self.redditdev,1ew2gk,r.login failing,2,0,3,http://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/,"I'm making a bot, and whenever r.login runs, I get this error: 
raise SSLError(e)
requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available

My user_agent string doesn't contain 'bot', and it was working before the praw update.  Are there any packages I'm missing or something I should be doing?",,False,,t5_2qizd,False,,,True,t3_1ew2gk,http://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/,
1369259633.0,2,self.redditdev,1ev2ex,unexplained login errors using PRAW,2,0,2,http://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/,"I have been using praw to make a bot that makes comments.  I got everything to work, but 2 or 3 days ago there was a new update for praw. I installed this, and since then I have had many errors.  I login without getting errors, but at the point when it comments it tells me that I am not logged in.  I ran this many times in the shell using different accounts, user agents, and messages.  It always does the same thing.  Is this just a mistake, or is there something I need to do to fix this?",,False,,t5_2qizd,False,,,True,t3_1ev2ex,http://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/,
1369152234.0,2,self.redditdev,1erqio,Get submissions by date and subreddit with PRAW,2,0,1,http://www.reddit.com/r/redditdev/comments/1erqio/get_submissions_by_date_and_subreddit_with_praw/,"This seems like it would be pretty basic so there's probably an easy answer, but I couldn't find it on the PRAW website or in the help() docs. 

Basically, I want to get only links submitted to a particular subreddit within a time frame (and their scores, comments, etc.). I found a way to do it constructing the API calls without PRAW, but I'd like to know if this is possible (and easier) using PRAW. ",,False,,t5_2qizd,False,,,True,t3_1erqio,http://www.reddit.com/r/redditdev/comments/1erqio/get_submissions_by_date_and_subreddit_with_praw/,
1368466799.0,2,self.redditdev,1e98d5,A quick question about archive posts...,2,0,1,http://www.reddit.com/r/redditdev/comments/1e98d5/a_quick_question_about_archive_posts/,"So if you try to vote on a post from a few years ago you get the following response:

    sorry, this has been archived and can no longer be voted on

I was wondering how long is it before a submission is archived like this.

Also, is this done for by submission (with all child comments and replies also getting archived at this point) or is it done on a comment by comment basis (e.g. 6 months of inactivity on a comment)


Thanks for the help guys! :)",,False,,t5_2qizd,False,,,True,t3_1e98d5,http://www.reddit.com/r/redditdev/comments/1e98d5/a_quick_question_about_archive_posts/,
1367510266.0,2,self.redditdev,1dk2ag,"What ""features"" do you want to see in an api wrapper?",2,0,5,http://www.reddit.com/r/redditdev/comments/1dk2ag/what_features_do_you_want_to_see_in_an_api_wrapper/,"I'm writing a C# Api wrapper for Reddit and want to make it something people actually want to use, so I thought I would ask what features would you like to see in a wrapper if you were going to use it?

Would you prefer just a simple implementation of all the api methods? or some more complex features?

At the moment I had the idea of maybe making something to handle paging so the user only has to call back / next type methods and it wraps up the before / after parameters and handles them itself.

Also I was thinking of creating some kind of request manager so the 30 calls per minute rule is easy to enforce. Use of this could be optional though.

Any other ideas / suggestions would be welcome.

",,False,,t5_2qizd,False,,,True,t3_1dk2ag,http://www.reddit.com/r/redditdev/comments/1dk2ag/what_features_do_you_want_to_see_in_an_api_wrapper/,
1366472871.0,2,self.redditdev,1cque4,Newbie question: Is there any way to install the reddit source on a system with sysvinit and not upstart (e.g. Raspbian on a raspberry pi),3,1,0,http://www.reddit.com/r/redditdev/comments/1cque4/newbie_question_is_there_any_way_to_install_the/,,,False,,t5_2qizd,False,,,True,t3_1cque4,http://www.reddit.com/r/redditdev/comments/1cque4/newbie_question_is_there_any_way_to_install_the/,
1366283057.0,2,self.redditdev,1clkrz,Account activity now showing up on clone,5,3,12,http://www.reddit.com/r/redditdev/comments/1clkrz/account_activity_now_showing_up_on_clone/,"Hey,

So, as the title states, theres nothing in the account activity list. Even the
     You are currently accessing reddit from this IP address: 127.0.0.1

This shows regardless of where I am accessing site from. The clone is on the development server, so its not in VM or on my local machine.",,False,,t5_2qizd,False,,,True,t3_1clkrz,http://www.reddit.com/r/redditdev/comments/1clkrz/account_activity_now_showing_up_on_clone/,
1365825097.0,1,self.redditdev,1c931l,PRAW and comments.,4,3,7,http://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/,"I'm currently using PRAW to make a bot, but I don't want the bot to ever respond to itself, so before I the bot comments, I want to check if the comment it is replying to was posted by the bot's username. I have searched through the docs and have not found it in the source so im asking here. How do I get the username of a commenter from a comment object? Thanks",,False,,t5_2qizd,False,,,True,t3_1c931l,http://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/,
1365001313.0,2,self.redditdev,1bl9lh,I'm trying to grab a json feed of my front page and the top 6 comments from each story.. is there a batch way to get top comments from a bunch of stories or a way to include top comments with the stories in the json that is generated?,3,1,3,http://www.reddit.com/r/redditdev/comments/1bl9lh/im_trying_to_grab_a_json_feed_of_my_front_page/,..,,False,,t5_2qizd,False,,,True,t3_1bl9lh,http://www.reddit.com/r/redditdev/comments/1bl9lh/im_trying_to_grab_a_json_feed_of_my_front_page/,
1364897512.0,2,self.redditdev,1bi9la,I've created a bookmarklet for image-based subreddits. It replaces the regular page with a page of the linked images. Anyone want to take a look?,2,0,2,http://www.reddit.com/r/redditdev/comments/1bi9la/ive_created_a_bookmarklet_for_imagebased/,"Apologies if this isn't the right sub for this kind of thing.

It was just for my own use, but I figured other people might be interested. Tested on Firefox only and doesn't handle Imgur gallery links.

    javascript:(function(){if(typeof(after)!='string'){after=null}
    window.c=function(json){document.getElementsByTagName('body')[0].innerHTML='';for(i=0;i&lt;json['data']['children'].length;i++){s=json['data']['children'][i]['data']['url'];s=s.match(/imgur\.com\/([^\/]+)\/?$/)?s+'.jpg':s;p=document.createElement('img');p.width=256;p.src=s;document.getElementsByTagName('body')[0].appendChild(p);}
    after=json['data']['after'];if(typeof(first_page)!='string'){first_page='n';}
    if(after==null&amp;&amp;first_page=='n'){alert('Last page')}};var u=window.location.href.toString();u.match(/reddit\.com\/r\/([^\/]+)/);var r=RegExp.$1;var j='http://reddit.com/r/'+r+'/.json?jsonp=c';if(after!=''){j+='&amp;after='+after}
    var s=document.createElement('script');s.src=j;document.getElementsByTagName('head')[0].appendChild(s);})()",,False,,t5_2qizd,1364898192.0,,,True,t3_1bi9la,http://www.reddit.com/r/redditdev/comments/1bi9la/ive_created_a_bookmarklet_for_imagebased/,
1363953420.0,2,self.redditdev,1aslst,Quick Question: Which .ini variable changes the number of submissions a new user can make?,2,0,1,http://www.reddit.com/r/redditdev/comments/1aslst/quick_question_which_ini_variable_changes_the/,"Newly registered users are limited to the number of posts they can make. I'm wondering if that quota can be increased.

There is a variable QUOTA_THRESHOLD = 5. I'm not sure if that is what I'm looking for or not, or what that exactly means.",,False,,t5_2qizd,False,,,True,t3_1aslst,http://www.reddit.com/r/redditdev/comments/1aslst/quick_question_which_ini_variable_changes_the/,
1362432087.0,2,self.redditdev,19ntpk,make fails:  [build/public/static/reddit.css] Error 127,2,0,2,http://www.reddit.com/r/redditdev/comments/19ntpk/make_fails_buildpublicstaticredditcss_error_127/,"----

----
**Update: Solved by (ahem) Buttscicles.  New dependency, ""nodejs"" added since I first tried building reddit code.**
----

----

I can't get make to work properly.  I deleted the directory, re-clone-d the reddit code from github, and can get all the way to the ""make"" bit of the install guide, but that crashes (error at the bottom of the post).

Oddly enough, I can actually run the server, and it works.

Help, please?

-----------------

make error:

    $ make
    [+] including definitions from Makefile.py
    # see above
    rm -f build/public/static/sprite-reddit.png build/public/static/reddit.css
    python r2/lib/nymph.py build/public/static/css/reddit.less build/public/static/sprite-reddit.png &gt; build/public/static/reddit.less.tmp
    r2/lib/contrib/less.js/bin/lessc build/public/static/reddit.less.tmp &gt; build/public/static/reddit.css.tmp
    /usr/bin/env: node: No such file or directory
    make: *** [build/public/static/reddit.css] Error 127
",,False,,t5_2qizd,1362438539.0,,,True,t3_19ntpk,http://www.reddit.com/r/redditdev/comments/19ntpk/make_fails_buildpublicstaticredditcss_error_127/,
1362008749.0,2,self.redditdev,19czth,Configuration Guide After Running Install Script on Amazon EC2,2,0,8,http://www.reddit.com/r/redditdev/comments/19czth/configuration_guide_after_running_install_script/,"Looking through previous redditdev posts and the IRC logs, I noticed that many others are having/had the same problem. I'm wondering if those with the reddit source code up and running on Amazon Web Services could provide the configuration steps they took after using the Ubuntu 12.04 install script, as a future guide to all those looking to set up Reddit externally.

So far, I've:

1. Opened up port 80 on the EC2 instance's security settings

2. Changed the hostname reddit uses to the public ip Amazon has assigned to my instance. (Updated the run.ini domain and ran ""make ini"" from the directory)

I'm wondering if there are other steps necessary to run Reddit externally, like make changes to the haproxy configuration?

Additional steps after site is running:

1. Recreate Amazon S3 Storage

2. Set up IndexTank from source for search function",,False,,t5_2qizd,1362089297.0,,,True,t3_19czth,http://www.reddit.com/r/redditdev/comments/19czth/configuration_guide_after_running_install_script/,
1359851741.0,2,self.redditdev,17s4xh,Can we get an API endpoint for flair template lists?,2,0,1,http://www.reddit.com/r/redditdev/comments/17s4xh/can_we_get_an_api_endpoint_for_flair_template/,"One for user flair, one for link flair. Or one for both, I suppose.

I had to do [this](https://github.com/SirCmpwn/RedditSharp/commit/a4fbbc66b841c5143e5d489b16acc90f782b05aa) as a workaround and I don't like it.",,False,,t5_2qizd,1359852789.0,,,True,t3_17s4xh,http://www.reddit.com/r/redditdev/comments/17s4xh/can_we_get_an_api_endpoint_for_flair_template/,
1359340263.0,2,self.redditdev,17ek0g,"HTTP code 500 on accept_moderator_invite
",2,0,4,http://www.reddit.com/r/redditdev/comments/17ek0g/http_code_500_on_accept_moderator_invite/,"I'm submitting my userhash, with an open invitation to moderate a subreddit, to http://www.reddit.com/api/accept_moderator_invite, and getting a 500. It doesn't prompt for subreddit or anything; according to the reddit api docs the only required (or accepted) parameter is a userhash. Any ideas what I'm doing wrong?",,False,,t5_2qizd,False,,,True,t3_17ek0g,http://www.reddit.com/r/redditdev/comments/17ek0g/http_code_500_on_accept_moderator_invite/,
1358223210.0,2,self.redditdev,16lk45,Is there a way to check if my User-Agent has been banned?,2,0,2,http://www.reddit.com/r/redditdev/comments/16lk45/is_there_a_way_to_check_if_my_useragent_has_been/,"I've been doing a lot of testing with a jQuery based reddit client (lots of gets and posts) and I think my votes are getting rejected. I'm know I've violated the 1 request:2 second rule, but it only happens in bursts. So, is there a way to see if I've been blacklisted or something?",,False,,t5_2qizd,False,,,True,t3_16lk45,http://www.reddit.com/r/redditdev/comments/16lk45/is_there_a_way_to_check_if_my_useragent_has_been/,
1357912193.0,2,reddit.com,16dmro,r/RedditPower - A new subreddit for discussing what is possible with reddit and the reddit API.,4,2,4,http://www.reddit.com/r/redditdev/comments/16dmro/rredditpower_a_new_subreddit_for_discussing_what/,,,False,,t5_2qizd,False,,,False,t3_16dmro,http://www.reddit.com/r/RedditPower/comments/16daw7/welcome_to_rredditpower/,
1357840856.0,2,self.redditdev,16bpd7,Trying to get /api/login/ to work in JavaScript.,2,0,1,http://www.reddit.com/r/redditdev/comments/16bpd7/trying_to_get_apilogin_to_work_in_javascript/,"Hi,

i'm running out of attempts... just got this ***""you are doing that too much. try again in 27 minutes.""*** so I thought I would come here and ask.

______________

This is the request I'm sending

**URL:** 

http://www.reddit.com/api/login/

**Headers:**

User-Agent: ""Reddit test app""

**POST DATA:** 

user=USERNAME&amp;passwd=PASSWORD&amp;api_type=json

___________________

Putting the parameters in the URL instead... strangely works! I would like to avoid this though as it may not be safe to pass the password in the URL.

e.g. http://www.reddit.com/api/login/?user=USERNAME&amp;passwd=PASSWORD&amp;api_type=json

________________

Another question... how do I access /api/v1/me/ ?

Tried passing modhash as a GET/POST parameter, and also by passing ""uh"" as a Header. None of them work.
_______________________

Any ideas what I'm doing wrong? Thanks! :)",,False,,t5_2qizd,False,,,True,t3_16bpd7,http://www.reddit.com/r/redditdev/comments/16bpd7/trying_to_get_apilogin_to_work_in_javascript/,
1357598202.0,3,self.redditdev,1659bu,How does reddit navigate to the next page?,7,4,1,http://www.reddit.com/r/redditdev/comments/1659bu/how_does_reddit_navigate_to_the_next_page/,I'm currently working on a small app and I'm using the standard .xml output to read the posts froma subreddit. Now I was working on adding a next page function and I noticed that reddit allows you to add ?limit=AMOUNT and ?after= but how would I be able to receive the posts of a next page using limit and after?,,False,,t5_2qizd,False,,,True,t3_1659bu,http://www.reddit.com/r/redditdev/comments/1659bu/how_does_reddit_navigate_to_the_next_page/,
1356705010.0,2,self.redditdev,15kn5x,Does PRAW support the new wiki API?,3,1,4,http://www.reddit.com/r/redditdev/comments/15kn5x/does_praw_support_the_new_wiki_api/,"Haven't seen any references to it in the commit log, so I think no?",,False,,t5_2qizd,False,,,True,t3_15kn5x,http://www.reddit.com/r/redditdev/comments/15kn5x/does_praw_support_the_new_wiki_api/,
1356593424.0,2,self.redditdev,15id8l,"What exactly is the ""create application"" section of the preferences for?",2,0,6,http://www.reddit.com/r/redditdev/comments/15id8l/what_exactly_is_the_create_application_section_of/,"I've developed a Chrome plugin for Reddit, but am not sure if the ""create an app"" portion of the [preferences](https://ssl.reddit.com/prefs/apps/) is a requirement, for a listing someplace, or simply just so Reddit knows it's out there.

The form doesn't have any links to a description or usage. Anyone know what it's for exactly?",,False,,t5_2qizd,False,,,True,t3_15id8l,http://www.reddit.com/r/redditdev/comments/15id8l/what_exactly_is_the_create_application_section_of/,
1355030240.0,2,self.redditdev,14j9wi,Are post titles/comment text filtered?,4,2,3,http://www.reddit.com/r/redditdev/comments/14j9wi/are_post_titlescomment_text_filtered/,"Say someone puts scripts/code in their post title or comment text, e.g. `&lt;script&gt;alert("""");&lt;/script&gt;` or `--DROP TABLES *`, and I scrape that post/comment text, will I get the raw script or the filtered version (e.g. symbols turned into the HTML symbol equivalent)?",,False,,t5_2qizd,False,,,True,t3_14j9wi,http://www.reddit.com/r/redditdev/comments/14j9wi/are_post_titlescomment_text_filtered/,
1354646089.0,2,self.redditdev,149uhs,Memory Usage for small reddit implementation,2,0,2,http://www.reddit.com/r/redditdev/comments/149uhs/memory_usage_for_small_reddit_implementation/,"I'm trying to install an implementation of reddit on a VPS with 1128MB of memory and I'm running in to issues on the install when Cassandra starts the JVM. I've decreased the maximum heap-size to the point where the JVM will start, but I'm running in to memory related issues further in the install (the VPS that I am running on is Ubuntu, so I'm just using the install script).

The site will be used for internal communications of a small team (~20), so there won't be a tremendous amount of activity at any one time.

Does anybody have any experience implementing reddit at this scale? Any suggestions on where I can scale my memory allocations?",,False,,t5_2qizd,False,,,True,t3_149uhs,http://www.reddit.com/r/redditdev/comments/149uhs/memory_usage_for_small_reddit_implementation/,
1351025745.0,2,self.redditdev,11ytjs,"Is there a way to get the ""hotness"" or normalization factor for reddits?",2,0,5,http://www.reddit.com/r/redditdev/comments/11ytjs/is_there_a_way_to_get_the_hotness_or/,"Reddit uses somethig called normalized hotness to mix content from subreddits with very various levels of activity into the home page.

Is the normalization factor available somewhere? A json API would be awesome :)",,False,,t5_2qizd,False,,,True,t3_11ytjs,http://www.reddit.com/r/redditdev/comments/11ytjs/is_there_a_way_to_get_the_hotness_or/,
1350915093.0,0,self.redditdev,11w97m,503 Error,4,4,4,http://www.reddit.com/r/redditdev/comments/11w97m/503_error/,"Im getting a 503 error in my browser  when I think My reddit starts up.  Where can I find the log(s)  that show any startup errors?  What should I be looking for to resolve this?
",,False,,t5_2qizd,False,,,True,t3_11w97m,http://www.reddit.com/r/redditdev/comments/11w97m/503_error/,
1349811162.0,2,self.redditdev,117lc3,API to reply to modmail?,13,11,1,http://www.reddit.com/r/redditdev/comments/117lc3/api_to_reply_to_modmail/,Is there an API I can use to reply to modmail? I presume supply it an id and a message?,,False,,t5_2qizd,False,,,True,t3_117lc3,http://www.reddit.com/r/redditdev/comments/117lc3/api_to_reply_to_modmail/,
1349630077.0,2,self.redditdev,113dxo,"If I convert reddit json pages to an array with php I get NULL for 'message/inbox' while all other pages work fine. 
",3,1,11,http://www.reddit.com/r/redditdev/comments/113dxo/if_i_convert_reddit_json_pages_to_an_array_with/,"I am currently working on an php script to access the json formatted information that is on some pages. 
The code for that script can be found here: https://github.com/creesch/reddit-api-frankenstein-wrapper/blob/master/reddit.php

Since I'd like to use the information in an array I used 

    json_decode($reddit_json, TRUE);

To convert it to an associative array and for most pages this seems to work just fine. However some results give me NULL as result, 'message/inbox' is a good example. At first I thought I might had requested the wrong adress but when I try it with 

    json_decode($reddit_json);

I do get the right information but in an object. 

Since receiving the data didn't seem to be the problem I had a look at the data itself with json_last_error() and this gave me a clue. The result came back as JSON_ERROR_SYNTAX. So I decided to again check the information, this time I didn't change anything about the data I got I just wrote it to a text file with the intention to get it through a validator. But when I do that I get the following 

    &lt;html&gt;
      &lt;head&gt;&lt;title&gt;Found&lt;/title&gt;&lt;/head&gt;
      &lt;body&gt;
        &lt;h1&gt;Found&lt;/h1&gt;
        &lt;p&gt;The resource was found at &lt;a href=""http://www.reddit.com/login.json?dest=%2Fmessage%2Finbox.json""&gt;http://www.reddit.com/login.json?dest=%2Fmessage%2Finbox.json&lt;/a&gt;;
    you should be redirected automatically.

    &lt;!--  --&gt;&lt;/p&gt;
        &lt;hr noshade&gt;
        &lt;div align=""right""&gt;WSGI Server&lt;/div&gt;
      &lt;/body&gt;
    &lt;/html&gt;

And that is where I get lost, when I decode the json to an object I get all the information, when I try to decode it to an array I get NULL and when I do nothing with it I get send this html code. 
If I following the link I do indeed get to see the json formated information but when I try to use the url given in my script it does not return anything. 

So basically: **Why am I able to get the information if I decode it to a object, but do I seem to get a redirect page if I try to decode it to a array?**

edit: 
I have found that my problem is not limited to that page. If I try to do the same on moderator related pages I actually get an array containing a 404 error, if I convert to an object it works just fine. 


",,False,,t5_2qizd,1349641135.0,,,True,t3_113dxo,http://www.reddit.com/r/redditdev/comments/113dxo/if_i_convert_reddit_json_pages_to_an_array_with/,
1349494862.0,2,self.redditdev,1110oc,Bulk Requests for User Info,2,0,5,http://www.reddit.com/r/redditdev/comments/1110oc/bulk_requests_for_user_info/,"Does anyone have any strategies on implementing bulk requests for user information? Is it possible? Ideally I would like to pass in ... say 10-25 usernames and get back json 

For instance, [take my info](http://www.reddit.com/user/binaryechoes/about/.json)
but for more than just one user. 

http://www.reddit.com/user/usernameA+usernameB+usernameC/about/.json doesn't work but that may help illustrate what I'm after.",,False,,t5_2qizd,1349495172.0,,,True,t3_1110oc,http://www.reddit.com/r/redditdev/comments/1110oc/bulk_requests_for_user_info/,
1349211486.0,1,self.redditdev,10u4ud,What license are reddit posts/comments available under?,6,5,9,http://www.reddit.com/r/redditdev/comments/10u4ud/what_license_are_reddit_postscomments_available/,"I understand the rules about the API when it comes to queries/sec and the like, but can anyone help me understand under what terms are we allowed to use the posts and comments?
",,False,,t5_2qizd,False,,,True,t3_10u4ud,http://www.reddit.com/r/redditdev/comments/10u4ud/what_license_are_reddit_postscomments_available/,
1348933300.0,2,self.redditdev,10o4iz,Fetching imgur album best practice,3,1,5,http://www.reddit.com/r/redditdev/comments/10o4iz/fetching_imgur_album_best_practice/,"Hi Devs - I have created a [metro app for Windows 8](http://apps.microsoft.com/webpdp/en-US/app/reddit-pictures/c4647c77-20ae-4754-bd2b-0d2b66527fe2) that can parse and show pictures incrementally from various sub-reddits. It all works as expected other than imgur albums.

Since imgur do not have a standard convention to parse all the image links for a given album (see for example http://imgur.com/a/XTVbp) is there is any other alternative to access the images other than by parsing the HTML?",,False,,t5_2qizd,False,,,True,t3_10o4iz,http://www.reddit.com/r/redditdev/comments/10o4iz/fetching_imgur_album_best_practice/,
1348889881.0,2,self.redditdev,10njpx,Help with oauth2 access_token,2,0,6,http://www.reddit.com/r/redditdev/comments/10njpx/help_with_oauth2_access_token/,"I have tried and tried and tried... I am trying to setup grails to use it.  I can't find any examples of anyone using grails with it.


Where i am having great difficulty is with the access_token.

I did get it to work with the rauth python example, but I cannot for the life of me figure out what I have to do to custom code it.

I am working on adding reddit to this oauth java lib https://github.com/fernandezpablo85/scribe-java.

Anyways I can't get it to work.. so I figure I would try just getting the url working manually as a good first step then I can just monitor what the code is generating and compare it to something I know works.  Problem is I can't get it to work outside of the app either.

So I guess the first question is to make sure the request can be sent as a get.  I do have the url form to get an access code down pat.

This is the URL I have built:
&gt;https://ssl.reddit.com/api/v1/access_token?code=vbla6zKhckQDRVpwSIBRh7EitHU&amp;client_id=MY_CLIENT_ID&amp;redirect_uri=http%3A%2F%2Flocalhost%3A8080%2FRedditApi%2FbuildApi%2FtestReceive&amp;client_secret=MY_CLIENT_SECRET&amp;grant_type=authorization_code

Anyways if someone could give me a url format that would actually work I would greatly appreciate it.  I have been banging my head against this for hours... I suspect the answer is really simple, but it has eluded me thus far.  I have seen some things that suggest I might need to do:
&gt;https://CLIENT_ID:CLIENT_SECRET@ssl.reddit.com/api/v1/access_token

But that hasn't worked with me trying it along with the various parameters from the other url.


Also just in case someone else gets stuck on generating the authorize link this url is working for me:  https://ssl.reddit.com/api/v1/authorize?client_id=CLIENT_ID&amp;redirect_uri=http%3A%2F%2Flocalhost%3A8080%2FRedditApi%2FbuildApi%2FtestReceive&amp;response_type=code&amp;scope=identity&amp;state=aaaabbbb",,False,,t5_2qizd,1348890101.0,,,True,t3_10njpx,http://www.reddit.com/r/redditdev/comments/10njpx/help_with_oauth2_access_token/,
1348859385.0,2,self.redditdev,10msc8,How to calculate more comments count (Not just direct children),4,2,12,http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/,"I am using, the comments api (/comments/id/.json) and when I get a 'more' thing, I'd like to display the number of more objects remaining.

The reddit.com site does this, but I can't figure out how to do it myself with one call. The more object has the notion of 'children', and I could count the number of children, but the issue is that is direct decedents. 

So I could have 'one' child, but that child can have 10 replies, and the real number I want is 11.

Am I overlooking something? How can this be done?",,False,,t5_2qizd,False,,,True,t3_10msc8,http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/,
1348848547.0,2,self.redditdev,10mgqc,Snudown info/docs/examples,2,0,8,http://www.reddit.com/r/redditdev/comments/10mgqc/snudown_infodocsexamples/,"I'm having trouble locating info, docs, examples, etc for Snudown. Does anyone have any insight?

Thanks",,False,,t5_2qizd,False,,,True,t3_10mgqc,http://www.reddit.com/r/redditdev/comments/10mgqc/snudown_infodocsexamples/,
1348708476.0,2,self.redditdev,10jf9z,Why FRONT-ALL-RANDOM-MOD buttons are not translated?,3,1,0,http://www.reddit.com/r/redditdev/comments/10jf9z/why_frontallrandommod_buttons_are_not_translated/,"I think FRONT-ALL-RANDOM-MOD buttons are already translated in reddit-i18n. However translation are not applied as you can try. 
why is that?",,False,,t5_2qizd,False,,,True,t3_10jf9z,http://www.reddit.com/r/redditdev/comments/10jf9z/why_frontallrandommod_buttons_are_not_translated/,
1347250302.0,2,self.redditdev,zmyom,"[fixed] I encounter this error, any help..?",2,0,1,http://www.reddit.com/r/redditdev/comments/zmyom/fixed_i_encounter_this_error_any_help/,"File '/usr/lib/pymodules/python2.7/paste/exceptions/errormiddleware.py', line 144 in __call__
  app_iter = self.application(environ, sr_checker)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 145 in __call__
  return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 250 in __call__
  return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 208 in __call__
  return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 223 in __call__
  return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 333 in __call__
  return self.app(environ, start_response)
File '/home/reddit/reddit/r2/r2/config/middleware.py', line 359 in __call__
  return self.app(environ, custom_start_response)
File '/usr/lib/pymodules/python2.7/pylons/wsgiapp.py', line 314 in __call__
  return self.app(environ, start_response)
File '/usr/lib/pymodules/python2.7/beaker/middleware.py', line 73 in __call__
  return self.app(environ, start_response)
File '/usr/lib/pymodules/python2.7/beaker/middleware.py', line 152 in __call__
  return self.wrap_app(environ, session_start_response)
File '/usr/lib/python2.7/dist-packages/routes/middleware.py', line 99 in __call__
  response = self.app(environ, start_response)
File '/usr/lib/pymodules/python2.7/pylons/wsgiapp.py', line 95 in __call__
  response = self.dispatch(controller, environ, start_response)
File '/usr/lib/pymodules/python2.7/pylons/wsgiapp.py', line 236 in dispatch
  return controller(environ, start_response)
File '/home/reddit/reddit/r2/r2/lib/base.py', line 146 in __call__
  res = WSGIController.__call__(self, environ, start_response)
File '/usr/lib/pymodules/python2.7/pylons/controllers/core.py', line 164 in __call__
  response = self._dispatch_call()
File '/usr/lib/pymodules/python2.7/pylons/controllers/core.py', line 120 in _dispatch_call
  response = self._inspect_call(func)
File '/usr/lib/pymodules/python2.7/pylons/controllers/core.py', line 79 in _inspect_call
  result = func(**args)
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 393 in GET_listing
  return ListingController.GET_listing(self, **env)
File '/home/reddit/reddit/r2/r2/controllers/validator/validator.py', line 170 in newfn
  return fn(self, *a, **kw)
File '/home/reddit/reddit/r2/r2/controllers/reddit_base.py', line 501 in new_fn
  return fn(self, **kw)
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 189 in GET_listing
  return self.build_listing(**env)
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 107 in build_listing
  content = self.content()
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 381 in content
  spotlight = self.spotlight()
File '/home/reddit/reddit/r2/r2/controllers/listingcontroller.py', line 329 in spotlight
  promo_visible = promote.is_promo(s.lookup[vislink])
KeyError: 't9_5'


My clone was working fine, suddenly I got this kind of error. How can I solve it? Please help",,False,,t5_2qizd,False,,,True,t3_zmyom,http://www.reddit.com/r/redditdev/comments/zmyom/fixed_i_encounter_this_error_any_help/,
1346634252.0,2,self.redditdev,z98ru,How do i change the default user preferences?,3,1,2,http://www.reddit.com/r/redditdev/comments/z98ru/how_do_i_change_the_default_user_preferences/,"Specifically, I'd like to make the JS libraries load off my server instead of Google's by default (it's a rather privacy conscious and Google hating community). I know I can edit a python file (js.py or some such), but that messes with git and, unless I fork it and such, I can't pull the latest version without unediting it. Is there a way to do this without messing with git?

thanks",,False,,t5_2qizd,False,,,True,t3_z98ru,http://www.reddit.com/r/redditdev/comments/z98ru/how_do_i_change_the_default_user_preferences/,
1345674771.0,2,self.redditdev,ynvf0,Batch requests for things by id,2,0,0,http://www.reddit.com/r/redditdev/comments/ynvf0/batch_requests_for_things_by_id/,"Is there a way to look up multiple posts by id?  Let's say that I fetch a listing and immediately display the fetched item to the user.  Now whenever the user enters a page full of items, I'd like to fetch some metadata for all the items (e.g. number of comments, whether they've voted for the item, score for the item), but don't want to perform fetches for each of the items individually because the client would probably get rate limited.  Maybe I'm missing something, but does this kind of functionality exist?  It would be super helpful and would save developers a lot of hassle.

EDIT: nvm i figured it out.  This isn't documented, but if you pass multiple comma-separated full names into the by_id endpoint, you'll get results for each name. ",,False,,t5_2qizd,1345676007.0,,,True,t3_ynvf0,http://www.reddit.com/r/redditdev/comments/ynvf0/batch_requests_for_things_by_id/,
1345337294.0,2,self.redditdev,yg9yj,I want to read the numbers of users logged in.,3,1,2,http://www.reddit.com/r/redditdev/comments/yg9yj/i_want_to_read_the_numbers_of_users_logged_in/,How can I get there data of the users online of distinct subredddits. ,,False,,t5_2qizd,False,,,True,t3_yg9yj,http://www.reddit.com/r/redditdev/comments/yg9yj/i_want_to_read_the_numbers_of_users_logged_in/,
1345070965.0,2,self.redditdev,yad5u,Please HELP Remote DB issues,3,1,0,http://www.reddit.com/r/redditdev/comments/yad5u/please_help_remote_db_issues/,"Hi can someone help me figure out what to change in settings to have a remote DB. I backedup local database from Devserver and put it on the main DB server. I installed reddit on 2 different servers based off my GitHub Repo I made of the reddit install on a DevServer. Please help in trying to change or install things on bothe DB server and the two reddit servers to make it work. I cant figure out what settings need to be changed. 
*Note* I can psql into DB servers psql DB. I just cant get reddit site to load DB for whatever reason. I am trying to host online and it gives 503 error.",,False,,t5_2qizd,False,,,True,t3_yad5u,http://www.reddit.com/r/redditdev/comments/yad5u/please_help_remote_db_issues/,
1342035479.0,2,self.redditdev,wefso,"""More"" comment gives incorrect ids",3,1,3,http://www.reddit.com/r/redditdev/comments/wefso/more_comment_gives_incorrect_ids/,"Hi, I'm trying to implement loading more comments, but there seems to be an issue with the values being returned in the json from the API.  When I'm trying to load more comments using http://www.reddit.com/comments/[story id]/_/[comment id].json, the entire comment tree is returned if [comment id] is incorrect or not found.  Moreover, the http response is always 200 and the resulting json gives no indication (that I can find, at least) that the id was incorrect.  Is anyone else having this problem?  Apart from checking the ids of the returned comments against ones we've already loaded, is there any way to tell when the .json that's returned is actually returning new ids?",,False,,t5_2qizd,False,,,True,t3_wefso,http://www.reddit.com/r/redditdev/comments/wefso/more_comment_gives_incorrect_ids/,
1339754663.0,2,self.redditdev,v37yn,Getting a 403 Forbidden HTTP Error when I click the +/- expand/collapse button on video posts,4,2,0,http://www.reddit.com/r/redditdev/comments/v37yn/getting_a_403_forbidden_http_error_when_i_click/,"I'm getting the following error when I click the +/- expand button on video posts. Here's an example of [one such video post on my (incomplete) reddit site](http://fiamthu.com/r/Rimawi/comments/m/mad_season_river_of_deceit/). 

Following is the error you'll get when you click the expand/collapse button for the video.

&gt;403 Forbidden
Code: AccessDenied
Message: Access Denied 
RequestId: D65E014C079D1D84
HostId: WgQqHF4Dp9tdUHR6TFIgmETf6NnnSeVoRLA30zDqigbjf2JNHoFSnFAdpaTi90uM

Sometimes, it shows the following error.

&gt;Server not found
Firefox can't find the server at thumbs.mysite.com.s3-website-ap-southeast-1.amazonaws.com.  

Has this got to do with some incomplete configuration at amazonaws or the VPS (Linode)?",,False,,t5_2qizd,False,,,True,t3_v37yn,http://www.reddit.com/r/redditdev/comments/v37yn/getting_a_403_forbidden_http_error_when_i_click/,
1337334495.0,2,self.redditdev,tt2h3,Edit posts link flair,2,0,0,http://www.reddit.com/r/redditdev/comments/tt2h3/edit_posts_link_flair/,"I am trying to get the bot on r/RedditDayOf to automatically set each posts link flair with the daily topic. How would I achieve this programatically?

Thanks in advance for any help.",,False,,t5_2qizd,False,,,True,t3_tt2h3,http://www.reddit.com/r/redditdev/comments/tt2h3/edit_posts_link_flair/,
1336487113.0,2,self.redditdev,tcyew,jQuery examples?,3,1,0,http://www.reddit.com/r/redditdev/comments/tcyew/jquery_examples/,"I recently added a feature to [moderate all comments](http://www.reddit.com/r/redditdev/comments/t21bh/moderate_all_comments/), however it does not notify the user that they are being moderated.  I noticed that in *POST_comment()* in r2/controller/api.py, and many other functions for that matter, there is a jquery object which is used to display the comment without updating the page.  Is there a way to use this to show something like an alert that their comment is pending moderator approval?  How exactly does that jQuery interface work?  I greped for other uses, and tried a couple things, but couldn't get it to work :/.",,False,,t5_2qizd,False,,,True,t3_tcyew,http://www.reddit.com/r/redditdev/comments/tcyew/jquery_examples/,
1335034509.0,2,self.redditdev,slkmc,Trying to edit comments using python reddit_api... is this even possible?,6,4,4,http://www.reddit.com/r/redditdev/comments/slkmc/trying_to_edit_comments_using_python_reddit_api/,"I'm trying to modify a simple reddit bot I built to automatically update periodically with a countdown timer, but I can't figure out how to edit posts. 

I can't find reddit_api's method for editing comments. Is this possible with reddit_api, is it not possible at all, or am I just missing something?

Thanks for any help.",,False,,t5_2qizd,False,,,True,t3_slkmc,http://www.reddit.com/r/redditdev/comments/slkmc/trying_to_edit_comments_using_python_reddit_api/,
1334523238.0,2,self.redditdev,sb9ss,Monitoring new comments,5,3,4,http://www.reddit.com/r/redditdev/comments/sb9ss/monitoring_new_comments/,"Hey folks.

So I'm trying to monitor new comments to look for something specific in them. Right now I'm querying /r/all/comments but I'm only getting 25 comments every time. I started hitting it every 5 seconds, but noticed that the results were the same for about a minute. I remembered reading about the caching, so I set it to 1 minute, and I would still get similar results.

So the best I can do is to check 25 comments every two minutes. That's really not a lot of coverage. Is there any better way to be able to monitor as many comments as possible?",,False,,t5_2qizd,False,,,True,t3_sb9ss,http://www.reddit.com/r/redditdev/comments/sb9ss/monitoring_new_comments/,
1334063019.0,2,self.redditdev,s2guv,video expand button didn't show up,3,1,2,http://www.reddit.com/r/redditdev/comments/s2guv/video_expand_button_didnt_show_up/,"I run a clone of reddit in ubuntu server 11.04, everthing is ok until I found that the expand button didn't show up  when I submitted a video link, then I submitted the exactly link to reddit.com,  the expand button show up right under the title beyond doubt.
  I also did some debug work,it seems like that the expand button has something to do with 

/home/reddit/reddit/r2/data/templates/link.html.py

        if thing.link_child and not thing.link_child.expand and not selftext_hide:
            # SOURCE LINE 107
            __M_writer(u'    &lt;div class=""expando-button collapsed\n                ')


when I debugging , the   thing.link_child  is always None. 



",,False,,t5_2qizd,False,,,True,t3_s2guv,http://www.reddit.com/r/redditdev/comments/s2guv/video_expand_button_didnt_show_up/,
1331755311.0,2,self.redditdev,qwn6e,API calls to access a user's trophy case?,3,1,0,http://www.reddit.com/r/redditdev/comments/qwn6e/api_calls_to_access_a_users_trophy_case/,"I've looked through the API documentation, and then through the API source - but I can't seem to find any method calls to get a user's trophies. The most I could find under the API source was methods to give and remove trophies.

I know it is possible, as I've seen at least the android BaconReader app able to display them - does anyone know how this might be done? Thanks for any help!",,False,,t5_2qizd,False,,,True,t3_qwn6e,http://www.reddit.com/r/redditdev/comments/qwn6e/api_calls_to_access_a_users_trophy_case/,
1330821613.0,2,self.redditdev,qgjti,How to get the json of pages?,2,0,0,http://www.reddit.com/r/redditdev/comments/qgjti/how_to_get_the_json_of_pages/,"I can go to http://www.reddit.com/.json and get the json for the home page.  But how does one do it for particular pages?

The next page of reddit for me shows:

http://www.reddit.com/?count=25&amp;after=t3_qfu4u

I tried:

http://www.reddit.com/?count=25&amp;after=t3_qfu4u/.json but the json is not returned.

Any help would be greatly appreciated.",,False,,t5_2qizd,False,,,True,t3_qgjti,http://www.reddit.com/r/redditdev/comments/qgjti/how_to_get_the_json_of_pages/,
1328583787.0,2,self.redditdev,pe3na,install error: no module named `script.appinstall`,3,1,0,http://www.reddit.com/r/redditdev/comments/pe3na/install_error_no_module_named_scriptappinstall/,"What's wrong here? `paste` is installed correctly... And yet it doesn't work. Thoughts? There are more errors like this, but Reddit won't install write without `script.appinstall`.

`Traceback (most recent call last):`
`  File ""r2/lib/js.py"", line 8, in &lt;module&gt;`

    from r2.lib.translation import iter_langs
`  File ""/home/reddit/reddit/r2/r2/lib/translation.py"", line 24, in &lt;module&gt;`

    import pylons
`  File ""/usr/lib/pymodules/python2.7/pylons/__init__.py"", line 4, in &lt;module&gt;`

    from pylons.config import config
`  File ""/usr/lib/pymodules/python2.7/pylons/config.py"", line 2, in &lt;module&gt;`

    from pylons.configuration import *
`  File ""/usr/lib/pymodules/python2.7/pylons/configuration.py"", line 16, in &lt;module&gt;`

    import pylons.legacy
`  File ""/usr/lib/pymodules/python2.7/pylons/legacy.py"", line 12, in &lt;module&gt;`

    from pylons.util import deprecated, func_move
`  File ""/usr/lib/pymodules/python2.7/pylons/util.py"", line 10, in &lt;module&gt;`

    from paste.script.appinstall import Installer
`ImportError: No module named script.appinstall`",,False,,t5_2qizd,False,,,True,t3_pe3na,http://www.reddit.com/r/redditdev/comments/pe3na/install_error_no_module_named_scriptappinstall/,
1328477415.0,2,self.redditdev,pca3r,Problem with selecting a subreddit while submitting a link using submit.compact,3,1,0,http://www.reddit.com/r/redditdev/comments/pca3r/problem_with_selecting_a_subreddit_while/,"For example, when I go to this URL, http://www.reddit.com/r/fffffffuuuuuuuuuuuu/submit.compact?url=http://imgur.com/blah, the reddit field in the form used to be populated with fffffffuuuuuuuuuuuu.

Now it gets populated with &lt;Subreddit 4606764&gt;. If you try to submit the form, it says ""that subreddit doesn't exist"".

Why is this happening?",,False,,t5_2qizd,False,,,True,t3_pca3r,http://www.reddit.com/r/redditdev/comments/pca3r/problem_with_selecting_a_subreddit_while/,
1327453533.0,2,reddit.com,ovbel,"What are the requirements for running a Reddit clone? Can Webfaction do it? If not, what is a low-cost alternative? [X-post from r/technology]",3,1,1,http://www.reddit.com/r/redditdev/comments/ovbel/what_are_the_requirements_for_running_a_reddit/,,,False,,t5_2qizd,False,,,False,t3_ovbel,http://www.reddit.com/r/technology/comments/otoqs/what_are_the_requirements_for_running_a_reddit/,
1326210716.0,2,self.redditdev,oax4s,"Installed reddit using the install script, yet server outputs a Pylons error",2,0,3,http://www.reddit.com/r/redditdev/comments/oax4s/installed_reddit_using_the_install_script_yet/,"Hello! I'm trying to learn how reddit works, so I decided to fire up 11.04 on a VM of mine and install reddit on it to mess around. The install script seemed to have worked fine, but apparently something went wrong since I'm getting [this weird error](http://pastebin.com/QvKJUwkY).

Notice the lack of &lt;html&gt; and &lt;head&gt; tags up top: it seems the page was cut off. By adding those tags, I was able to see the error page without all the styling code, and here's what I got.

&gt;Error Traceback
&gt;
&gt;URL: http://192.168.7.127:8000/
Module pylons.error:245 in respond         
&gt;&gt;  app_iter = self.application(environ, detect_start_response)
Module r2.config.middleware:313 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:409 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:367 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:382 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:118 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:118 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:118 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:457 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module r2.config.middleware:483 in __call__         
&gt;&gt;  return self.app(environ, custom_start_response)
Module pylons.wsgiapp:314 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module beaker.middleware:73 in __call__         
&gt;&gt;  return self.app(environ, start_response)
Module beaker.middleware:152 in __call__         
&gt;&gt;  return self.wrap_app(environ, session_start_response)
Module routes.middleware:99 in __call__         
&gt;&gt;  response = self.app(environ, start_response)
Module pylons.wsgiapp:94 in __call__         
&gt;&gt;  controller = self.resolve(environ, start_response)
Module pylons.wsgiapp:170 in resolve         
&gt;&gt;  return self.find_controller(controller)
Module r2.config.middleware:494 in find_controller         
&gt;&gt;  __import__(self.package_name + '.controllers')
Module ?:22 in &lt;module&gt;         
&gt;&gt;  from listingcontroller import ListingController
Module ?:22 in &lt;module&gt;         
&gt;&gt;  from reddit_base import RedditController, base_listing, organic_pos
Module ?:28 in &lt;module&gt;         
&gt;&gt;  from r2.lib import pages, utils, filters, amqp, stats
Module ?:22 in &lt;module&gt;         
&gt;&gt;  from pages import *
Module ?:52 in &lt;module&gt;         
&gt;&gt;  from r2.lib.subreddit_search import popular_searches
Module ?:8 in &lt;module&gt;         
&gt;&gt;  class SubredditsByPartialName(tdb_cassandra.View):
Module r2.lib.db.tdb_cassandra:115 in __init__         
&gt;&gt;  raise InvariantException(""Redefining type %r?"" % (cls._type_prefix))
&lt;class 'r2.lib.db.tdb_cassandra.InvariantException'&gt;: Redefining type 'SubredditsByPartialName'?

I didn't see the error in the FAQ, and I don't really know how to solve it. Should I start over from scratch and hope for the best, or is there something I can do to salvage my installation?

Thank you for your time. =)",,False,,t5_2qizd,False,,,True,t3_oax4s,http://www.reddit.com/r/redditdev/comments/oax4s/installed_reddit_using_the_install_script_yet/,
1323755386.0,2,self.redditdev,nao4c,Getting a redirect (302) instead of 200 for login.,3,1,2,http://www.reddit.com/r/redditdev/comments/nao4c/getting_a_redirect_302_instead_of_200_for_login/,"I'm using the unsecured login API as specified (https://github.com/reddit/reddit/wiki/API%3A-login), but I am not getting a 200 and a JSON string in the response. Instead I'm getting a redirect (see below) Anyone know why? Thanks. 

    POST /api/login/schemingpanda1 HTTP/1.1

    User-Agent: curl/7.19.7 (i486-pc-linux-gnu) libcurl/7.19.7 OpenSSL/0.9.8k zlib/1.2.3.3 libidn/1.15

    Host: reddit.com

    Accept: */*

    Content-Length: 50

    Content-Type: application/x-www-form-urlencoded



    api_type=json&amp;user=schemingpanda1&amp;passwd=*********
    HTTP/1.1 302 Moved Temporarily

    Server: AkamaiGHost

    Content-Length: 0

    Location: http://www.reddit.com/api/login/schemingpanda1

    Date: Tue, 13 Dec 2011 05:42:37 GMT

    Connection: keep-alive",,False,,t5_2qizd,False,,,True,t3_nao4c,http://www.reddit.com/r/redditdev/comments/nao4c/getting_a_redirect_302_instead_of_200_for_login/,
1323419909.0,2,self.redditdev,n624n,Submission ids question,2,0,7,http://www.reddit.com/r/redditdev/comments/n624n/submission_ids_question/,"I've noticed that submission IDs for the most part are sequential base36 numbers. For instance this one is n624n. I'm trying to approximate the number of submissions made to reddit by time and I noticed that there are some inconsistencies in the submission IDs. As I don't have access to the database, can an admin confirm I have correctly identified the inconsistencies?

 * 2005-06-23 11:43:53 through 2006-01-17 23:49:23
    * IDs [87](http://www.reddit.com/comments/87/) through [28128](http://www.reddit.com/comments/87/) and grow in a base10 manor
 * 2006-01-18 01:00:41 through 2007-10-14 01:43:26
    * IDs [sqh](http://www.reddit.com/comments/sqh/) through [2zyj4](http://www.reddit.com/comments/2zyj4/) and use only the base36 numbers that have at least 1 a-z character
 * 2006-01-24 10:10:22 through 2007-07-25 04:38:09 (WTF section)
    * This section is anomalous as the IDs are base36 but in reverse chronological order from [5xowi](http://www.reddit.com/comments/5xowi/) to [5yba0](http://www.reddit.com/comments/5yba0/).
 * 2007-10-15 01:16:02 through the present
    * Starting at [5yba1](http://www.reddit.com/comments/5yba1/) and continuing to grow as base36 numbers.

Going backwards I see that [5yba1](http://www.reddit.com/comments/5yba1/) is a post jedberg made about the new comment system on beta.reddit.com. Can an admin explain the anomolous section? Also what prompted the switch to base36 numbers in the first place? I'm guessing to keep the urls short?

This brings up another question- does that mean when the base36 system was put into place, all the old ids had to be updated in the database to their base10 equivalent of the base36 number? For instance where the first post (id 87) would have been key 87 in the database, it would have to be updated to key 295?

Finally is this an appropriate approximation? Each million submissions (including doubles and spam) since the new comment system has been in place occurs at the following times:

|Submission|Date|Time|
:--|:--|:--
http://www.reddit.com/comments/5yba1/|2007-10-15|01:16:02
http://www.reddit.com/comments/6jqvt/|2008-05-17|02:15:58
http://www.reddit.com/comments/756hl/|2008-10-03|22:04:45
http://www.reddit.com/comments/7qm3d/|2009-01-18|07:18:51
http://www.reddit.com/comments/8c1p5/|2009-04-13|01:14:30
http://www.reddit.com/comments/8xhax/|2009-07-01|19:35:20
http://www.reddit.com/comments/9iwwp/|2009-09-09|11:41:53
http://www.reddit.com/comments/a4cih/|2009-11-14|05:33:58
http://www.reddit.com/comments/aps49/|2010-01-14|16:55:41
http://www.reddit.com/comments/bb7q1/|2010-03-09|09:19:30
http://www.reddit.com/comments/bwnbt/|2010-04-27|02:12:57
http://www.reddit.com/comments/ci2xl/|2010-06-23|03:11:42
http://www.reddit.com/comments/d3ijd/|2010-08-20|10:08:21
http://www.reddit.com/comments/doy55/|2010-10-09|04:38:22
http://www.reddit.com/comments/eadqx/|2010-11-22|22:41:11
http://www.reddit.com/comments/evtcp/|2011-01-03|20:45:53
http://www.reddit.com/comments/fh8yh/|2011-02-07|19:08:19
http://www.reddit.com/comments/g2ok9/|2011-03-12|10:20:57
http://www.reddit.com/comments/go461/|2011-04-12|03:26:36
http://www.reddit.com/comments/h9jrt/|2011-05-11|22:29:39
http://www.reddit.com/comments/huzdl/|2011-06-08|14:52:07
http://www.reddit.com/comments/igezd/|2011-07-04|07:50:40
http://www.reddit.com/comments/j1ul5/|2011-07-27|21:05:02
http://www.reddit.com/comments/jna6x/|2011-08-18|16:17:59
http://www.reddit.com/comments/k8psp/|2011-09-08|04:38:39
http://www.reddit.com/comments/ku5eh/|2011-09-28|08:21:20
http://www.reddit.com/comments/lfl09/|2011-10-17|14:49:00
http://www.reddit.com/comments/m10m1/|2011-11-04|19:11:29
http://www.reddit.com/comments/mmg7t/|2011-11-22|22:17:03",,False,,t5_2qizd,True,,,True,t3_n624n,http://www.reddit.com/r/redditdev/comments/n624n/submission_ids_question/,
1317586582.0,2,self.redditdev,kym4b,Captcha not showing up,3,1,1,http://www.reddit.com/r/redditdev/comments/kym4b/captcha_not_showing_up/,"I just installed reddit but Captcha isn't working when I try to sign up. I installed python-imaging like it says to [here](http://code.reddit.com/wiki/RedditStartToFinish#Everythingisdone), but that didn't work. Any ideas?",,False,,t5_2qizd,False,,,True,t3_kym4b,http://www.reddit.com/r/redditdev/comments/kym4b/captcha_not_showing_up/,
1316119227.0,2,self.redditdev,kgwai,Reddit API - confused about score,6,4,3,http://www.reddit.com/r/redditdev/comments/kgwai/reddit_api_confused_about_score/,"According to http://www.reddit.com/help/faq :

&gt; Please note that the vote numbers are not ""real"" numbers, they have been ""fuzzed"" to prevent spam bots etc. So taking the above example, if five users upvoted the submission, and three users downvote it, the upvote/downvote numbers may say 23 upvotes and 21 downvotes, or 12 upvotes, and 10 downvotes. The points score is  correct, but the vote totals are ""fuzzed"". 

But according to http://www.reddit.com/help/voting :

&gt; Upon voting, the highlighted score will have changed +1 or -1      depending on your vote up or down, respectively. Regardless of individual karmas, all votes affect submission scores equally.

So which is it?  In an upvote a single person upvoting?  If so, then it's NOT ""fuzzed"", but then again, it says it IS fuzzed.  Oh reddit...why u so confusing?!",,False,,t5_2qizd,False,,,True,t3_kgwai,http://www.reddit.com/r/redditdev/comments/kgwai/reddit_api_confused_about_score/,
1312052204.0,2,self.redditdev,j4a1c,What database engine does Reddit use?,2,0,2,http://www.reddit.com/r/redditdev/comments/j4a1c/what_database_engine_does_reddit_use/,What database engine does Reddit use? A custom one maybe or an open source DB like PostgreSQL?,,False,,t5_2qizd,False,,,True,t3_j4a1c,http://www.reddit.com/r/redditdev/comments/j4a1c/what_database_engine_does_reddit_use/,
1310506747.0,2,self.redditdev,intcq,ERROR: 'NoneType' object has no attribute 'utf_8_decode',2,0,4,http://www.reddit.com/r/redditdev/comments/intcq/error_nonetype_object_has_no_attribute_utf_8/,"So, I'm trying to compile reddit on ubuntu maverick, but when I get to the ""python setup.py develop"" step, I get this: http://pastebin.com/RKKVpCKq
Is there a way I can get cython to work? If I do it without cython, will it be a big deal?",,False,,t5_2qizd,False,,,True,t3_intcq,http://www.reddit.com/r/redditdev/comments/intcq/error_nonetype_object_has_no_attribute_utf_8/,
1298140438.0,2,self.redditdev,folld,Reddit VM setup question,4,2,4,http://www.reddit.com/r/redditdev/comments/folld/reddit_vm_setup_question/,"For background, Im exclusively a Windows user although I suppose you could count Dos as my experience with command line execution.  Im here because I recognize an opportunity to implement a Reddit clone internally at my office.  I downloaded the VM image and was able to achieve the basics of accessing the site by modifying my host file with reddit.local.  This is well and good to get me started but I have absolutely no idea had to do much anything else within the VM.  I reviewed the The One Page Linux Manual but Im still not sure how to do the most basic of things.  I realize by not relying on the pre-built VM I could better learn whats actually happening but at this stage Im looking to do something along the lines of a rapid proof of concept.  For this, the VM is perfect.  I just need a little bit of assistance to allow me to delve deeper.

Do I need to be able and login the Reddit VM as a super user?  I tried su but I guess I dont have the right password.  I havent been able to find anything about this on the Reddit blog.  I wanted to create my own user account and explor that aspect of the system.  

What are the commands to read/edit files? (Such as INIs)  I was looking for something like type example.ini or edit example.ini but with no success.  

I hate feeling so helpless on a computer.  If someone could just help me with these simple inquires I bet they would get me on my way to better understanding the environment.  Any recommendations for getting more familiar with the Reddit VM build would be appreciated.  
",,False,,t5_2qizd,False,,,True,t3_folld,http://www.reddit.com/r/redditdev/comments/folld/reddit_vm_setup_question/,
1295549917.0,2,self.redditdev,f5yrf,Clicking on Comment Context Link Causes Out-of-Memory Exception,3,1,10,http://www.reddit.com/r/redditdev/comments/f5yrf/clicking_on_comment_context_link_causes/,"**TL;DR**  paster process dies w/ MemoryError when comment context link is clicked. This is repeatable. The *parents* dictionary variable containing child-parent comment ID references is corrupt/incorrect. Is this a known issue and/or has this been addressed already?

Once my dev environment was functional and loaded with test data I set about trying to learn more about the architecture and inner working of the application. However, before making any changes I decided it would be best to put together some simple test scripts which would allow me to gather some before/after metrics to ensure that any attempted changes don't make performance noticeably worse.  While running an early version of the testing script I was also simultaneously randomly exploring the application where I was checking out the admin/moderator features and evaluating the links/comments being added by the script. At one point I clicked on the context link on a comment displayed in a test user's inbox (comment replies). The page took a very long time to return and when it did the following error text was presented:
    File '/home/reddit/reddit/r2/r2/controllers/validator/validator.py', line 135 in newfn
      return fn(self, *a, **kw)
    File '/home/reddit/reddit/r2/r2/controllers/front.py', line 280 in GET_comments
      comment, context, num, **kw))
    File '/home/reddit/reddit/r2/r2/lib/pages/pages.py', line 896 in __init__
      my_listing = renderer()
    File '/home/reddit/reddit/r2/r2/lib/pages/pages.py', line 893 in renderer
      return listing.listing()
    File '/home/reddit/reddit/r2/r2/models/listing.py', line 106 in listing
      wrapped_items = self.get_items(num = self.num)
    File '/home/reddit/reddit/r2/r2/models/listing.py', line 58 in get_items
      builder_items = self.builder.get_items(*a, **kw)
    File '_builder.pyx', line 203 in r2.models._builder._CommentBuilder.get_items (r2/models/_builder.c:3346)
    MemoryError: 

At that point the *paster* process running the Reddit server had ab-ended. I am able to repeat this issue and when monitoring the system using *top* I found that indeed one *paster* process would continue to suck up memory to the point it would eventually max out and die. I did do some digging in the DB and didn't find anything anomalous with any of the link's comments. However when I put some debugging statements in the code (_builder.pyx), it did reveal that the *parents* variable used by the *get_items* function  and populated by the *link_comments_and_sort* function does contain circular references (i.e. the child and parent ids for some entries are the same).

I'd love to dig into this some more, but I feel compelled to ask if anyone else has experienced this and if they are aware of a fix for this situation besides resetting the system? I'd hate to spend any more time on this and find that this has been addressed or is going to be addressed shortly.

Here are screen shots of the browser showing the [page displaying the link](http://imgur.com/5Sdag) in question and the [resulting error page](http://imgur.com/9te7x).
",,False,,t5_2qizd,False,,,True,t3_f5yrf,http://www.reddit.com/r/redditdev/comments/f5yrf/clicking_on_comment_context_link_causes/,
1289356715.0,2,reddit.com,e3t8t,Some iReddit ideas (cross-post from ideasfortheadmins). Anyone want to try implementing these first?,4,2,0,http://www.reddit.com/r/redditdev/comments/e3t8t/some_ireddit_ideas_crosspost_from/,,,False,,t5_2qizd,False,,,False,t3_e3t8t,http://www.reddit.com/r/ideasfortheadmins/comments/e3r4j/some_ireddit_ideas/,
1284496959.0,2,self.redditdev,dduvh,Only show submissions submitted in the last x hours?,3,1,1,http://www.reddit.com/r/redditdev/comments/dduvh/only_show_submissions_submitted_in_the_last_x/,"Sorry I don't know if this is the right place to ask, but is this possible? I seem to remember seeing someone mention it maybe a year ago.

On a busy day when I want to see just the top submissions it would be nice to sort by 'top', read through the most upvoted submissions in my subreddits in the last 12 hours at 8am, and same thing at 4pm but seeing only submissions in the last 8 hours.",,False,,t5_2qizd,False,,,True,t3_dduvh,http://www.reddit.com/r/redditdev/comments/dduvh/only_show_submissions_submitted_in_the_last_x/,
1283201799.0,2,self.redditdev,d7egb,How to get more json results? I get only 30,3,1,3,http://www.reddit.com/r/redditdev/comments/d7egb/how_to_get_more_json_results_i_get_only_30/,"I have the script below which calls a subreddit but I only get 30 items listed, how do I go about getting all the entries?

&lt;script src=""http://code.jquery.com/jquery-1.4.2.min.js""&gt;&lt;/script&gt;
&lt;script&gt;
$.getJSON(""http://www.reddit.com/r/pics/.json?jsonp=?"", function(data) {
    var items = data.data.children;
    $.each(data.data.children, function(i, item) {
		$(""&lt;a/&gt;"", {
            href: item.data.url,
			text: item.data.title + "" - ("" + item.data.score + "" points)""
        }).appendTo($(""#images""));		
		$(""&lt;br&gt;"", {
        }).appendTo($(""#images""));

    });
	    });
&lt;/script&gt;",,False,,t5_2qizd,False,,,True,t3_d7egb,http://www.reddit.com/r/redditdev/comments/d7egb/how_to_get_more_json_results_i_get_only_30/,
1282790467.0,2,self.redditdev,d5j8f,Virtualbox image won't import,3,1,0,http://www.reddit.com/r/redditdev/comments/d5j8f/virtualbox_image_wont_import/,"So I recently tried to load the virtualbox image, but virtualbox gave me this error on import:

&gt; too many IDE controllers - virtualbox only supports 1

Looks like a conversion to the OVF format from vmware didn't go so well... so I cracked open the OVF file and commented out the second IDE controller there like so:

	&lt;!--
      &lt;Item&gt;
        &lt;rasd:Address&gt;1&lt;/rasd:Address&gt;
        &lt;rasd:Caption&gt;ideController1&lt;/rasd:Caption&gt;
        &lt;rasd:Description&gt;IDE Controller&lt;/rasd:Description&gt;
        &lt;rasd:ElementName&gt;ideController1&lt;/rasd:ElementName&gt;
        &lt;rasd:InstanceID&gt;4&lt;/rasd:InstanceID&gt;
        &lt;rasd:ResourceSubType&gt;PIIX4&lt;/rasd:ResourceSubType&gt;
        &lt;rasd:ResourceType&gt;5&lt;/rasd:ResourceType&gt;
      &lt;/Item&gt;
	--&gt;

This seemed to fix it as the vm imported and booted up.  Note that I had to exit the import window and go back into it for it to actually reload the OVF file after I modified it. 

Just throwing it out there in case anyone else gets this error. 

EDIT:  And now it boots but doesn't recognise any ethernet cards... sigh.  Sleep time I guess. ",,False,,t5_2qizd,True,,,True,t3_d5j8f,http://www.reddit.com/r/redditdev/comments/d5j8f/virtualbox_image_wont_import/,
1280092539.0,2,self.redditdev,ctkxj,"Want to experiment with a reddit clone, no idea how to start. Please help?",3,1,4,http://www.reddit.com/r/redditdev/comments/ctkxj/want_to_experiment_with_a_reddit_clone_no_idea/,"I've never used linux or worked with any scripts, etc. Just html/css/photoshop. I really want to work with reddit's code but am having a bit of trouble setting it up. 

I'm on nearlyfreespeech.net running freebsd 7.2 (http://example72.nfshost.com/versions.php). so i tried to use this guide: http://code.reddit.com/wiki/RedditStartToFinishFreeBSD 

i was able to $ git clone http://code.reddit.com/repo/reddit.git but that's about it. i can't use sudo or pkg_add, etc so i wasn't able to do anything else. has anyone installed reddit on nfs before or can explain the steps in simpler terms for me? 

",,False,,t5_2qizd,False,,,True,t3_ctkxj,http://www.reddit.com/r/redditdev/comments/ctkxj/want_to_experiment_with_a_reddit_clone_no_idea/,
1277899782.0,2,self.redditdev,ckigm,New Installation: Using the Ubuntu 64-bit.vmdk but on startup of the Sun Virtual Box I get a black screen and no prompt.,3,1,1,http://www.reddit.com/r/redditdev/comments/ckigm/new_installation_using_the_ubuntu_64bitvmdk_but/,"Apologies in advance for my thickheadedness. First time trying to get reddit up and running on VirtualBox. 

A couple of questions:

* I've got the VMDK up and running but I get no prompt, and certainly nowhere to put the username and password as suggested on the [Get Code](http://code.reddit.com/) page. 

* I also have the reddit-vm.mf and the reddit-vm.ovf files, but have no idea what to do with them. Suggestions?

Many thanks in advance.


EDIT 1: Now I'm importing the OVF and getting everything set up that way (File &gt; Import Appliance). However I get one of two errors. 

* The appliance import wizard doesn't like that the OVF has two instances of a Hard Disk Controller. Both of type PIIX4

* If the OVF is changed to have only one Hard Disk Controller, the appliance import wizard explains that the SHA1 digest of 'reddit-vm.ovf' doesn't match to the one in 'reddit-vm.mf'. 


EDIT 2: I have upgraded to the latest version of VirtualBox and this seems to have done the trick. The two instances of a Hard Disk Controller are accepted and I'm creating the appliance now.


EDIT 3: Now the reddit-vm exists in VirtualBox. However, on starting it up I get:

* Failed to open a session for the virtual machine reddit-vm. The VM session was closed before any attempt to power it on. 

* Result Code: E_Fail (0x80004005)

* Component: Machine

* ISession: {12f4dcdb-12b2-4ec1-b7cd-ddd9f6c5bf4d}


EDIT 4: This was caused by the VM machine having assigned too much memory. Not enough was left for the 
host OS. 


EDIT 5: Now: Failed to open a session for the virtual machine reddit-vm. 

* VD: error VERR_NOT_SUPPORTED opening image file 'c:\...\Ubuntu 64-bit.vmdk'(VERR_NOT_SUPPORTED).

* Unknown error creating VM (WERR_CFGM_VALUE_NOT_FOUND).


EDIT 6: I removed the reddit vmdk from the list in the VirtualBox settings, exited and restarted Virtualbox. Now, the VM appears to be running. However, with black screen and non-blinking cursor. 

EDIT 7: Failing the above. I tried the VMWare from the download page. Same problem. Black screen and non-blinking cursor. Anyone?",,False,,t5_2qizd,True,,,True,t3_ckigm,http://www.reddit.com/r/redditdev/comments/ckigm/new_installation_using_the_ubuntu_64bitvmdk_but/,
1275619262.0,2,self.redditdev,cb9cq,Switching from a Pligg CMS to reddit,3,1,6,http://www.reddit.com/r/redditdev/comments/cb9cq/switching_from_a_pligg_cms_to_reddit/,"I'm interested in switching my site over from a very very heavily modified Pligg code to this fancy reddit system.  

What should I know before going to developers about this?  How difficult would this be?",,False,,t5_2qizd,False,,,True,t3_cb9cq,http://www.reddit.com/r/redditdev/comments/cb9cq/switching_from_a_pligg_cms_to_reddit/,
1274331113.0,2,self.redditdev,c66o1,Access more links with the reddit api,2,0,4,http://www.reddit.com/r/redditdev/comments/c66o1/access_more_links_with_the_reddit_api/,I'm creating a reddit client application in C#. Could anyone point me to the right way to access data from reddit. How do you get more than 25 links from the rss. It's currently only returning the top 25 and I want to be able to access more items (i.e. next button functionality).,,False,,t5_2qizd,False,,,True,t3_c66o1,http://www.reddit.com/r/redditdev/comments/c66o1/access_more_links_with_the_reddit_api/,
1274177252.0,2,self.redditdev,c5hlu,Just set up my reddit clone. Are these problems because of my server or configuration?,3,1,12,http://www.reddit.com/r/redditdev/comments/c5hlu/just_set_up_my_reddit_clone_are_these_problems/,"Hey folks, I just had Reddit installed on a Dreamhost VPS, and I've been playing around with it to see if I can get things going.

Over the last few hours, I've been getting intermittent errors, and I'm wondering if this is due to a server issue or configuration problem. Could it be that I have an old version of something? This can happen when submitting a new link, clicking on a category, or with login. The problem eventually goes away with reloads. I'm attaching just 2 separate errors because of the character limit:

Module r2.controllers.listingcontroller:563 in GETlisting 
&lt;&lt; def GETlisting(self, where, **env):

        self.where = where
        return ListingController.GET_listing(self, **env)

class MyredditsController(ListingController):&gt;&gt;  return ListingController.GET_listing(self, **env)
Module r2.controllers.listingcontroller:163 in GET_listing 
&lt;&lt;

    def GET_listing(self, **env):
        return self.build_listing(**env)

class FixListing(object):&gt;&gt;  return self.build_listing(**env)
Module r2.controllers.validator.validator:134 in newfn 
&lt;&lt; try:

                kw = _make_validated_kw(fn, simple_vals, param_vals, env)
                return fn(self, *a, **kw)
            except UserRequiredException:
                return self.intermediate_redirect('/login')&gt;&gt;  return fn(self, *a, **kw)
Module r2.controllers.redditbase:429 in newfn 
&lt;&lt; kw['reverse'] = True

        return fn(self, **kw)
    return new_fn
&gt;&gt;  return fn(self, **kw)
Module r2.controllers.listingcontroller:95 in buildlisting 
&lt;&lt; self.queryobj = self.query()

        self.builder_obj = self.builder()
        self.listing_obj = self.listing()
        content = self.content()
        res =  self.render_cls(content = content,&gt;&gt;  self.listing_obj = self.listing()
Module r2.controllers.listingcontroller:150 in listing 
&lt;&lt; abort(403, 'forbidden')

        listing = LinkListing(self.builder_obj, show_nums = self.show_nums)
        return listing.listing()

    def title(self):&gt;&gt;  return listing.listing()
Module r2.models.listing:65 in listing 
&lt;&lt;

    def listing(self):
        self.things, prev, next, bcount, acount = self.get_items()

        self.max_num = max(acount, bcount)&gt;&gt;  self.things, prev, next, bcount, acount = self.get_items()
Module r2.models.listing:58 in getitems 
&lt;&lt; """"""Wrapper around builder's getitems that caches the rendering.""""""

        from r2.lib.template_helpers import replace_render
        builder_items = self.builder.get_items(*a, **kw)
        for item in self.builder.item_iter(builder_items):
            # rewrite the render method&gt;&gt;  builder_items = self.builder.get_items(*a, **kw)
Module r2.models.builder:338 in get_items 
&lt;&lt; #wrap

            if self.wrap:
                new_items = self.wrap_items(new_items)

            #skip and count&gt;&gt;  new_items = self.wrap_items(new_items)
Module r2.models.builder:71 in wrap_items 
&lt;&lt; aids = None

        authors = Account._byID(aids, True) if aids else {}
        # srids = set(l.sr_id for l in items if hasattr(l, ""sr_id""))
        subreddits = Subreddit.load_subreddits(items)&gt;&gt;  authors = Account._byID(aids, True) if aids else {}
Module r2.lib.db.thing:279 in _byID 
&lt;&lt; return items

        bases = sgm(cache, ids, items_db, prefix)

        #check to see if we found everything we asked for&gt;&gt;  bases = sgm(cache, ids, items_db, prefix)
Module r2.lib.cache:202 in sgm 
&lt;&lt; keys = set(keys)

    s_keys = dict((str(k), k) for k in keys)
    r = cache.get_multi(s_keys.keys(), prefix)
    if miss_fn and len(r.keys()) &lt; len(keys):
        need = set(s_keys.keys()) - set(r.keys())&gt;&gt;  r = cache.get_multi(s_keys.keys(), prefix)
Module r2.lib.cache:45 in getmulti 
&lt;&lt; keymap = dict((str(k), k) for k in keys)

        r = self.simple_get_multi(key_map.keys())

        if not partial and len(r.keys()) &lt; len(key_map):&gt;&gt;  r = self.simple_get_multi(key_map.keys())
Module r2.lib.cache:186 in simplegetmulti 
&lt;&lt; if len(out) == len(keys):

                break
            r = c.simple_get_multi(need)
            #update other caches
            if r:&gt;&gt;  r = c.simple_get_multi(need)
Module r2.lib.contrib.memcache:763 in get_multi 
&lt;&lt; if rkey is not None:

                        val = self._recv_value(server, flags, rlen)
                        retvals[prefixed_to_orig_key[rkey]] = val   # un-prefix returned key.
                    line = server.readline()
            except (_Error, socket.error), msg:&gt;&gt;  retvals[prefixed_to_orig_key[rkey]] = val # un-prefix returned key.
&lt;type 'exceptions.KeyError'&gt;: 'c38413febefbd93dbe9ec58231c153d3'

""


--------------------------------------------------------------------------------

""Module r2.lib.base:47 in before 
&lt;&lt;

    def __before__(self):
        self.pre()

    def __call__(self, environ, start_response):&gt;&gt;  self.pre()
Module r2.controllers.redditbase:497 in pre 
&lt;&lt; validcookie(c.cookies[g.login_cookie].value

                         if g.login_cookie in c.cookies
                         else '')

        if c.user:&gt;&gt;  else '')
Module r2.models.account:304 in valid_cookie 
&lt;&lt;

    try:
        account = Account._byID(uid, True)
        if account._deleted:
            return (False, False)&gt;&gt;  account = Account._byID(uid, True)
Module r2.lib.db.thing:279 in _byID 
&lt;&lt; return items

        bases = sgm(cache, ids, items_db, prefix)

        #check to see if we found everything we asked for&gt;&gt;  bases = sgm(cache, ids, items_db, prefix)
Module r2.lib.cache:202 in sgm 
&lt;&lt; keys = set(keys)

    s_keys = dict((str(k), k) for k in keys)
    r = cache.get_multi(s_keys.keys(), prefix)
    if miss_fn and len(r.keys()) &lt; len(keys):
        need = set(s_keys.keys()) - set(r.keys())&gt;&gt;  r = cache.get_multi(s_keys.keys(), prefix)
Module r2.lib.cache:45 in getmulti 
&lt;&lt; keymap = dict((str(k), k) for k in keys)

        r = self.simple_get_multi(key_map.keys())

        if not partial and len(r.keys()) &lt; len(key_map):&gt;&gt;  r = self.simple_get_multi(key_map.keys())
Module r2.lib.cache:186 in simplegetmulti 
&lt;&lt; if len(out) == len(keys):

                break
            r = c.simple_get_multi(need)
            #update other caches
            if r:&gt;&gt;  r = c.simple_get_multi(need)
Module r2.lib.contrib.memcache:763 in get_multi 
&lt;&lt; if rkey is not None:

                        val = self._recv_value(server, flags, rlen)
                        retvals[prefixed_to_orig_key[rkey]] = val   # un-prefix returned key.
                    line = server.readline()
            except (_Error, socket.error), msg:&gt;&gt;  retvals[prefixed_to_orig_key[rkey]] = val # un-prefix returned key.
&lt;type 'exceptions.KeyError'&gt;: '85f25d6583f48885936b39b8382b3279' ""


--------------------------------------------------------------------------------

Things seemed to be ok at first, which is why I'm also wondering if I'm already falling behind on some housekeeping operations (but there have just been two people posting things for a day). Any thoughts? And thanks. I'm going to put in the time to learn things, but I know I'll need the help.",,False,,t5_2qizd,False,,,True,t3_c5hlu,http://www.reddit.com/r/redditdev/comments/c5hlu/just_set_up_my_reddit_clone_are_these_problems/,
1272324932.0,2,self.redditdev,bwgi3,wrong 'awards' table name?,2,0,1,http://www.reddit.com/r/redditdev/comments/bwgi3/wrong_awards_table_name/,"in the RedditStartToFinishIntrepid wiki page, there's a:

    postgres$ createdb -E utf8 awards

but in example.ini, the award database name is:
 
    award_db =       award,       127.0.0.1, ri,   password

see the inconsistency with the 's'?

This is the error i get when i run paster shell example.ini without fixing the name.

    sqlalchemy.exc.OperationalError: (OperationalError) FATAL:  database ""award"" does not exist         ",,False,,t5_2qizd,False,,,True,t3_bwgi3,http://www.reddit.com/r/redditdev/comments/bwgi3/wrong_awards_table_name/,
1264098667.0,2,self.redditdev,ashz1,Ask Proggit: Help with using GreaseMonkey + Reddit API to hide resubs on other sites?,3,1,2,http://www.reddit.com/r/redditdev/comments/ashz1/ask_proggit_help_with_using_greasemonkey_reddit/,"I'm creating a GreaseMonkey script to hide obvious reddit re-submissions to Digg, so I have less of a reason to cringe while browsing around that site. My first thought was to just blacklist anything from imgur on Digg, but then got to thinking that the reddit API could prove useful in cross referencing stories and such.

Anyone have suggestions on how to utilize the reddit API to hide stories on Digg (and other sites) that appeared on reddit the day before?",,False,,t5_2qizd,False,,,True,t3_ashz1,http://www.reddit.com/r/redditdev/comments/ashz1/ask_proggit_help_with_using_greasemonkey_reddit/,
1256842758.0,2,self.redditdev,9z3nv,Rate Limiter Help,3,1,7,http://www.reddit.com/r/redditdev/comments/9z3nv/rate_limiter_help/,"I'm having trouble finding information on how the rate limiter works.  Basically what obscene number of requests is required to trip it?  I ask because I turned the dial on my obscene number of requests generator from low to moderately ridiculous and flipped it on.  It seems that, part of the way through, I start getting 503 errors.  As a side effect I seem to accumulate some sort of fun ban timer as a result.  I tried to look into how it works in the source, but it wasn't immediately clear, and there's no explanation anywhere in code.reddit as far as I can see.

Edit: Obscene request generator is an app with a purpose.  I'm not arbitrarily pounding reddit.",,False,,t5_2qizd,True,,,True,t3_9z3nv,http://www.reddit.com/r/redditdev/comments/9z3nv/rate_limiter_help/,
1256027934.0,2,self.redditdev,9vtbc,Trophy suggestion,3,1,4,http://www.reddit.com/r/redditdev/comments/9vtbc/trophy_suggestion/,"Include a link to the post in question on relevant trophies in the [honour roll](http://www.reddit.com/awards). I'd love to see some of them, especially the controversial ones.

While you're at it, if you create [a supporter of Reddit trophy](http://www.reddit.com/r/programming/comments/9pw51/stackoverflow_advertising_and_the_ethics_of_a/c0dvt7a), I'll buy one.",,False,,t5_2qizd,False,,,True,t3_9vtbc,http://www.reddit.com/r/redditdev/comments/9vtbc/trophy_suggestion/,
1255056162.0,2,self.redditdev,9s9j2,"Redditdevs, why is gonzone's post appearing in my overview?",3,1,3,http://www.reddit.com/r/redditdev/comments/9s9j2/redditdevs_why_is_gonzones_post_appearing_in_my/,"I was trying to submit this:

http://www.change.org/actions/view/help_strengthen_americas_toxic_chemicals_standards

to /r/health when it happened",,False,,t5_2qizd,True,,,True,t3_9s9j2,http://www.reddit.com/r/redditdev/comments/9s9j2/redditdevs_why_is_gonzones_post_appearing_in_my/,
1252884135.0,2,self.redditdev,9k75r,Is it possible to pick up the icon for a subreddit from the json?,2,0,2,http://www.reddit.com/r/redditdev/comments/9k75r/is_it_possible_to_pick_up_the_icon_for_a/,"I'm writing a reddit client for the Nokia N900 as a hands-on tutorial.  (You can read more about that at http://blogs.gnome.org/tthurman/2009/09/06/n900-tutorial-contents/ if you want.)

I'm going to be adding support for subreddits to a forthcoming version.  I would like to give the user a selector showing the images from the top left-hand corners of the reddits, but there's no copy of the URL for these in the json for each subreddit, nor in the json for www.reddit.com/reddits/.

It did occur to me that there might be a rule that they appeared at a url on thumbs.reddit.com which could be deduced from the ID of the subreddit, just as thumbnails of posts appear at an analogous http://thumbs.reddit.com/t3_*.png address.  This does actually work for many of them:

http://thumbs.reddit.com/t5_2qh1i.png is /r/AskReddit
http://thumbs.reddit.com/t5_2qh61.png is /r/WTF
http://thumbs.reddit.com/t5_2qhh7.png is /r/lgbt
http://thumbs.reddit.com/t5_2qhhn.png is /r/marijuana

BUT
http://thumbs.reddit.com/t5_2fwo.png is a false negative for /r/programming; it does have a custom image but it's elsewhere
http://thumbs.reddit.com/t5_2qh0u.png is a false positive for /r/pics, which doesn't have a custom image

Is there any way of working this out from the API, or should I just offer them a plain list without icons?",,False,,t5_2qizd,False,,,True,t3_9k75r,http://www.reddit.com/r/redditdev/comments/9k75r/is_it_possible_to_pick_up_the_icon_for_a/,
1250753984.0,2,self.redditdev,9ccrl,the service you request is temporarily unavailable. please try again later. ,2,0,7,http://www.reddit.com/r/redditdev/comments/9ccrl/the_service_you_request_is_temporarily/,"Hi all,
I'm running a reddit and serving it through nginx. Occasionally I get the error in the title when opening some pages, and I have no errors in the paster's output. As far as you know, is this error printed by nginx? How could I investigate and get rid of this? It happens fairly often, especially after paster's been online for a while.",,False,,t5_2qizd,False,,,True,t3_9ccrl,http://www.reddit.com/r/redditdev/comments/9ccrl/the_service_you_request_is_temporarily/,
